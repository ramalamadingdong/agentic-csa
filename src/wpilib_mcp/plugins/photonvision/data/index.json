{
  "vendor": "photonvision",
  "version": "latest",
  "built_at": "2025-12-04T22:18:54.032843",
  "pages": [
    {
      "url": "https://docs.photonvision.org/en/latest/",
      "title": "PhotonVision Docs",
      "section": "General",
      "language": "All",
      "content": "Welcome to the official documentation of PhotonVision! PhotonVision is the free, fast, and easy-to-use vision processing solution for the FIRST Robotics Competition. PhotonVision is designed to get vision working on your robot quickly , without the significant cost of other similar solutions. PhotonVision supports a variety of COTS hardware, including the Raspberry Pi 3, 4, and 5, the SnakeEyes Pi hat , and the Orange Pi 5. Content  Quick Start Quick start to using Photonvision. Quick Start Advanced Installation Get started with installing PhotonVision on non-supported hardware. Advanced Installation Programming Reference and PhotonLib Learn more about PhotonLib, our vendor dependency which makes it easier for teams to retrieve vision data, make various calculations, and more. Programming Reference Integration Pick how to use vision processing results to control a physical robot. Robot Integration Code Examples View various step by step guides on how to use data from PhotonVision in your code, along with game-specific examples. Code Examples Hardware Select appropriate hardware for high-quality and easy vision target detection. Hardware Selection Contributing Interested in helping with PhotonVision? Learn more about how to contribute to our main code base, documentation, and more. Contributing to PhotonVision Projects Source Code  The source code for all PhotonVision projects is available through our GitHub organization . PhotonVision Contact Us  To report a bug or submit a feature request in PhotonVision, please submit an issue on the PhotonVision GitHub or contact the developers on Discord . If you find a problem in this documentation, please submit an issue on the PhotonVision Documentation GitHub . License  PhotonVision is licensed under the GNU GPL v3 .",
      "content_preview": "Welcome to the official documentation of PhotonVision! PhotonVision is the free, fast, and easy-to-use vision processing solution for the FIRST Robotics Competition. PhotonVision is designed to get vision working on your robot quickly , without the significant cost of other similar solutions."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/programming/photonlib/index.html",
      "title": "PhotonLib: Robot Code Interface",
      "section": "PhotonLib",
      "language": "All",
      "content": "PhotonLib: Robot Code Interface  Installing PhotonLib Getting Target Data Using Target Data AprilTags and PhotonPoseEstimator Driver Mode and Pipeline Index/Latency Controlling LEDs",
      "content_preview": "PhotonLib: Robot Code Interface  Installing PhotonLib Getting Target Data Using Target Data AprilTags and PhotonPoseEstimator Driver Mode and Pipeline Index/Latency Controlling LEDs"
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/programming/photonlib/getting-target-data.html",
      "title": "Getting Target Data",
      "section": "PhotonLib",
      "language": "All",
      "content": "Getting Target Data  Constructing a PhotonCamera  What is a PhotonCamera?  PhotonCamera is a class in PhotonLib that allows a user to interact with one camera that is connected to hardware that is running PhotonVision. Through this class, users can retrieve yaw, pitch, roll, robot-relative pose, latency, and a wealth of other information. The PhotonCamera class has two constructors: one that takes a NetworkTable and another that takes in the name of the network table that PhotonVision is broadcasting information over. For ease of use, it is recommended to use the latter. The name of the NetworkTable (for the string constructor) should be the same as the camera’s nickname (from the PhotonVision UI). JAVA // Change this to match the name of your camera PhotonCamera camera = new PhotonCamera ( \"photonvision\" ); C++ // Change this to match the name of your camera photonlib :: PhotonCamera camera { \"photonvision\" }; PYTHON # Change this to match the name of your camera as shown in the web ui self . camera = PhotonCamera ( \"your_camera_name_here\" ) Warning Teams must have unique names for all of their cameras regardless of which coprocessor they are attached to. Getting the Pipeline Result  What is a Photon Pipeline Result?  A PhotonPipelineResult is a container that contains all information about currently detected targets from a PhotonCamera . You can retrieve the latest pipeline result using the PhotonCamera instance. Use the getLatestResult() / GetLatestResult() (Java and C++ respectively) to obtain the latest pipeline result. An advantage of using this method is that it returns a container with information that is guaranteed to be from the same timestamp. This is important if you are using this data for latency compensation or in an estimator. JAVA // Query the latest result from PhotonVision var result = camera . getLatestResult (); C++ // Query the latest result from PhotonVision photonlib :: PhotonPipelineResult result = camera . GetLatestResult (); PYTHON # Query the latest result from PhotonVision result = self . camera . getLatestResult () Note Unlike other vision software solutions, using the latest result guarantees that all information is from the same timestamp. This is achievable because the PhotonVision backend sends a byte-packed string of data which is then deserialized by PhotonLib to get target data. For more information, check out the PhotonLib source code . Checking for Existence of Targets  Each pipeline result has a hasTargets() / HasTargets() (Java and C++ respectively) method to inform the user as to whether the result contains any targets. JAVA // Check if the latest result has any targets. boolean hasTargets = result . hasTargets (); C++ // Check if the latest result has any targets. bool hasTargets = result . HasTargets (); PYTHON # Check if the latest result has any targets. hasTargets = result . hasTargets () Warning In Java/C++, You must always check if the result has a target via hasTargets() / HasTargets() before getting targets or else you may get a null pointer exception. Further, you must use the same result in every subsequent call in that loop. Getting a List of Targets  What is a Photon Tracked Target?  A tracked target contains information about each target from a pipeline result. This information includes yaw, pitch, area, and robot relative pose. You can get a list of tracked targets using the getTargets() / GetTargets() (Java and C++ respectively) method from a pipeline result. JAVA // Get a list of currently tracked targets. List < PhotonTrackedTarget > targets = result . getTargets (); C++ // Get a list of currently tracked targets. wpi :: ArrayRef < photonlib :: PhotonTrackedTarget > targets = result . GetTargets (); PYTHON # Get a list of currently tracked targets. targets = result . getTargets () Getting the Best Target  You can get the best target using getBestTarget() / GetBestTarget() (Java and C++ respectively) method from the pipeline result. JAVA // Get the current best target. PhotonTrackedTarget target = result . getBestTarget (); C++ // Get the current best target. photonlib :: PhotonTrackedTarget target = result . GetBestTarget (); PYTHON # Coming Soon! Getting Data From A Target  double getYaw() / GetYaw() : The yaw of the target in degrees (positive left). double getPitch() / GetPitch() : The pitch of the target in degrees (positive up). double getArea() / GetArea() : The area (how much of the camera feed the bounding box takes up) as a percent (0-100). double getSkew() / GetSkew() : The skew of the target in degrees (counter-clockwise positive). double[] getCorners() / GetCorners() : The 4 corners of the minimum bounding box rectangle. Transform2d getCameraToTarget() / GetCameraToTarget() : The camera to target transform. See 2d transform documentation here . JAVA // Get information from target. double yaw = target . getYaw (); double pitch = target . getPitch (); double area = target . getArea (); double skew = target . getSkew (); Transform2d pose = target . getCameraToTarget (); List < TargetCorner > corners = target . getCorners (); C++ // Get information from target. double yaw = target . GetYaw (); double pitch = target . GetPitch (); double area = target . GetArea (); double skew = target . GetSkew (); frc :: Transform2d pose = target . GetCameraToTarget (); wpi :: SmallVector < std :: pair < double , double > , 4 > corners = target . GetCorners (); PYTHON # Get information from target. yaw = target . getYaw () pitch = target . getPitch () area = target . getArea () skew = target . getSkew () pose = target . getCameraToTarget () corners = target . getDetectedCorners () Getting AprilTag Data From A Target  Note All of the data above ( except skew ) is available when using AprilTags. int getFiducialId() / GetFiducialId() : The ID of the detected fiducial marker. double getPoseAmbiguity() / GetPoseAmbiguity() : How ambiguous the pose of the target is (see below). Transform3d getBestCameraToTarget() / GetBestCameraToTarget() : Get the transform that maps camera space (X = forward, Y = left, Z = up) to object/fiducial tag space (X forward, Y left, Z up) with the lowest reprojection error. Transform3d getAlternateCameraToTarget() / GetAlternateCameraToTarget() : Get the transform that maps camera space (X = forward, Y = left, Z = up) to object/fiducial tag space (X forward, Y left, Z up) with the highest reprojection error. JAVA // Get information from target. int targetID = target . getFiducialId (); double poseAmbiguity = target . getPoseAmbiguity (); Transform3d bestCameraToTarget = target . getBestCameraToTarget (); Transform3d alternateCameraToTarget = target . getAlternateCameraToTarget (); C++ // Get information from target. int targetID = target . GetFiducialId (); double poseAmbiguity = target . GetPoseAmbiguity (); frc :: Transform3d bestCameraToTarget = target . getBestCameraToTarget (); frc :: Transform3d alternateCameraToTarget = target . getAlternateCameraToTarget (); PYTHON # Get information from target. targetID = target . getFiducialId () poseAmbiguity = target . getPoseAmbiguity () bestCameraToTarget = target . getBestCameraToTarget () alternateCameraToTarget = target . getAlternateCameraToTarget () Saving Pictures to File  A PhotonCamera can save still images from the input or output video streams to file. This is useful for debugging what a camera is seeing while on the field and confirming targets are being identified properly. Images are stored within the PhotonVision configuration directory. Running the “Export” operation in the settings tab will download a .zip file which contains the image captures. JAVA // Capture pre-process camera stream image camera . takeInputSnapshot (); // Capture post-process camera stream image camera . takeOutputSnapshot (); C++ // Capture pre-process camera stream image camera . TakeInputSnapshot (); // Capture post-process camera stream image camera . TakeOutputSnapshot (); PYTHON # Capture pre-process camera stream image camera . takeInputSnapshot () # Capture post-process camera stream image camera . takeOutputSnapshot () Note Saving images to file takes a bit of time and uses up disk space, so doing it frequently is not recommended. In general, the camera will save an image every 500ms. Calling these methods faster will not result in additional images. Consider tying image captures to a button press on the driver controller, or an appropriate point in an autonomous routine.",
      "content_preview": "Getting Target Data  Constructing a PhotonCamera  What is a PhotonCamera?  PhotonCamera is a class in PhotonLib that allows a user to interact with one camera that is connected to hardware that is running PhotonVision."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/reflectiveAndShape/contour-filtering.html",
      "title": "Contour Filtering and Grouping",
      "section": "Reflective & Shape Detection",
      "language": "All",
      "content": "Contour Filtering and Grouping  Contours that make it past thresholding are filtered and grouped so that only likely targets remain. Filtering Options  Reflective  Contours can be filtered by area, width/height ratio, “fullness”, and “speckle rejection” percentage. Area filtering adjusts the percentage of overall image area that contours are allowed to occupy. The area of valid contours is shown in the “target info” card on the right. Ratio adjusts the width to height ratio of allowable contours. For example, a width to height filtering range of [2, 3] would allow targets that are 250 x 100 pixels in size through. Fullness is a measurement of the ratio between the contour’s area and the area of its bounding rectangle. This can be used to reject contours that are for example solid blobs. Finally, speckle rejection is an algorithm that can discard contours whose area are below a certain percentage of the average area of all visible contours. This might be useful in rejecting stray lights or image noise. Your browser does not support the video tag. Colored Shape  The contours tab has new options for specifying the properties of your colored shape. The target shape types are: Circle - No edges Triangle - 3 edges Quadrilateral - 4 edges Polygon - Any number of edges Only the settings used for the current target shape are available. Shape Simplification - This is the only setting available for polygon, triangle, and quadrilateral target shapes. If you are having issues with edges being “noisy” or “unclean”, adjust this setting to be higher (>75). This high setting helps prevent imperfections in the edge from being counted as a separate edge. Circle Match Distance - How close the centroid of a contour must be to the center of the circle in order for them to be matched. This value is usually pretty small (<25) as you usually only want to identify circles that are nearly centered in the contour. Radius - Percentage of the frame that the radius of the circle represents. Max Canny Threshold - This sets the amount of change between pixels needed to be considered an edge. The smaller it is, the more false circles may be detected. Circles with more points along their ring having high contrast values will be returned first. Circle Accuracy - This determines how perfect the circle contour must be in order to be considered a circle. Low values (<40) are required to detect things that aren’t perfect circles. Contour Grouping and Sorting  These options change how contours are grouped together and sorted. Target grouping can pair adjacent contours, such as the targets found in 2019. Target intersection defines where the targets would intersect if you extended them infinitely, for example, to only group targets tipped “towards” each other in 2019. Finally, target sort defines how targets are ranked, from “best” to “worst.” The available options are: Largest Smallest Highest (towards the top of the image) Lowest Rightmost (Best target on the right, worst on left) Leftmost Centermost Your browser does not support the video tag.",
      "content_preview": "Contour Filtering and Grouping  Contours that make it past thresholding are filtered and grouped so that only likely targets remain. Filtering Options  Reflective  Contours can be filtered by area, width/height ratio, “fullness”, and “speckle rejection” percentage."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/programming/photonlib/robot-pose-estimator.html",
      "title": "AprilTags and PhotonPoseEstimator",
      "section": "PhotonLib",
      "language": "All",
      "content": "AprilTags and PhotonPoseEstimator  Note For more information on how to methods to get AprilTag data, look here . PhotonLib includes a PhotonPoseEstimator class, which allows you to combine the pose data from all tags in view in order to get a field relative pose. For each camera, a separate instance of the PhotonPoseEstimator class should be created. Creating an AprilTagFieldLayout  AprilTagFieldLayout is used to represent a layout of AprilTags within a space (field, shop at home, classroom, etc.). WPILib provides a JSON that describes the layout of AprilTags on the field which you can then use in the AprilTagFieldLayout constructor. You can also specify a custom layout. The API documentation can be found in here: Java , C++ , and Python . JAVA public static final AprilTagFieldLayout kTagLayout = AprilTagFieldLayout . loadField ( AprilTagFields . kDefaultField ); C++ inline const frc :: AprilTagFieldLayout kTagLayout { frc :: AprilTagFieldLayout :: LoadField ( frc :: AprilTagField :: kDefaultField )}; PYTHON AprilTagFieldLayout . loadField ( AprilTagField . kDefaultField ), Defining the Robot to Camera Transform3d  Another necessary argument for creating a PhotonPoseEstimator is the Transform3d representing the robot-relative location and orientation of the camera. A Transform3d contains a Translation3d and a Rotation3d . The Translation3d is created in meters and the Rotation3d is created with radians. For more information on the coordinate system, please see the Coordinate Systems documentation. JAVA public static final Transform3d kRobotToCam = new Transform3d ( new Translation3d ( 0.5 , 0.0 , 0.5 ), new Rotation3d ( 0 , 0 , 0 )); C++ inline const frc :: Transform3d kRobotToCam { frc :: Translation3d { 0.5 _m , 0.0 _m , 0.5 _m }, frc :: Rotation3d { 0 _rad , -30 _deg , 0 _rad }}; PYTHON kRobotToCam = wpimath . geometry . Transform3d ( wpimath . geometry . Translation3d ( 0.5 , 0.0 , 0.5 ), wpimath . geometry . Rotation3d . fromDegrees ( 0.0 , - 30.0 , 0.0 ), ) Creating a PhotonPoseEstimator  The PhotonPoseEstimator has a constructor that takes an AprilTagFieldLayout (see above), PoseStrategy , PhotonCamera , and Transform3d . PoseStrategy has nine possible values: MULTI_TAG_PNP_ON_COPROCESSOR Calculates a new robot position estimate by combining all visible tag corners. Recommended for all teams as it will be the most accurate. Must configure the AprilTagFieldLayout properly in the UI, please see here for more information. LOWEST_AMBIGUITY Choose the Pose with the lowest ambiguity. CLOSEST_TO_CAMERA_HEIGHT Choose the Pose which is closest to the camera height. CLOSEST_TO_REFERENCE_POSE Choose the Pose which is closest to the pose from setReferencePose(). CLOSEST_TO_LAST_POSE Choose the Pose which is closest to the last pose calculated. AVERAGE_BEST_TARGETS Choose the Pose which is the average of all the poses from each tag. MULTI_TAG_PNP_ON_RIO A slower, older version of MULTI_TAG_PNP_ON_COPROCESSOR, not recommended for use. PNP_DISTANCE_TRIG_SOLVE Use distance data from best visible tag to compute a Pose. This runs on the RoboRIO in order to access the robot’s yaw heading, and MUST have addHeadingData called every frame so heading data is up-to-date. Based on a reference implementation by FRC Team 6328 Mechanical Advantage . CONSTRAINED_SOLVEPNP Solve a constrained version of the Perspective-n-Point problem with the robot’s drivebase flat on the floor. This computation takes place on the RoboRIO, and should not take more than 2ms. This also requires addHeadingData to be called every frame so heading data is up to date. If Multi-Tag PNP is enabled on the coprocessor, it will be used to provide an initial seed to the optimization algorithm – otherwise, the multi-tag fallback strategy will be used as the seed. JAVA photonEstimator = new PhotonPoseEstimator ( kTagLayout , PoseStrategy . MULTI_TAG_PNP_ON_COPROCESSOR , kRobotToCam ); C++ constants :: Vision :: kTagLayout , photon :: PoseStrategy :: MULTI_TAG_PNP_ON_COPROCESSOR , constants :: Vision :: kRobotToCam }; photon :: PhotonCamera camera { constants :: Vision :: kCameraName }; PYTHON self . camPoseEst = PhotonPoseEstimator ( AprilTagFieldLayout . loadField ( AprilTagField . kDefaultField ), PoseStrategy . LOWEST_AMBIGUITY , self . cam , kRobotToCam , ) Note Python still takes a PhotonCamera in the constructor, so you must create the camera as shown in the next section and then return and use it to create the PhotonPoseEstimator . Using a PhotonPoseEstimator  The final prerequisite to using your PhotonPoseEstimator is creating a PhotonCamera . To do this, you must set the name of your camera in Photon Client. From there you can define the camera in code. JAVA camera = new PhotonCamera ( kCameraName ); C++ SwerveDrive drivetrain {}; PYTHON self . cam = PhotonCamera ( \"YOUR CAMERA NAME\" ) Calling update() on your PhotonPoseEstimator will return an EstimatedRobotPose , which includes a Pose3d of the latest estimated pose (using the selected strategy) along with a double of the timestamp when the robot pose was estimated. JAVA Optional < EstimatedRobotPose > visionEst = Optional . empty (); for ( var change : camera . getAllUnreadResults ()) { visionEst = photonEstimator . update ( change ); updateEstimationStdDevs ( visionEst , change . getTargets ()); if ( Robot . isSimulation ()) { visionEst . ifPresentOrElse ( est -> getSimDebugField () . getObject ( \"VisionEstimation\" ) . setPose ( est . estimatedPose . toPose2d ()), () -> { getSimDebugField (). getObject ( \"VisionEstimation\" ). setPoses (); }); } visionEst . ifPresent ( est -> { // Change our trust in the measurement based on the tags we can see var estStdDevs = getEstimationStdDevs (); estConsumer . accept ( est . estimatedPose . toPose2d (), est . timestampSeconds , estStdDevs ); }); } C++ for ( const auto & result : camera . GetAllUnreadResults ()) { // cache result and update pose estimator auto visionEst = photonEstimator . Update ( result ); m_latestResult = result ; // In sim only, add our vision estimate to the sim debug field if ( frc :: RobotBase :: IsSimulation ()) { if ( visionEst ) { GetSimDebugField () . GetObject ( \"VisionEstimation\" ) -> SetPose ( visionEst -> estimatedPose . ToPose2d ()); } else { GetSimDebugField (). GetObject ( \"VisionEstimation\" ) -> SetPoses ({}); } } if ( visionEst ) { estConsumer ( visionEst -> estimatedPose . ToPose2d (), visionEst -> timestamp , GetEstimationStdDevs ( visionEst -> estimatedPose . ToPose2d ())); } } PYTHON camEstPose = self . camPoseEst . update () You should be updating your drivetrain pose estimator with the result from the PhotonPoseEstimator every loop using addVisionMeasurement() . JAVA vision = new Vision ( drivetrain :: addVisionMeasurement ); C++ Vision vision {[ = , this ]( frc :: Pose2d pose , units :: second_t timestamp , Eigen :: Matrix < double , 3 , 1 > stddevs ) { drivetrain . AddVisionMeasurement ( pose , timestamp , stddevs ); }}; PYTHON if camEstPose : self . swerve . addVisionPoseEstimate ( camEstPose . estimatedPose , camEstPose . timestampSeconds ) Complete Examples  The complete examples for the PhotonPoseEstimator can be found in the following locations: Java C++ Python Additional PhotonPoseEstimator Methods  For more information on the PhotonPoseEstimator class, please see the API documentation. Java Documentation C++ Documentation Python Documentation",
      "content_preview": "AprilTags and PhotonPoseEstimator  Note For more information on how to methods to get AprilTag data, look here . PhotonLib includes a PhotonPoseEstimator class, which allows you to combine the pose data from all tags in view in order to get a field relative pose."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/apriltag-pipelines/coordinate-systems.html",
      "title": "Coordinate Systems",
      "section": "AprilTag Detection",
      "language": "All",
      "content": "Coordinate Systems  Field and Robot Coordinate Frame  PhotonVision follows the WPILib conventions for the robot and field coordinate systems, as defined here . You define the camera to robot transform in the robot coordinate frame. Camera Coordinate Frame  OpenCV by default uses x-left/y-down/z-out for camera transforms. PhotonVision applies a base rotation to this transformation to make robot to tag transforms more in line with the WPILib coordinate system. The x, y, and z axes are also shown in red, green, and blue in the 3D mini-map and targeting overlay in the UI. The origin is the focal point of the camera lens The x-axis points out of the camera The y-axis points to the left The z-axis points upwards AprilTag Coordinate Frame  The AprilTag coordinate system is defined as follows, relative to the center of the AprilTag itself, and when viewing the tag as a robot would. Again, PhotonVision changes this coordinate system to be more in line with WPILib. This means that a robot facing a tag head-on would see a robot-to-tag transform with a translation only in x, and a rotation of 180 degrees about z. The tag coordinate system is also shown with x/y/z in red/green/blue in the UI target overlay and mini-map. The origin is the center of the tag The x-axis is normal to the plane the tag is printed on, pointing outward from the visible side of the tag. The y-axis points to the right The z-axis points upwards",
      "content_preview": "Coordinate Systems  Field and Robot Coordinate Frame  PhotonVision follows the WPILib conventions for the robot and field coordinate systems, as defined here . You define the camera to robot transform in the robot coordinate frame."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/apriltag-pipelines/multitag.html",
      "title": "MultiTag Localization",
      "section": "AprilTag Detection",
      "language": "All",
      "content": "MultiTag Localization  PhotonVision can combine AprilTag detections from multiple simultaneously observed AprilTags from a particular camera with information about where tags are expected to be located on the field to produce a better estimate of where the camera (and therefore robot) is located on the field. PhotonVision can calculate this multi-target result on your coprocessor, reducing CPU usage on your RoboRio. This result is sent over NetworkTables along with other detected targets as part of the PhotonPipelineResult provided by PhotonLib. Warning MultiTag requires an accurate field layout JSON to be uploaded! Differences between this layout and the tags’ physical location will drive error in the estimated pose output. Warning For the 2025 Reefscape Season, there are two different field layouts. The first is the welded field layout , which photonvision ships with. The second is the Andymark field layout . It is very important to ensure that you use the correct field layout, both in the PhotonPoseEstimator and on the coprocessor . Enabling MultiTag  Ensure that your camera is calibrated and 3D mode is enabled. Navigate to the Output tab and enable “Do Multi-Target Estimation”. This enables MultiTag to use the uploaded field layout JSON to calculate your camera’s pose in the field. This 3D transform will be shown as an additional table in the “targets” tab, along with the IDs of AprilTags used to compute this transform. Note By default, enabling multi-target will disable calculating camera-to-target transforms for each observed AprilTag target to increase performance; the X/Y/angle numbers shown in the target table of the UI are instead calculated using the tag’s expected location (per the field layout JSON) and the field-to-camera transform calculated using MultiTag. If you additionally want the individual camera-to-target transform calculated using SolvePNP for each target, enable “Always Do Single-Target Estimation”. This multi-target pose estimate can be accessed using PhotonLib. We suggest using the PhotonPoseEstimator class with the MULTI_TAG_PNP_ON_COPROCESSOR strategy to simplify code, but the transform can be directly accessed using getMultiTagResult / MultiTagResult() / multitagResult (Java/C++/Python). JAVA var results = camera . getAllUnreadResults (); for ( var result : results ) { var multiTagResult = result . getMultiTagResult (); if ( multiTagResult . isPresent ()) { var fieldToCamera = multiTagResult . get (). estimatedPose . best ; } } C++ auto results = camera . GetAllUnreadResults (); for ( auto & result : results ) { auto multiTagResult = result . MultiTagResult (); if ( multiTagResult . has_value ()) { frc :: Transform3d fieldToCamera = multiTagResult -> estimatedPose . best ; } } PYTHON results = camera . getAllUnreadResults () for result in results : multitagResult = result . multitagResult if multitagResult is not None : fieldToCamera = multitagResult . estimatedPose . best Note The returned field to camera transform is a transform from the fixed field origin to the camera’s coordinate system. This does not change based on alliance color, and by convention is on the BLUE ALLIANCE wall. Updating the Field Layout  PhotonVision ships by default with the 2025 welded field layout JSON . The layout can be inspected by navigating to the settings tab and scrolling down to the “AprilTag Field Layout” card, as shown below. An updated field layout can be uploaded by navigating to the “Device Control” card of the Settings tab and clicking “Import Settings”. In the pop-up dialog, select the “AprilTag Layout” type and choose an updated layout JSON (in the same format as the WPILib field layout JSON linked above) using the paperclip icon, and select “Import Settings”. The AprilTag layout in the “AprilTag Field Layout” card below should be updated to reflect the new layout. Note Currently, there is no way to update this layout using PhotonLib, although this feature is under consideration.",
      "content_preview": "MultiTag Localization  PhotonVision can combine AprilTag detections from multiple simultaneously observed AprilTags from a particular camera with information about where tags are expected to be located on the field to produce a better estimate of where the camera (and therefore robot) is located..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/programming/photonlib/controlling-led.html",
      "title": "Controlling LEDs",
      "section": "PhotonLib",
      "language": "All",
      "content": "Controlling LEDs  You can control the vision LEDs of supported hardware via PhotonLib using the setLED() method on a PhotonCamera instance. In Java and C++, an VisionLEDMode enum class is provided to choose values from. These values include, kOff , kOn , kBlink , and kDefault . kDefault uses the default LED value from the selected pipeline. JAVA // Blink the LEDs. camera . setLED ( VisionLEDMode . kBlink ); C++ // Blink the LEDs. camera . SetLED ( photonlib :: VisionLEDMode :: kBlink ); PYTHON # Coming Soon!",
      "content_preview": "Controlling LEDs  You can control the vision LEDs of supported hardware via PhotonLib using the setLED() method on a PhotonCamera instance. In Java and C++, an VisionLEDMode enum class is provided to choose values from. These values include, kOff , kOn , kBlink , and kDefault ."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/programming/photonlib/adding-vendordep.html",
      "title": "Installing PhotonLib",
      "section": "PhotonLib",
      "language": "All",
      "content": "Installing PhotonLib  What is PhotonLib?  PhotonLib is the C++ and Java vendor dependency that accompanies PhotonVision. We created this vendor dependency to make it easier for teams to retrieve vision data from their integrated vision system. PhotonLibPy is a minimal, pure-python implementation of PhotonLib. Online Install - Java/C++  Click on the WPILib logo in the activity bar to access the Vendor Dependencies interface. Select the install button for the “PhotonLib” dependency. Note The Dependency Manager will automatically build your program when it loses focus. This allows you to use the changed dependencies. When an update is available for PhotonLib, a “To Latest” button will become available. This will update the vendordep to the latest version of PhotonLib. Refer to The WPILib docs for more details on installing vendor libraries. Offline Install - Java/C++  Download the latest PhotonLib release from our GitHub releases page (named in the format photonlib-VERSION.zip ), and extract the contents to ~/wpilib/YYYY/vendordeps (where YYYY is the year and ~ is C:\\Users\\Public on Windows). This adds PhotonLib maven artifacts to your local maven repository. PhotonLib will now also appear available in the “install vendor libraries (offline)” menu in WPILib VSCode. Refer to the WPILib docs for more details on installing vendor libraries offline. Install - Python  Add photonlibpy to pyproject.toml . # Other pip packages to install requires = [ \"photonlibpy\" , ] See The WPILib/RobotPy docs for more information on using pyproject.toml. Install Specific Version - Java/C++  In cases where you want to test a specific version of PhotonLib, make sure you have finished the steps in Online Install - Java/C++ and then manually change the version string in the PhotonLib vendordep json file(at /path/to/your/project/vendordep/photonlib.json ) to your desired version.",
      "content_preview": "Installing PhotonLib  What is PhotonLib?  PhotonLib is the C++ and Java vendor dependency that accompanies PhotonVision. We created this vendor dependency to make it easier for teams to retrieve vision data from their integrated vision system."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/programming/photonlib/driver-mode-pipeline-index.html",
      "title": "Driver Mode and Pipeline Index/Latency",
      "section": "PhotonLib",
      "language": "All",
      "content": "Driver Mode and Pipeline Index/Latency  After creating a PhotonCamera , one can toggle Driver Mode and change the Pipeline Index of the vision program from robot code. Toggle Driver Mode  You can use the setDriverMode() / SetDriverMode() (Java and C++ respectively) to toggle driver mode from your robot program. Driver mode is an unfiltered / normal view of the camera to be used while driving the robot. JAVA // Set driver mode to on. camera . setDriverMode ( true ); C++ // Set driver mode to on. camera . SetDriverMode ( true ); PYTHON # Coming Soon! Setting the Pipeline Index  You can use the setPipelineIndex() / SetPipelineIndex() (Java and C++ respectively) to dynamically change the vision pipeline from your robot program. JAVA // Change pipeline to 2 camera . setPipelineIndex ( 2 ); C++ // Change pipeline to 2 camera . SetPipelineIndex ( 2 ); PYTHON # Coming Soon! Getting the Pipeline Latency  You can also get the pipeline latency from a pipeline result using the getLatencyMillis() / GetLatency() (Java and C++ respectively) methods on a PhotonPipelineResult . JAVA // Get the pipeline latency. double latencySeconds = result . getLatencyMillis () / 1000.0 ; C++ // Get the pipeline latency. units :: second_t latency = result . GetLatency (); PYTHON # Coming Soon! Note The C++ version of PhotonLib returns the latency in a unit container. For more information on the Units library, see here .",
      "content_preview": "Driver Mode and Pipeline Index/Latency  After creating a PhotonCamera , one can toggle Driver Mode and change the Pipeline Index of the vision program from robot code."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/programming/photonlib/using-target-data.html",
      "title": "Using Target Data",
      "section": "PhotonLib",
      "language": "All",
      "content": "Using Target Data  A PhotonUtils class with helpful common calculations is included within PhotonLib to aid teams in using AprilTag data in order to get positional information on the field. This class contains two methods, calculateDistanceToTargetMeters() / CalculateDistanceToTarget() and estimateTargetTranslation2d() / EstimateTargetTranslation() (Java and C++ respectively). Estimating Field Relative Pose with AprilTags  estimateFieldToRobotAprilTag(Transform3d cameraToTarget, Pose3d fieldRelativeTagPose, Transform3d cameraToRobot) returns your robot’s Pose3d on the field using the pose of the AprilTag relative to the camera, pose of the AprilTag relative to the field, and the transform from the camera to the origin of the robot. JAVA // Calculate robot's field relative pose if ( aprilTagFieldLayout . getTagPose ( target . getFiducialId ()). isPresent ()) { Pose3d robotPose = PhotonUtils . estimateFieldToRobotAprilTag ( target . getBestCameraToTarget (), aprilTagFieldLayout . getTagPose ( target . getFiducialId ()). get (), cameraToRobot ); } C++ //TODO PYTHON # Coming Soon! Estimating Field Relative Pose (Traditional)  You can get your robot’s Pose2D on the field using various camera data, target yaw, gyro angle, target pose, and camera position. This method estimates the target’s relative position using estimateCameraToTargetTranslation (which uses pitch and yaw to estimate range and heading), and the robot’s gyro to estimate the rotation of the target. JAVA // Calculate robot's field relative pose Pose2D robotPose = PhotonUtils . estimateFieldToRobot ( kCameraHeight , kTargetHeight , kCameraPitch , kTargetPitch , Rotation2d . fromDegrees ( - target . getYaw ()), gyro . getRotation2d (), targetPose , cameraToRobot ); C++ // Calculate robot's field relative pose frc :: Pose2D robotPose = photonlib :: EstimateFieldToRobot ( kCameraHeight , kTargetHeight , kCameraPitch , kTargetPitch , frc :: Rotation2d ( units :: degree_t ( - target . GetYaw ())), frc :: Rotation2d ( units :: degree_t ( gyro . GetRotation2d )), targetPose , cameraToRobot ); PYTHON # Coming Soon! Calculating Distance to Target  If your camera is at a fixed height on your robot and the height of the target is fixed, you can calculate the distance to the target based on your camera’s pitch and the pitch to the target. JAVA // TODO C++ // TODO PYTHON # Coming Soon! Note The C++ version of PhotonLib uses the Units library. For more information, see here . Calculating Distance Between Two Poses  getDistanceToPose(Pose2d robotPose, Pose2d targetPose) allows you to calculate the distance between two poses. This is useful when using AprilTags, given that there may not be an AprilTag directly on the target. JAVA double distanceToTarget = PhotonUtils . getDistanceToPose ( robotPose , targetPose ); C++ //TODO PYTHON # Coming Soon! Estimating Camera Translation to Target  You can get a translation to the target based on the distance to the target (calculated above) and angle to the target (yaw). JAVA // Calculate a translation from the camera to the target. Translation2d translation = PhotonUtils . estimateCameraToTargetTranslation ( distanceMeters , Rotation2d . fromDegrees ( - target . getYaw ())); C++ // Calculate a translation from the camera to the target. frc :: Translation2d translation = photonlib :: PhotonUtils :: EstimateCameraToTargetTranslation ( distance , frc :: Rotation2d ( units :: degree_t ( - target . GetYaw ()))); PYTHON # Coming Soon! Note We are negating the yaw from the camera from CV (computer vision) conventions to standard mathematical conventions. In standard mathematical conventions, as you turn counter-clockwise, angles become more positive. Getting the Yaw To a Pose  getYawToPose(Pose2d robotPose, Pose2d targetPose) returns the Rotation2d between your robot and a target. This is useful when turning towards an arbitrary target on the field (ex. the center of the hub in 2022). JAVA Rotation2d targetYaw = PhotonUtils . getYawToPose ( robotPose , targetPose ); C++ //TODO PYTHON # Coming Soon!",
      "content_preview": "Using Target Data  A PhotonUtils class with helpful common calculations is included within PhotonLib to aid teams in using AprilTag data in order to get positional information on the field."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/index.html",
      "title": "Contributing to PhotonVision Projects",
      "section": "Contributing",
      "language": "All",
      "content": "Contributing to PhotonVision Projects  Build Instructions Development Setup Prerequisites Compiling Instructions Getting the Source Code Install Necessary Node JS Dependencies Using hot reload on the UI Build and Run PhotonVision Build and Run PhotonVision on a Raspberry Pi Coprocessor Using PhotonLib Builds VSCode Test Runner Extension Running Tests With UI Debugging PhotonVision Running Locally Debugging PhotonVision Running on a CoProcessor Running examples Running C++/Java Running Python Downloading Pipeline Artifacts MacOS Builds Forcing Object Detection in the UI Building the PhotonVision Documentation Cloning the Documentation Repository Installing Python Dependencies Building the Documentation Opening the Documentation Docs Builds on Pull Requests Style Guide Linting the PhotonVision Codebase Versions Frontend Linting the frontend Backend wpiformat installation Linting the backend Documentation doc8 installation Linting the documentation Alias PhotonVision Developer Documentation Photonlib Developer Docs Backing up using Rsync Software Architecture Design Descriptions Calibration and Image Rotation Rotating Points Image Distortion Time Synchronization Protocol Specification, Version 1.0 Background Prior Art Roles Transport Message Format TSP Ping TSP Pong Optional Protocol Extensions Camera Matching Initial Setup UI Workflow Activate New Camera Deactivate Camera Reactivate a CameraConfig Camera Matching Requirements Definitions Startup: Camera (re)enumeration: Creating from a new camera Deactivate Reactivate Latency Characterization A primer on time CSCore’s Frame Time Latency Testing Test Setup Test Results Future Work",
      "content_preview": "Contributing to PhotonVision Projects  Build Instructions Development Setup Prerequisites Compiling Instructions Getting the Source Code Install Necessary Node JS Dependencies Using hot reload on the UI Build and Run PhotonVision Build and Run PhotonVision on a Raspberry Pi Coprocessor Using..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/design-descriptions/image-rotation.html",
      "title": "Calibration and Image Rotation",
      "section": "Contributing",
      "language": "All",
      "content": "Calibration and Image Rotation  Rotating Points  To stay consistent with the OpenCV camera coordinate frame, we put the origin in the top left, with X right, Y down, and Z out (as required by the right-hand rule). Intuitively though, if I ask you to rotate an image 90 degrees clockwise though, you’d probably rotate it about -Z in this coordinate system. Just be aware of this inconsistency. If we have any one point in any of those coordinate systems, we can transform it into any of the other ones using standard geometry libraries by performing relative transformations (like in this pseudocode): Translation2d tag_corner1 = new Translation2d (); Translation2d rotated = tag_corner1 . relativeTo ( ORIGIN_ROTATED_90_CCW ); Image Distortion  The distortion coefficients for OPENCV8 is given in order [k1 k2 p1 p2 k3 k4 k5 k6] . Mrcal names these coefficients [k_0 k_1, k_2, k_3, k_4, k_5, k_6, k_7] . \\[\\begin{split} \\begin{align*} \\vec P &\\equiv \\frac{\\vec p_{xy}}{p_z} \\\\ r &\\equiv \\left|\\vec P\\right| \\\\ \\vec P_\\mathrm{radial} &\\equiv \\frac{ 1 + k_0 r^2 + k_1 r^4 + k_4 r^6}{ 1 + k_5 r^2 + k_6 r^4 + k_7 r^6} \\vec P \\\\ \\vec P_\\mathrm{tangential} &\\equiv \\left[ \\begin{aligned} 2 k_2 P_0 P_1 &+ k_3 \\left(r^2 + 2 P_0^2 \\right) \\\\ 2 k_3 P_0 P_1 &+ k_2 \\left(r^2 + 2 P_1^2 \\right) \\end{aligned}\\right] \\\\ \\vec q &= \\vec f_{xy} \\left( \\vec P_\\mathrm{radial} + \\vec P_\\mathrm{tangential} \\right) + \\vec c_{xy} \\end{align*}\\end{split}\\] From this, we observe at k_0, k_1, k_4, k_5, k_6, k_7 depend only on the norm of \\(\\vec P\\) , and will be constant given a rotated image. However, k_2 and k_3 go with \\(P_0 \\cdot P_1\\) , k_3 with \\(P_0^2\\) , and k_2 with \\(P_1^2\\) . Let’s try a concrete example. With a 90 degree CCW rotation, we have \\(P0=-P_{1\\mathrm{rotated}}\\) and \\(P1=P_{0\\mathrm{rotated}}\\) . Let’s substitute in \\[\\begin{split} \\begin{align*} \\left[ \\begin{aligned} 2 k_2 P_0 P_1 &+ k_3 \\left(r^2 + 2 P_0^2 \\right) \\\\ 2 k_3 P_0 P_1 &+ k_2 \\left(r^2 + 2 P_1^2 \\right) \\end{aligned}\\right] &= \\left[ \\begin{aligned} 2 k_{2\\mathrm{rotated}} (-P_{1\\mathrm{rotated}}) P_{0\\mathrm{rotated}} &+ k_{3\\mathrm{rotated}} \\left(r^2 + 2 (-P_{1\\mathrm{rotated}})^2 \\right) \\\\ 2 k_{3\\mathrm{rotated}} (-P_{1\\mathrm{rotated}}) P_{0\\mathrm{rotated}} &+ k_{2\\mathrm{rotated}} \\left(r^2 + 2 P_{0\\mathrm{rotated}}^2 \\right) \\end{aligned}\\right] \\\\ &= \\left[ \\begin{aligned} -2 k_{2\\mathrm{rotated}} P_{1\\mathrm{rotated}} P_{0\\mathrm{rotated}} &+ k_{3\\mathrm{rotated}} \\left(r^2 + 2 P_{1\\mathrm{rotated}}^2 \\right) \\\\ -2 k_{3\\mathrm{rotated}} P_{1\\mathrm{rotated}} P_{0\\mathrm{rotated}} &+ k_{2\\mathrm{rotated}} \\left(r^2 + 2 P_{0\\mathrm{rotated}}^2 \\right) \\end{aligned}\\right] \\end{align*}\\end{split}\\] By inspection, this results in just applying another 90 degree rotation to the k2/k3 parameters. Proof is left as an exercise for the reader. Note that we can repeat this rotation to yield equations for tangential distortion for 180 and 270 degrees. \\[ k_2'=-k_3 k_3'=k_2\\]",
      "content_preview": "Calibration and Image Rotation  Rotating Points  To stay consistent with the OpenCV camera coordinate frame, we put the origin in the top left, with X right, Y down, and Z out (as required by the right-hand rule)."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/developer-docs/photonlib-backups.html",
      "title": "Photonlib Developer Docs",
      "section": "Contributing",
      "language": "All",
      "content": "Photonlib Developer Docs  Our maven server is located at https://maven.photonvision.org/#/. This server runs Reposilite in Docker, and uses Caddy for serving requests. Backing up using Rsync  The Clarkson Open Source Institute at Clarkson University provides a mirror of our artifacts available online . Learn more about them at their homepage . Artifacts from our Maven server can also be backed up locally to a folder called photonlib-backup using the following command, which excludes “snapshots” for space reasons: rsync - avzrHy -- no - perms -- no - group -- no - owner -- ignore - errors -- exclude \".~tmp~\" -- exclude \"snapshots/org/photonvision/photontargeting*\" \\ -- exclude \"snapshots/org/photonvision/photonlib*\" maven . photonvision . org :: reposilite - data \\ / path / to / photonlib - backup",
      "content_preview": "Photonlib Developer Docs  Our maven server is located at https://maven.photonvision.org/#/. This server runs Reposilite in Docker, and uses Caddy for serving requests."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/design-descriptions/index.html",
      "title": "Software Architecture Design Descriptions",
      "section": "Contributing",
      "language": "All",
      "content": "Software Architecture Design Descriptions  Calibration and Image Rotation Time Synchronization Protocol Specification, Version 1.0 Camera Matching Camera Matching Requirements Latency Characterization",
      "content_preview": "Software Architecture Design Descriptions  Calibration and Image Rotation Time Synchronization Protocol Specification, Version 1.0 Camera Matching Camera Matching Requirements Latency Characterization"
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/design-descriptions/camera-matching.html",
      "title": "Camera Matching",
      "section": "Contributing",
      "language": "All",
      "content": "Camera Matching  Diagrams generated by the PlantUML UML editor . Copy the image URLs below and decode in the editor to make changes. Initial Setup  When PhotonVision first starts, settings are loaded from disk and VisionSources are created for every serialized & active Camera Configuration UI Workflow  A background thread will periodically query CSCore and Libcamera for what cameras we currently see connected. This list is provided to the web UI for display. This UI allows users to “Activate” a camera that’s never been seen before, or activate a CameraConfiguration we’ve seen before but was disabled. Allowing camera configurations to be saved but not loaded by default lets us support temporarily disabling/unplugging a camera without flooding log files. Since our backend logic intentionally does not protect users from plugging camera B into the port that camera A was active on, the UI shall show a warning but vision processing will (attempt to) continue like normal. Activate New Camera  When a new camera (ie, one we can’t match by-path to a deserialized CameraConfiguration) is activated, we’ll create a spin up a new Vision Module for it Deactivate Camera  Deactivating a camera will release the native resources it owns, and return the CameraConfiguration to the pool of currently disabled cameras we can re-enable later. Reactivate a CameraConfig  When a new camera (ie, one we can’t match by-path to a deserialized CameraConfiguration) is activated, we’ll create and spin up a new Vision Module for it. Camera Matching Requirements  Definitions  VALID USB PATH: a path in the form /dev/v4l/by-path/[UUID] VIDEO DEVICE PATH: a CSCore-provided identifier derived from the V4L path /dev/video[N] on Linux, or an opaque string on Windows UNIQUE NAME: an identifier that is unique within the set of all deserialized CameraConfigurations and unmatched USB cameras I don’t love this, it means that a USB camera matched to a VisionModule will share a UNIQUE NAME, right? DESERIALIZED CAMERA CONFIGURATIONS: The set of camera configurations loaded from disk and provided to the VisionSourceManager. This configuration data structure includes the UNIQUE NAME CURRENTLY ACTIVE CAMERAS: The set of VisionModules currently active and processing vision data, and associated metadata Startup:  GIVEN An empty set of deserialized Camera Configurations WHEN PhotonVision starts THEN no VisionModules will be started GIVEN A valid set of deserialized Camera Configurations WHEN PhotonVision starts THEN VisionModules will be started FOR EACH un-DISABLED config GIVEN A valid set of deserialized Camera Configurations WHEN PhotonVision starts THEN VisionModules will NOT be started FOR EACH DISABLED config GIVEN A CameraConfiguration with a VALID USB PATH WHEN a VisionModule is created THEN The VisionModule shall open the camera using the USB path GIVEN A CameraConfiguration without a valid USB path WHEN a VisionModule is created THEN The VisionModule shall open the camera using the VIDEO DEVICE PATH Camera (re)enumeration:  GIVEN a NEW USB CAMERA is available for enumeration WHEN a USB camera is discovered by VisionSourceManager AND the USB camera’s VIDEO DEVICE PATH is not in the set of DESERIALIZED CAMERA CONFIGURATIONS THEN a UNIQUE NAME will be assigned to the camera info GIVEN a NEW USB CAMERA is available for enumeration WHEN a USB camera is discovered by VisionSourceManager AND the USB camera’s VIDEO DEVICE PATH is in the set of DESERIALIZED CAMERA CONFIGURATIONS THEN a UNIQUE NAME equal to the matching DESERIALIZED CAMERA CONFIGURATION will be assigned to the camera info This is a weird case. How -should- we handle this? see above Creating from a new camera  Given: A UNIQUE NAME from a NEW USB CAMERA WHEN I request a new VisionModule is created for this NEW USB CAMERA AND the camera has a VALID USB PATH AND the camera’s VALID USB PATH is not in use by any CURRENTLY ACTIVE CAMERAS THEN a NEW VisionModule will be started for the NEW USB CAMERA using the VALID USB PATH Given: A UNIQUE NAME from a NEW USB CAMERA WHEN I request a new VisionModule is created for this NEW USB CAMERA AND the camera does not have a VALID USB PATH AND the camera’s VIDEO DEVICE PATH is not in use by any CURRENTLY ACTIVE CAMERAS THEN a NEW VisionModule will be started for the NEW USB CAMERA using the VIDEO DEVICE PATH Deactivate  Given: A UNIQUE NAME from a CURRENTLY ACTIVE CAMERA WHEN I request the VisionModule be DEACTIVATED THEN the VisionModule will be stopped for the given CURRENTLY ACTIVE CAMERA AND the CameraConfiguration DISABLED flag will be set to TRUE Reactivate  Given: A UNIQUE NAME from a DESERIALIZED CAMERA CONFIGURATIONS WHEN I request the VisionModule be ACTIVATED AND the CameraConfiguration’s DISABLED flag is TRUE THEN a VisionModule will be created and started for the camera",
      "content_preview": "Camera Matching  Diagrams generated by the PlantUML UML editor . Copy the image URLs below and decode in the editor to make changes. Initial Setup  When PhotonVision first starts, settings are loaded from disk and VisionSources are created for every serialized & active Camera Configuration UI..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/design-descriptions/e2e-latency.html",
      "title": "Latency Characterization",
      "section": "Contributing",
      "language": "All",
      "content": "Latency Characterization  A primer on time  Especially starting around 2022 with AprilTags making localization easier, providing a way to know when a camera image was captured at became more important for localization. Since the creation of USBFrameProvider , we used the time provided by CSCore to tell when a camera image was captured at, but just keeping track of “CSCore told us frame N was captured 104.21s after the Raspberry Pi turned on” isn’t very helpful. We can decompose this into asking: At what time was a particular image captured at, in the coprocessor’s timebase? How do I convert a time in a coprocessor’s timebase into the RoboRIO’s timebase, so I can integrate the measurement with my other sensor measurements (like encoders)? The first one seems easy - CSCore tells us the time, so just keep track of that? Should be easy. For the second, translating this time, as measured by the coprocessor’s clock, into a timebase also used by user code on the RoboRIO, is actually a fairly hard problem that involved reinventing PTP . And on latency vs timestamps - PhotonVision has exposed a magic “latency” number since forever, but latency (as in, the time from image capture to acting on data) can be useful for benchmarking code, but robots actually want to answer “what time was this image from, relative to “? CSCore’s Frame Time  WPILib’s CSCore is a platform-agnostic wrapper around Windows, Linux, and MacOS camera APIs. On Linux, CSCore uses Video4Linux to access USB Video Class (UVC) devices like webcams, as well as CSI cameras on some platforms. At a high level, CSCore’s Linux USB Camera driver works by: Opening a camera with open Creating and mmap ing a handful of buffers V4L will fill with frame data into program memory Asking V4L to start streaming While the camera is running: Wait for new frames Dequeue one buffer Call SourceImpl::PutFrame , which will copy the image out and convert as needed Return the buffer to V4L to fill again Prior to https://github.com/wpilibsuite/allwpilib/pull/7609, CSCore used the time it dequeued the buffer at as the image capture time. But this doesn’t account for exposure time or latency introduced by the camera + USB stack + Linux itself. V4L does expose (with some very heavy caveats for some troublesome cameras) its best guess at the time an image was captured at via buffer flags . In my testing, all my cameras were able to provide timestamps with both these flags set: V4L2_BUF_FLAG_TIMESTAMP_MONOTONIC : The buffer timestamp has been taken from the CLOCK_MONOTONIC clock […] accessible via clock_gettime() . V4L2_BUF_FLAG_TSTAMP_SRC_SOE : Start Of Exposure. The buffer timestamp has been taken when the exposure of the frame has begun. I’m sure that we’ll find a camera that doesn’t play nice, because we can’t have nice things :). But until then, using this timestamp gets us a free accuracy bump. Other things to note: This gets us an estimate at when the camera started collecting photons. The camera’s sensor will remain collecting light for up to the total integration time, plus readout time for rolling shutter cameras. Latency Testing  Here, I’ve got a RoboRIO with an LED, an Orange Pi 5, and a network switch on a test bench. The LED is assumed to turn on basically instantly once we apply current, and based on DMA testing, the total time to switch a digital output on is on the order of 10uS. The RoboRIO is running a TimeSync Server, and the Orange Pi is running a TimeSync Client. Test Setup  Show RoboRIO Test Code package frc.robot ; import org.photonvision.PhotonCamera ; import edu.wpi.first.wpilibj.DigitalOutput ; import edu.wpi.first.wpilibj.TimedRobot ; import edu.wpi.first.wpilibj.Timer ; import edu.wpi.first.wpilibj.smartdashboard.SmartDashboard ; public class Robot extends TimedRobot { PhotonCamera camera ; DigitalOutput light ; @Override public void robotInit () { camera = new PhotonCamera ( \"Arducam_OV9782_USB_Camera\" ); light = new DigitalOutput ( 0 ); light . set ( false ); } @Override public void robotPeriodic () { super . robotPeriodic (); try { light . set ( false ); for ( int i = 0 ; i < 50 ; i ++ ) { Thread . sleep ( 20 ); camera . getAllUnreadResults (); } var t1 = Timer . getFPGATimestamp (); light . set ( true ); var t2 = Timer . getFPGATimestamp (); for ( int i = 0 ; i < 100 ; i ++ ) { for ( var result : camera . getAllUnreadResults ()) { if ( result . hasTargets ()) { var t3 = result . getTimestampSeconds (); var t1p5 = ( t1 + t2 ) / 2 ; var error = t3 - t1p5 ; SmartDashboard . putNumber ( \"blink_error_ms\" , error * 1000 ); return ; } } Thread . sleep ( 20 ); } } catch ( InterruptedException e ) { e . printStackTrace (); } } } I’ve decreased camera exposure as much as possible (so we know with reasonable confidence that the image was collected right at the start of the exposure time reported by V4L), but we only get back new images at 60fps. So we don’t know when between frame N and N+1 the LED turned on - just that sometime between now and 1/60th of a second a go, the LED turned on. The test coprocessor was an Orange Pi 5 running a PhotonVision 2025 (Ubuntu 24.04 based) image, with an ArduCam OV9782 at 1280x800, 60fps, MJPG running a reflective pipeline. Test Results  The videos above show the difference between when the RoboRIO turned the LED on and when PhotonVision first seeing a camera frame with the LED on, what I’ve called error and plotted in yellow with units of seconds. This error decreases when I use the frame time reported by V4L from a mean delta of 26 ms to a mean delta of 11 ms (below the maximum temporal resolution of my camera). Old CSCore: Your browser does not support the video tag. CSCore using V4L frame time: Your browser does not support the video tag. With the camera capturing at 60fps, the time between successive frames is only ~16.7 ms, so I don’t expect to be able to resolve anything smaller. Given sufficient time and with perfect latency compensation, and with more noise in the robot program to make sure we vary LED toggle times, I’d expect the error to converge to ~half the interval between frames - so being within this frame interval with CSCore updates is a very good sign. Future Work  This test also makes no effort to isolate error from time synchronization from error introduced by frame time measurement - we’re just interested in overall error. Future work could investigate the latency contribution",
      "content_preview": "Latency Characterization  A primer on time  Especially starting around 2022 with AprilTags making localization easier, providing a way to know when a camera image was captured at became more important for localization."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/design-descriptions/time-sync.html",
      "title": "Time Synchronization Protocol Specification, Version 1.0",
      "section": "Contributing",
      "language": "All",
      "content": "Time Synchronization Protocol Specification, Version 1.0  Protocol Revision 1.0, 08/25/2024 Background  In a distributed compute environment like robots, time synchronization between computers is increasingly important. Currently, NetworkTables Version 4.1 provides support for time synchronization of clients with the NetworkTables server using binary PING/PONG messages sent over WebSockets. This approach, while fundamentally the same as is described in this memo, has demonstrated some opportunities for improvement: PING/PONG messages are processed in the same queue as other NetworkTables messages. Depending on the underlying implementation and processor speed, this can incur message processing delays and increase client-calculated Round-Trip Time (RTT), and cause messages to arrive at the server timestamped in the future. Messages use WebSockets over TCP for their transport layer. We don’t need the robustness guarantees of TCP as our connection is stateless. For these reasons, a time synchronization solution separate from NetworkTables communication was desired. Architecture decisions made to address these issues are: Use the User Datagram Protocol (UDP) transport layer, as we don’t need the robustness guarantees afforded by TCP. As a Client, if a PING isn’t replied to, we’ll just try again at the start of the next PING window. As a bonus, we are free to use UDP port 5810 as NetworkTables only uses TCP Port 5810/5811 as of Version 4.1. Use a separate thread from the current NetworkTables libUV runner. Prior Art  The NetworkTables 4.1 timestamp synchronization approach, an implementation of Cristian’s Algorithm . We also implement Cristian’s Algorithm. The Precision Time Protocol at it’s core does something similar with Sync/Delay_Req/Delay_Resp. We do not have (guaranteed) access to hardware timestamping, but we utilize this PING/PONG pattern to estimate total round-trip time. Roles  Time Synchronization Protocol (TSP) participants can assume either a server role or a client role. The server role is responsible for listening for incoming time synchronization requests from clients and replying appropriately. The client role is responsible for sending “Ping” messages to the server and listening for “Pong” replies to estimate the offset between the server and client time bases. All time values shall use units of microseconds. The epoch of the time base this is measured against is unspecified. Clients shall periodically (e.g. every few seconds) send, in a manner that minimizes transmission delays, a TSP Ping Message that contains the client’s current local time. When the server receives a TSP Ping Message from any client, it shall respond to the client, in a manner that minimizes transmission delays, with a TSP Pong message encoding a timestamp of its (the server’s) current local time (in microseconds), and the client-provided data value. When the client receives a TSP Pong Message from the server, it shall verify that the Client Local Time corresponds to the currently in-flight TSP Ping message; if not, it shall drop this packet. The round trip time (RTT) shall be computed from the delta between the message’s data value and the current local time. If the RTT is less than that from previous measurements, the client shall use the timestamp in the message plus ½ the RTT as the server time equivalent to the current local time, and use this equivalence to compute server time base timestamps from local time for future messages. Transport  Communication between server and clients shall occur over the User Datagram Protocol (UDP) Port 5810. Message Format  The message format forgoes CRCs (as these are provided by the Ethernet physical layer) or packet delineation (as our packets are assumed be under the network MTU). TSP Ping and TSP Pong messages shall be encoded in a manor compatible with a WPILib packed struct with respect to byte alignment and endianness. TSP Ping  Offset Format Data Notes 0 uint8 Protocol version This field shall always set to 1 (0b1) for TSP Version 1. 1 uint8 Message ID This field shall always be set to 1 (0b1). 2 uint64 Client Local Time The client’s local time value, at the time this Ping message was sent. TSP Pong  Offset Format Data Notes 0 uint8 Protocol version This field shall always set to 1 (0b1) for TSP Version 1. 1 uint8 Message ID This field shall always be set to 2 (0b2). 2 uint64 Client Local Time The client’s local time value from the Ping message that this Pong is generated in response to. 10 uint64 Server Local Time The current time at the server, at the time this Pong message was sent. Optional Protocol Extensions  Clients may publish statistics to NetworkTables. If they do, they shall publish to a key that is globally unique per participant in the Time Synchronization network. If a client implements this, it shall provide the following publishers: Key Type Notes offset_us Integer The time offset that, when added to the client’s local clock, provides server time ping_tx_count Integer The total number of TSP Ping packets transmitted ping_rx_count Integer The total number of TSP Ping packets received pong_rx_time_us Integer The time, in client local time, that the last pong was received rtt2_us Integer The time in us from last complete (ping transmission to pong reception) PhotonVision has chosen to publish to the sub-table /photonvision/.timesync/{DEVICE_HOSTNAME} . Future implementations of this protocol may decide to implement this as a structured data type.",
      "content_preview": "Time Synchronization Protocol Specification, Version 1.0  Protocol Revision 1.0, 08/25/2024 Background  In a distributed compute environment like robots, time synchronization between computers is increasingly important."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/building-photon.html",
      "title": "Build Instructions",
      "section": "Contributing",
      "language": "All",
      "content": "Build Instructions  This section contains the build instructions from the source code available at our GitHub page . Development Setup  Prerequisites  Java Development Kit: This project requires Java Development Kit (JDK) 17 to be compiled. This is the same Java version that comes with WPILib for 2025+. Windows Users must use the JDK that ships with WPILib. For other platforms, you can follow the instructions to install JDK 17 for your platform here . Node JS: The UI is written in Node JS. To compile the UI, Node 22 or later is required. To install Node JS, follow the instructions for your platform on the official Node JS website . pnpm: pnpm is the package manager used to download dependencies for the UI. To install pnpm, follow the instructions on the official pnpm website . Cross-Compilation Toolchains (Optional): If you plan to deploy PhotonVision to a coprocessor like a Raspberry Pi, you will need to install the appropriate cross-compilation toolchain for your platform. For linuxarm64 devices, this can be accomplished by running ./gradlew installArm64Toolchain in the root folder of the project. Compiling Instructions  Getting the Source Code  Get the source code from git: git clone https://github.com/PhotonVision/photonvision or alternatively download the source code from GitHub and extract the zip: Install Necessary Node JS Dependencies  In the photon-client directory: pnpm install Using hot reload on the UI  In the photon-client directory: pnpm run dev This allows you to make UI changes quickly without having to spend time rebuilding the jar. Hot reload is enabled, so changes that you make and save are reflected in the UI immediately. Running this command will give you the URL for accessing the UI, which is on a different port than normal. You must use the printed URL to use hot reload. Build and Run PhotonVision  To compile and run the project, issue the following command in the root directory: Linux ./gradlew run macOS ./gradlew run Windows (cmd) gradlew run Running the following command under the root directory will build the jar under photon-server/build/libs : Linux ./gradlew shadowJar macOS ./gradlew shadowJar Windows (cmd) gradlew shadowJar Build and Run PhotonVision on a Raspberry Pi Coprocessor  As a convenience, the build has a built-in deploy command which builds, deploys, and starts the current source code on a coprocessor. It uses deploy-utils , so it works very similarly to deploys on robot projects. An architecture override is required to specify the deploy target’s architecture. Linux ./gradlew clean ./gradlew deploy -PArchOverride=linuxarm64 macOS ./gradlew clean ./gradlew deploy -PArchOverride=linuxarm64 Windows (cmd) gradlew clean gradlew deploy -PArchOverride=linuxarm64 The deploy command is tested against Raspberry Pi coprocessors. Other similar coprocessors may work too. Using PhotonLib Builds  The build process automatically generates a vendordep JSON of your local build at photon-lib/build/generated/vendordeps/photonlib.json . The photonlib source can be published to your local maven repository after building: Linux ./gradlew publishToMavenLocal macOS ./gradlew publishToMavenLocal Windows (cmd) gradlew publishToMavenLocal After adding the generated vendordep to your project, add the following to your project’s build.gradle under the plugins {} block. repositories { mavenLocal () } VSCode Test Runner Extension  With the VSCode Extension Pack for Java , you can get the Test Runner for Java and Gradle for Java extensions. This lets you easily run specific tests through the IDE: To correctly run PhotonVision tests this way, you must delegate the tests to Gradle . Debugging tests like this will not currently collect outputs. Running Tests With UI  By default, tests are run with UI disabled so they are not obtrusive during a build. All tests should be useful when the UI is disabled. However, if a particular test would benefit from having UI access (i.e. for debugging info), the UI can be enabled by passing the enableTestUi project property to Gradle. This will run all tests by default, but the Gradle --tests option can be used to filter for specific tests . Linux ./gradlew test -PenableTestUi macOS ./gradlew test -PenableTestUi Windows (cmd) gradlew test -PenableTestUi Debugging PhotonVision Running Locally  Unit tests can instead be debugged through the test Gradle task for a specific subproject in VSCode, found in the Gradle tab: However, this will run all tests in a subproject. Similarly, a local instance of PhotonVision can be debugged in the same way using the Gradle run task. In both cases, additional arguments can be specified: Debugging PhotonVision Running on a CoProcessor  Set up a VSCode configuration in launch.json { // Use IntelliSense to learn about possible attributes. // Hover to view descriptions of existing attributes. // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387 \"version\" : \"0.2.0\" , \"configurations\" : [ { \"type\" : \"java\" , \"name\" : \"Attach to CoProcessor\" , \"request\" : \"attach\" , \"hostName\" : \"photonvision.local\" , \"port\" : \"5801\" , \"projectName\" : \"photon-core\" }, ] } Stop any existing instance of PhotonVision. Launch the program with the following additional argument to the JVM: java -jar -agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=*:5801 photonvision.jar Once the program says it is listening on port 5801, launch the debug configuration in VSCode. The program will wait for the VSCode debugger to attach before proceeding. Running examples  You can run one of the many built in examples straight from the command line, too! They contain a fully featured robot project, and some include simulation support. The projects can be found inside the photonlib-*-examples subdirectories for each language. Running C++/Java  PhotonLib must first be published to your local maven repository. This will also copy the generated vendordep json file into each example. After that, the simulateJava/simulateNative task can be used like a normal robot project. Robot simulation with attached debugger is technically possible by using simulateExternalJava and modifying the launch script it exports, though not yet supported. ~/photonvision$ ./gradlew publishToMavenLocal ~/photonvision$ cd photonlib-java-examples ~/photonvision/photonlib-java-examples$ ./gradlew <example-name>:simulateJava ~/photonvision$ cd photonlib-cpp-examples ~/photonvision/photonlib-cpp-examples$ ./gradlew <example-name>:simulateNative Running Python  PhotonLibPy must first be built into a wheel. > cd photon - lib / py > buildAndTest . bat Then, you must enable using the development wheels. robotpy will use pip behind the scenes, and this bat file tells pip about your development artifacts. Note: This is best done in a virtual environment. > enableUsingDevBuilds . bat Then, run the examples: > cd photonlib - python - examples > run . bat < example name > Downloading Pipeline Artifacts  Using the GitHub CLI , we can download artifacts from pipelines by run ID and name: ~/photonvision$ gh run download 11759699679 -n jar-Linux MacOS Builds  MacOS builds are not published to releases as MacOS is not an officially supported platform. However, MacOS builds are still available from the MacOS build action, which can be found here . Forcing Object Detection in the UI  In order to force the Object Detection interface to be visible, it’s necessary to hardcode the platform that Platform.java returns. This can be done by changing the function that detects the RK3588S/QCS6490 platform to always return true, and changing the getCurrentPlatform() function to always return the RK3588S/QCS6490 architecture. Alternatively, it’s possible to modify the frontend code by changing all instances of useSettingsStore().general.supportedBackends.length > 0 to true , which will force the card to render. Make sure to revert these changes before submitting a Pull Request.",
      "content_preview": "Build Instructions  This section contains the build instructions from the source code available at our GitHub page . Development Setup  Prerequisites  Java Development Kit: This project requires Java Development Kit (JDK) 17 to be compiled."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/linting.html",
      "title": "Linting the PhotonVision Codebase",
      "section": "Contributing",
      "language": "All",
      "content": "Linting the PhotonVision Codebase  Versions  Note If you work on other projects that use different versions of the same linters as PhotonVision, you may find it beneficial to use a venv instead of installing the linters globally. This will allow you to have different versions of the same linter installed for different projects. The correct versions for each linter can be found under the linting workflow located here . For doc8 , the version can be found in docs/requirements.txt . If you’ve linted, and are still unable to pass CI, please check the versions of your linters. Frontend  Linting the frontend  In order to lint the frontend, run pnpm -C photon-client lint && pnpm -C photon-client format . This should be done from the base level of the repo. Backend  wpiformat installation  To lint the backend, PhotonVision uses wpiformat and spotless . Spotless is included with gradle, which means installation is not needed. To install wpiformat, run pipx install wpiformat . To install a specific version, run pipx install wpiformat==<version> . Linting the backend  To lint, run ./gradlew spotlessApply and wpiformat . Documentation  doc8 installation  To install doc8 , the python tool we use to lint our documentation, run pipx install doc8 . To install a specific version, run pipx install doc8==<version> . Linting the documentation  To lint the documentation, run doc8 docs from the root level of the docs. Alias  The following alias can be added to your shell config, which will allow you to lint the entirety of the PhotonVision project by running pvLint . The alias will work on Linux, macOS, Git Bash on Windows, and WSL. alias pvLint = 'wpiformat -v && ./gradlew spotlessApply && pnpm -C photon-client lint && pnpm -C photon-client format && doc8 docs'",
      "content_preview": "Linting the PhotonVision Codebase  Versions  Note If you work on other projects that use different versions of the same linters as PhotonVision, you may find it beneficial to use a venv instead of installing the linters globally."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/contributing/building-docs.html",
      "title": "Building the PhotonVision Documentation",
      "section": "Contributing",
      "language": "All",
      "content": "Building the PhotonVision Documentation  To build the PhotonVision documentation, you will require Git and Python 3.6 or greater . Cloning the Documentation Repository  Documentation lives within the main PhotonVision repository within the docs sub-folder. If you are planning on contributing, it is recommended to create a fork of the PhotonVision repository . To clone this fork, run the following command in a terminal window: git clone https://github.com/[your username]/photonvision Installing Python Dependencies  You must install a set of Python dependencies in order to build the documentation. To do so, you can run the following command in the docs sub-folder: ~/photonvision/docs$ python -m pip install -r requirements.txt Building the Documentation  In order to build the documentation, you can run the following command in the docs sub-folder. This will automatically build docs every time a file changes, and serves them locally at localhost:8000 by default. ~/photonvision/docs$ sphinx-autobuild --open-browser source source/_build/html Opening the Documentation  The built documentation is located at docs/build/html/index.html relative to the root project directory, or can be accessed via the local web server if using sphinx-autobuild. Docs Builds on Pull Requests  Pre-merge builds of docs can be found at: https://photonvision-docs--PRNUMBER.org.readthedocs.build/en/PRNUMBER/index.html . These docs are republished on every commit to a pull request made to PhotonVision/photonvision-docs. For example, PR 325 would have pre-merge documentation published to https://photonvision-docs--325.org.readthedocs.build/en/325/index.html . Additionally, the pull request will have a link directly to the pre-release build of the docs. This build only runs when there is a change to files in the docs sub-folder. Style Guide  PhotonVision follows the frc-docs style guide which can be found here . In order to run the linter locally (which builds on doc8 and checks for compliance with the style guide), follow the instructions on GitHub .",
      "content_preview": "Building the PhotonVision Documentation  To build the PhotonVision documentation, you will require Git and Python 3.6 or greater . Cloning the Documentation Repository  Documentation lives within the main PhotonVision repository within the docs sub-folder."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/examples/index.html",
      "title": "Code Examples",
      "section": "Code Examples",
      "language": "All",
      "content": "Code Examples  Aiming at a Target Combining Aiming and Getting in Range Using WPILib Pose Estimation, Simulation, and PhotonVision Together",
      "content_preview": "Code Examples  Aiming at a Target Combining Aiming and Getting in Range Using WPILib Pose Estimation, Simulation, and PhotonVision Together"
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/examples/aimingatatarget.html",
      "title": "Aiming at a Target",
      "section": "Code Examples",
      "language": "All",
      "content": "Aiming at a Target  The following example is from the PhotonLib example repository ( Java ). Knowledge and Equipment Needed  A Robot A camera mounted rigidly to the robot’s frame, centered and pointed forward. A coprocessor running PhotonVision with an AprilTag or ArUco 2D Pipeline. A printout of AprilTag 7 , mounted on a rigid and flat surface. Code  Now that you have properly set up your vision system and have tuned a pipeline, you can now aim your robot at an AprilTag using the data from PhotonVision. The yaw of the target is the critical piece of data that will be needed first. Yaw is reported to the roboRIO over Network Tables. PhotonLib, our vendor dependency, is the easiest way to access this data. The documentation for the Network Tables API can be found here and the documentation for PhotonLib here . In this example, while the operator holds a button down, the robot will turn towards the AprilTag using the P term of a PID loop. To learn more about how PID loops work, how WPILib implements them, and more, visit Advanced Controls (PID) and PID Control in WPILib . Java 77 @Override 78 public void teleopPeriodic () { 79 // Calculate drivetrain commands from Joystick values 80 double forward = - controller . getLeftY () * Constants . Swerve . kMaxLinearSpeed ; 81 double strafe = - controller . getLeftX () * Constants . Swerve . kMaxLinearSpeed ; 82 double turn = - controller . getRightX () * Constants . Swerve . kMaxAngularSpeed ; 83 84 // Read in relevant data from the Camera 85 boolean targetVisible = false ; 86 double targetYaw = 0.0 ; 87 var results = camera . getAllUnreadResults (); 88 if ( ! results . isEmpty ()) { 89 // Camera processed a new frame since last 90 // Get the last one in the list. 91 var result = results . get ( results . size () - 1 ); 92 if ( result . hasTargets ()) { 93 // At least one AprilTag was seen by the camera 94 for ( var target : result . getTargets ()) { 95 if ( target . getFiducialId () == 7 ) { 96 // Found Tag 7, record its information 97 targetYaw = target . getYaw (); 98 targetVisible = true ; 99 } 100 } 101 } 102 } 103 104 // Auto-align when requested 105 if ( controller . getAButton () && targetVisible ) { 106 // Driver wants auto-alignment to tag 7 107 // And, tag 7 is in sight, so we can turn toward it. 108 // Override the driver's turn command with an automatic one that turns toward the tag. 109 turn = - 1.0 * targetYaw * VISION_TURN_kP * Constants . Swerve . kMaxAngularSpeed ; 110 } 111 112 // Command drivetrain motors based on target speeds 113 drivetrain . drive ( forward , strafe , turn ); 114 115 // Put debug information to the dashboard 116 SmartDashboard . putBoolean ( \"Vision Target Visible\" , targetVisible ); 117 } C++ (Header) 25 #pragma once 26 27 #include <photon/PhotonCamera.h> 28 29 #include <frc/TimedRobot.h> 30 #include <frc/XboxController.h> 31 32 #include \"Constants.h\" 33 #include \"VisionSim.h\" 34 #include \"subsystems/SwerveDrive.h\" 35 36 class Robot : public frc :: TimedRobot { 37 public : 38 void RobotInit () override ; 39 void RobotPeriodic () override ; 40 void DisabledInit () override ; 41 void DisabledPeriodic () override ; 42 void DisabledExit () override ; 43 void AutonomousInit () override ; 44 void AutonomousPeriodic () override ; 45 void AutonomousExit () override ; 46 void TeleopInit () override ; 47 void TeleopPeriodic () override ; 48 void TeleopExit () override ; 49 void TestInit () override ; 50 void TestPeriodic () override ; 51 void TestExit () override ; 52 void SimulationPeriodic () override ; 53 54 private : 55 photon :: PhotonCamera camera { constants :: Vision :: kCameraName }; 56 SwerveDrive drivetrain {}; 57 VisionSim vision { & camera }; 58 frc :: XboxController controller { 0 }; 59 static constexpr double VISION_TURN_kP = 0.01 ; 60 }; C++ (Source) 56 void Robot::TeleopPeriodic () { 57 // Calculate drivetrain commands from Joystick values 58 auto forward = 59 -1.0 * controller . GetLeftY () * constants :: Swerve :: kMaxLinearSpeed ; 60 auto strafe = 61 -1.0 * controller . GetLeftX () * constants :: Swerve :: kMaxLinearSpeed ; 62 auto turn = 63 -1.0 * controller . GetRightX () * constants :: Swerve :: kMaxAngularSpeed ; 64 65 bool targetVisible = false ; 66 double targetYaw = 0.0 ; 67 auto results = camera . GetAllUnreadResults (); 68 if ( results . size () > 0 ) { 69 // Camera processed a new frame since last 70 // Get the last one in the list. 71 auto result = results [ results . size () - 1 ]; 72 if ( result . HasTargets ()) { 73 // At least one AprilTag was seen by the camera 74 for ( auto & target : result . GetTargets ()) { 75 if ( target . GetFiducialId () == 7 ) { 76 // Found Tag 7, record its information 77 targetYaw = target . GetYaw (); 78 targetVisible = true ; 79 } 80 } 81 } 82 } 83 84 // Auto-align when requested 85 if ( controller . GetAButton () && targetVisible ) { 86 // Driver wants auto-alignment to tag 7 87 // And, tag 7 is in sight, so we can turn toward it. 88 // Override the driver's turn command with an automatic one that turns 89 // toward the tag. 90 turn = 91 -1.0 * targetYaw * VISION_TURN_kP * constants :: Swerve :: kMaxAngularSpeed ; 92 } 93 94 // Command drivetrain motors based on target speeds 95 drivetrain . Drive ( forward , strafe , turn ); 96 } Python 46 def teleopPeriodic ( self ) -> None : 47 xSpeed = - 1.0 * self . controller . getLeftY () * drivetrain . kMaxSpeed 48 ySpeed = - 1.0 * self . controller . getLeftX () * drivetrain . kMaxSpeed 49 rot = - 1.0 * self . controller . getRightX () * drivetrain . kMaxAngularSpeed 50 51 # Get information from the camera 52 targetYaw = 0.0 53 targetVisible = False 54 results = self . cam . getAllUnreadResults () 55 if len ( results ) > 0 : 56 result = results [ - 1 ] # take the most recent result the camera had 57 for target in result . getTargets (): 58 if target . getFiducialId () == 7 : 59 # Found tag 7, record its information 60 targetVisible = True 61 targetYaw = target . getYaw () 62 63 if self . controller . getAButton () and targetVisible : 64 # Driver wants auto-alignment to tag 7 65 # And, tag 7 is in sight, so we can turn toward it. 66 # Override the driver's turn command with an automatic one that turns toward the tag. 67 rot = - 1.0 * targetYaw * VISION_TURN_kP * drivetrain . kMaxAngularSpeed 68 69 self . swerve . drive ( xSpeed , ySpeed , rot , True , self . getPeriod ()) 70",
      "content_preview": "Aiming at a Target  The following example is from the PhotonLib example repository ( Java ). Knowledge and Equipment Needed  A Robot A camera mounted rigidly to the robot’s frame, centered and pointed forward. A coprocessor running PhotonVision with an AprilTag or ArUco 2D Pipeline."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/additional-resources/nt-api.html",
      "title": "NetworkTables API",
      "section": "Additional Resources",
      "language": "All",
      "content": "NetworkTables API  About  Warning PhotonVision interfaces with PhotonLib, our vendor dependency, using NetworkTables. If you are running PhotonVision on a robot (ie. with a RoboRIO), you should turn the NetworkTables server switch (in the settings tab) off in order to get PhotonLib to work. Also ensure that you set your team number. The NetworkTables server should only be enabled if you know what you’re doing! API  Warning NetworkTables is not a supported setup/viable option when using PhotonVision as we only send one target at a time (this is problematic when using AprilTags, which will return data from multiple tags at once). We recommend using PhotonLib. The tables below contain the the name of the key for each entry that PhotonVision sends over the network and a short description of the key. The entries should be extracted from a subtable with your camera’s nickname (visible in the PhotonVision UI) under the main photonvision table. Getting Target Information  Key Type Description rawBytes byte[] A byte-packed string that contains target info from the same timestamp. latencyMillis double The latency of the pipeline in milliseconds. hasTarget boolean Whether the pipeline is detecting targets or not. targetPitch double The pitch of the target in degrees (positive up). targetYaw double The yaw of the target in degrees (positive right). targetArea double The area (percent of bounding box in screen) as a percent (0-100). targetSkew double The skew of the target in degrees (counter-clockwise positive). targetPose double[] The pose of the target relative to the robot (x, y, z, qw, qx, qy, qz) targetPixelsX double The target crosshair location horizontally, in pixels (origin top-right) targetPixelsY double The target crosshair location vertically, in pixels (origin top-right) Changing Settings  Key Type Description pipelineIndex int Changes the pipeline index. driverMode boolean Toggles driver mode. Saving Images  PhotonVision can save images to file on command. The image is saved when PhotonVision detects the command went from false to true . PhotonVision will automatically set these back to false after 500ms. Be careful saving images rapidly - it will slow vision processing performance and take up disk space very quickly. Images are returned as part of the .zip package from the “Export” operation in the Settings tab. Key Type Description inputSaveImgCmd boolean Triggers saving the current input image to file. outputSaveImgCmd boolean Triggers saving the current output image to file. Warning If you manage to make calls to these commands faster than 500ms (between calls), additional photos will not be captured. Global Entries  These entries are global, meaning that they should be called on the main photonvision table. Key Type Description ledMode int Sets the LED Mode (-1: default, 0: off, 1: on, 2: blink) Warning Setting the LED mode to -1 (default) when multiple cameras are connected may result in unexpected behavior. This is a known limitation of PhotonVision. Single camera operation should work without issue.",
      "content_preview": "NetworkTables API  About  Warning PhotonVision interfaces with PhotonLib, our vendor dependency, using NetworkTables. If you are running PhotonVision on a robot (ie. with a RoboRIO), you should turn the NetworkTables server switch (in the settings tab) off in order to get PhotonLib to work."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/troubleshooting/common-errors.html",
      "title": "Common Issues / Questions",
      "section": "Troubleshooting",
      "language": "All",
      "content": "Common Issues / Questions  This page will grow as needed in order to cover commonly seen issues by teams. If this page doesn’t help you and you need further assistance, feel free to Contact Us . Known Issues  All known issues can be found on our GitHub page . PS3Eye  Due to an issue with Linux kernels, the drivers for the PS3Eye are no longer supported. If you would still like to use the PS3Eye, you can downgrade your kernel with the following command: sudo CURL_CA_BUNDLE=/etc/ssl/certs/ca-certificates.crt rpi-update 866751bfd023e72bd96a8225cf567e03c334ecc4 . Note: You must be connected to the internet to run the command. LED Control  The logic for controlling LED mode when multiple cameras are connected is not fully fleshed out. In its current state, LED control is only enabled when a Pi Camera Module is not in driver mode—meaning a USB camera on its own is unable to control the LEDs. For now, if you are using multiple cameras, it is recommended that teams set the value of the NetworkTables entry photonvision/ledMode from the robot code to control LED state. Commonly Seen Issues  Networking Issues  Please refer to our comprehensive networking troubleshooting tips for debugging suggestions and possible causes. Camera won’t show up  Try these steps to troubleshoot your camera connection . If you are using a USB camera, it is possible your USB Camera isn’t supported by CSCore and therefore won’t work with PhotonVision. Camera is consistently returning incorrect values when in 3D mode  Read the tips on the camera calibration page , follow the advice there, and redo the calibration. Not getting data from PhotonLib  Ensure your coprocessor version and PhotonLib version match. This can be checked by the settings tab and examining the .json itself (respectively). Ensure that you have your team number set properly. Use Glass to verify that PhotonVision has connected to the NetworkTables server served by your robot. With Glass connected in client mode to your RoboRIO, we expect to see “photonvision” listed under the Clients tab of the NetworkTables Info pane. When creating a PhotonCamera in code, does the cameraName provided match the name in the upper-right card of the web interface? Glass can be used to verify the RoboRIO is receiving NetworkTables data by inspecting the photonvision subtable for your camera nickname. Unable to download PhotonLib  Ensure all of your network firewalls are disabled and you aren’t on a school-network. PhotonVision prompts for login on startup  This is normal. You don’t need to connect a display to your Raspberry Pi to use PhotonVision, just navigate to the relevant webpage (ex. photonvision.local:5800 ) in order to see the dashboard. Raspberry Pi enters into boot looping state when using PhotonVision  This is most commonly seen when your Pi doesn’t have adequate power / is being undervolted. Ensure that your power supply is functioning properly.",
      "content_preview": "Common Issues / Questions  This page will grow as needed in order to cover commonly seen issues by teams. If this page doesn’t help you and you need further assistance, feel free to Contact Us . Known Issues  All known issues can be found on our GitHub page ."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/calibration/calibration.html",
      "title": "Calibrating Your Camera",
      "section": "Camera Calibration",
      "language": "All",
      "content": "Calibrating Your Camera  Important In order to detect AprilTags and use 3D mode, your camera must be calibrated at the desired resolution! Inaccurate calibration will lead to poor performance. To calibrate a camera, images of a ChArUco board (or chessboard) are taken. By comparing where the grid corners should be in object space (for example, a corner once every inch in an 8x6 grid) with where they appear in the camera image, we can find a least-squares estimate for intrinsic camera properties like focal lengths, center point, and distortion coefficients. For more on camera calibration, please review the OpenCV documentation . Warning While any resolution can be calibrated, higher resolutions may be too performance-intensive for some coprocessors to handle. Therefore, we recommend experimenting to see what works best for your coprocessor. Note The calibration data collected during calibration is specific to each physical camera, as well as each individual resolution. Calibration Tips  Warning The usage of chessboards can result in bad calibration results if multiple similar images are taken. We strongly recommend that teams use ChArUco boards instead! Accurate camera calibration is required in order to get accurate pose measurements when using AprilTags and 3D mode. The tips below should help ensure success: Ensure the images you take have the target in different positions and angles, with as big of a difference between angles as possible. It is important to make sure the target overlay still lines up with the board while doing this. Tilt no more than 45 degrees. Use as big of a calibration target as your printer can print. Ensure that your printed pattern has enough white border around it. Ensure your camera stays in one position during the duration of the calibration. Make sure you get all 12 images from varying distances and angles. Take at least one image that covers the total image area, and generally ensure that you get even coverage of the lens with your image set. Have good lighting, having a diffusely lit target would be best (light specifically shining on the target without shadows). Ensure the calibration target is completely flat and does not bend or fold in any way. It should be mounted/taped down to something flat and then used for calibration, do not just hold it up. Avoid having targets that are parallel to the lens of the camera / straight on towards the camera as much as possible. You want angles and variations within your calibration images. Following the ideas above should help in getting an accurate calibration. Calibrating using PhotonVision  1. Navigate to the calibration section in the UI.  The Cameras tab of the UI houses PhotonVision’s camera calibration tooling. It assists users with calibrating their cameras, as well as allows them to view previously calibrated resolutions. We support both ChArUco and chessboard calibrations. 2. Print out the calibration target.  In the Camera Calibration tab, we’ll print out the calibration target using the “Download” button. This should be printed on 8.5x11 printer paper. This page shows using an 8x8 ChArUco board (or chessboard depending on the selected calibration type). Warning Ensure that there is no scaling applied during printing (it should be at 100%) and that the PDF is printed as is on regular printer paper. Check the square size with calipers or an accurate measuring device after printing to ensure squares are sized properly, and enter the true size of the square in the UI text box. For optimal results, various resources are available online to calibrate your specific printer if needed. 3. Select calibration resolution and fill in appropriate target data.  We’ll next select a resolution to calibrate and populate our pattern spacing, marker size, and board size. The provided chessboard and ChArUco board are an 8x8 grid of 1 inch square. The provided ChArUco board uses the 4x4 dictionary with a marker size of 0.75 inches (this board does not need the old OpenCV pattern selector selected). Printers are not perfect, and you need to measure your calibration target and enter the correct marker size (size of the ArUco marker) and pattern spacing (aka size of the black square) using calipers or similar. Finally, once our entered data is correct, we’ll click “start calibration.” Warning Old OpenCV Pattern selector. This should be used in the case that the calibration image is generated from a version of OpenCV before version 4.6.0. This would include targets created by calib.io. If this selector is not set correctly the calibration will be completely invalid. For more info view this GitHub issue . Note If you have a calib.io ChArUco Target you will have to enter the paramaters of your target. For example if your target says “9x12 | Checker Size: 30 mm | Marker Size: 22 mm | Dictionary: ArUco DICT 5x5”, you would have to set the board type to Dict_5x5_1000, the pattern spacing to 1.1811 in (30 mm converted to inches), the marker size 0.866142 in (22 mm converted to inches), the board width to 12 and the board height to 9. If you chose the wrong tag family the board wont be detected during calibration. If you swap the width and height your calibration will have a very high error. 4. Take at calibration images from various angles.  Now, we’ll capture images of our board from various angles. It’s important to check that the board overlay matches the board in your image. The further the overdrawn points are from the true position of the chessboard corners, the less accurate the final calibration will be. We’ll want to capture enough images to cover the whole camera’s FOV (with a minimum of 12). Once we’ve got our images, we’ll click “Finish calibration” and wait for the calibration process to complete. If all goes well, the mean error and FOVs will be shown in the table on the right. The FOV should be close to the camera’s specified FOV (usually found in a datasheet) usually within + or - 10 degrees. The mean error should also be low, usually less than 1 pixel. Your browser does not support the video tag. Accessing Calibration Images  Details about a particular calibration can be viewed by clicking on that resolution in the calibrations tab. This tab allows you to download raw calibration data, upload a previous calibration, and inspect details about calculated camera intrinsic. Note More info on what these parameters mean can be found in OpenCV’s docs Fx/Fy: Estimated camera focal length, in mm Fx/Cy: Estimated camera optical center, in pixels. This should be at about the center of the image Distortion: OpenCV camera model distortion coefficients FOV: calculated using estimated focal length and image size. Useful for gut-checking calibration results Mean Err: Mean reprojection error, or distance between expected and observed chessboard cameras for the full calibration dataset Below these outputs are the snapshots collected for calibration, along with a per-snapshot mean reprojection error. A snapshot with a larger reprojection error might indicate a bad snapshot, due to effects such as motion blur or misidentified chessboard corners. Calibration images can also be extracted from the downloaded JSON file using this Python script . This script will unpack calibration images, and also generate a VNL file for use with mrcal . python3 / path / to / calibrationUtils . py path / to / photon_calibration . json / path / to / output / folder Investigating Calibration Data with mrcal  mrcal is a command-line tool for camera calibration and visualization. PhotonVision has the option to use the mrcal backend during camera calibration to estimate intrinsics. mrcal can also be used post-calibration to inspect snapshots and provide feedback. These steps will closely follow the mrcal tour – I’m aggregating commands and notes here, but the mrcal documentation is much more thorough. Start by Installing mrcal . Note that while mrcal calibration using PhotonVision is supported on all platforms, but investigation right now only works on Linux. Some users have also reported luck using WSL 2 on Windows as well. You may also need to install feedgnuplot . On Ubuntu systems, these commands should be run from a standalone terminal and not the one built into vscode . Let’s run calibrationUtils.py as described above, and then cd into the output folder. From here, you can follow the mrcal tour, just replacing the VNL filename and camera imager size as necessary. My camera calibration was at 1280x720, so I’ve set the XY limits to that below. $ cd /path/to/output/folder $ ls matt@photonvision:~/Documents/Downloads/2024-01-02_lifecam_1280$ ls corners.vnl img0.png img10.png img11.png img12.png img13.png img1.png img2.png img3.png img4.png img5.png img6.png img7.png img8.png img9.png cameramodel_0.cameramodel $ < corners.vnl \\ vnl-filter -p x,y | \\ feedgnuplot --domain --square --set 'xrange [0:1280] noextend' --set 'yrange [720:0] noextend' As you can see, we didn’t do a fantastic job of covering our whole camera sensor – there’s a big gap across the whole right side, for example. We also only have 14 calibration images. We’ve also got our “cameramodel” file, which can be used by mrcal to display additional debug info. Let’s inspect our reprojection error residuals. We expect their magnitudes and directions to be random – if there’s patterns in the colors shown, then our calibration probably doesn’t fully explain our physical camera sensor. $ mrcal-show-residuals --magnitudes --set 'cbrange [0:1.5]' ./camera-0.cameramodel $ mrcal-show-residuals --directions --unset key ./camera-0.cameramodel Clearly we don’t have anywhere near enough data to draw any meaningful conclusions (yet). But for fun, let’s dig into camera uncertainty estimation . This diagram shows how expected projection error changes due to noise in calibration inputs. Lower projection error across a larger area of the sensor imply a better calibration that more fully covers the whole sensor. For my calibration data, you can tell the projection error isolines (lines of constant expected projection error) are skewed to the left, following my dataset (which was also skewed left). $ mrcal-show-projection-uncertainty --unset key ./cameramodel_0.cameramodel",
      "content_preview": "Calibrating Your Camera  Important In order to detect AprilTags and use 3D mode, your camera must be calibrated at the desired resolution! Inaccurate calibration will lead to poor performance. To calibrate a camera, images of a ChArUco board (or chessboard) are taken."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/troubleshooting/camera-troubleshooting.html",
      "title": "Camera Troubleshooting",
      "section": "Troubleshooting",
      "language": "All",
      "content": "Camera Troubleshooting  Pi Cameras  If you haven’t yet, please refer to the Pi CSI Camera Configuration page for information on updating config.txt for your use case. If you’ve tried that, and things still aren’t working, restart PhotonVision using the restart button in the settings tab, and press tilde (`) in the web UI once connection is restored. This should show the most recent boot log. Expected output Bad LibCamera driver initialization Successfully loaded libpicam shared object Failed to load native libraries! Camera detected Adding local video device - “unicam” at “/dev/video0” No output from VisionSourceManager VisionSource created Adding 1 configs to VMM. No output from VisionSourceManager If the driver isn’t loaded, you may be using a non-official Pi image, or an image not new enough. Try updating to the most recent image available (one released for 2023) – if that doesn’t resolve the problem, contact us with your settings ZIP file and Pi version/camera version/config.txt file used. If the camera is not detected, the most likely cause is either a config.txt file incorrectly set-up, or a ribbon cable attached backwards. Review the picam configuration page , and verify the ribbon cable is properly oriented at both ends, and that it is fully inserted into the FFC connector. Then, contact us with your settings ZIP file and Pi version/camera version/config.txt file used. USB cameras  USB cameras supported by CSCore require no libcamera driver initialization to work – however, similar troubleshooting steps apply. Restart PhotonVision using the restart button in the settings tab, and press tilde on your keyboard (`) when you’re in the web UI once connection is restored. We expect to see the following output: Expected output Bad Camera detected Adding local video device - “foobar” at “/dev/foobar” No output from VisionSourceManager VisionSource created Adding 1 configs to VMM. No output from VisionSourceManager Determining detected cameras in Video4Linux (v4l2)  On Linux devices (including Raspberry Pi), PhotonVision uses WPILib’s CSCore to interact with video devices, which internally uses Video4Linux (v4l2). CSCore, and therefore Photon, requires that cameras attached have good v4l drivers for proper functionality. These should be built into the Linux kernel, and do not need to be installed manually. Valid picamera setup (from /boot/config.txt) can also be determined using these steps. The list-devices command will show all valid video devices detected, and list-formats the list of “video modes” each camera can be in. For picams: edit the config.txt file as described in the picam configuration page SSH into your Pi: ssh pi@photonvision.local and enter the username “pi” & password “raspberry” run v4l2-ctl --list-devices and v4l2-ctl --list-formats We expect an output similar to the following. For picameras, note the “unicam” entry with path platform:3f801000.csi (if we don’t see this, that’s bad), and a huge list of valid video formats. USB cameras should show up similarly in the output of these commands. Working pi@photonvision:~ $ v4l2-ctl --list-devices unicam (platform:3f801000.csi): /dev/video0 /dev/media3 bcm2835-codec-decode (platform:bcm2835-codec): /dev/video10 /dev/video11 /dev/video12 /dev/video18 /dev/video31 /dev/media2 bcm2835-isp (platform:bcm2835-isp): /dev/video13 /dev/video14 /dev/video15 /dev/video16 /dev/video20 /dev/video21 /dev/video22 /dev/video23 /dev/media0 /dev/media1 pi@photonvision:~ $ v4l2-ctl --list-formats ioctl: VIDIOC_ENUM_FMT Type: Video Capture [0]: 'YUYV' (YUYV 4:2:2) [1]: 'UYVY' (UYVY 4:2:2) [2]: 'YVYU' (YVYU 4:2:2) [3]: 'VYUY' (VYUY 4:2:2) <snip> [42]: 'Y12P' (12-bit Greyscale (MIPI Packed)) [43]: 'Y12 ' (12-bit Greyscale) [44]: 'Y14P' (14-bit Greyscale (MIPI Packed)) [45]: 'Y14 ' (14-bit Greyscale) Not Working pi@photonvision:~ $ v4l2-ctl --list-devices bcm2835-codec-decode (platform:bcm2835-codec): /dev/video10 /dev/video11 /dev/video12 /dev/video18 /dev/video31 /dev/media3 bcm2835-isp (platform:bcm2835-isp): /dev/video13 /dev/video14 /dev/video15 /dev/video16 /dev/video20 /dev/video21 /dev/video22 /dev/video23 /dev/media0 /dev/media1 rpivid (platform:rpivid): /dev/video19 /dev/media2 Cannot open device /dev/video0, exiting. Random Disconnects  This is a common issue with Arducam cameras, or other cameras with exposed boards, similar to Arducams. We currently believe it occurs due to electrostatic discharge (ESD). The recommended solution is fully enclosing the camera inside of a case, which can be 3D-printed or sourced from a reputable vendor. Using an anti-static spray on the case can also be helpful in reducing ESD. If the camera was previously damaged, due to ESD or some other reason, this is not guaranteed to resolve the problem. To ensure that the camera remains safe from ESD when not in the case, it is recommended that when handling, cameras are only held by the corners. Additionally, cameras should be stored in ESD safe bags which they are commonly shipped in.",
      "content_preview": "Camera Troubleshooting  Pi Cameras  If you haven’t yet, please refer to the Pi CSI Camera Configuration page for information on updating config.txt for your use case."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/troubleshooting/networking-troubleshooting.html",
      "title": "Networking Troubleshooting",
      "section": "Troubleshooting",
      "language": "All",
      "content": "Networking Troubleshooting  Before reading further, ensure that you follow all the recommendations in our networking section . You should follow these guidelines in order for PhotonVision to work properly; other networking setups are not officially supported. Checklist  A few issues make up the majority of support requests. Run through this checklist quickly to catch some common mistakes. Is your camera connected to the robot’s radio through a network switch ? Ethernet straight from a laptop to a coprocessor will not work (most likely), due to the unreliability of link-local connections. Even if there’s a switch between your laptop and coprocessor, you’ll still want a radio or router in the loop somehow. The FRC radio is the only router we will officially support due to the innumerable variations between routers. (Raspberry Pi, Orange Pi & Limelight only) have you flashed the correct image, and is it up to date ? Is your robot code using a 2025 version of WPILib, and is your coprocessor using the most up to date 2025 release? 2022, 2023, 2024, and 2025 versions of either cannot be mix-and-matched! Your PhotonVision version can be checked on the settings tab. Is your team number correctly set on the settings tab? photonvision.local Not Found  Use Angry IP Scanner and look for an IP that has port 5800 open. Then go to your web browser and do <IP ADDRESS>:5800. Alternatively, you can plug your coprocessor into a display, plug in a keyboard, and run hostname -I in the terminal. This should give you the IP Address of your coprocessor, then go to your web browser and do <IP ADDRESS>:5800. If nothing shows up, ensure your coprocessor has power, and you are following all of our networking recommendations, feel free to contact us and we will help you. Can’t Connect To Robot  Please check that: 1. You don’t have the NetworkTables Server on (toggleable in the settings tab). Turn this off when doing work on a robot. 2. You have your team number set properly in the settings tab. 3. Your camera name in the PhotonCamera constructor matches the name in the UI. 4. You are using the 2025 version of WPILib and RoboRIO image. 5. Your robot is on. If all of the above are met and you still have issues, feel free to contact us and provide the following information: The WPILib version used by your robot code PhotonLib vendor dependency version PhotonVision version (from the UI) Your settings exported from your coprocessor (if you’re able to access it) How your RoboRIO/coprocessor are networked together",
      "content_preview": "Networking Troubleshooting  Before reading further, ensure that you follow all the recommendations in our networking section . You should follow these guidelines in order for PhotonVision to work properly; other networking setups are not officially supported."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/index.html",
      "title": "PhotonVision Docs",
      "section": "General",
      "language": "All",
      "content": "Welcome to the official documentation of PhotonVision! PhotonVision is the free, fast, and easy-to-use vision processing solution for the FIRST Robotics Competition. PhotonVision is designed to get vision working on your robot quickly , without the significant cost of other similar solutions. PhotonVision supports a variety of COTS hardware, including the Raspberry Pi 3, 4, and 5, the SnakeEyes Pi hat , and the Orange Pi 5. Content  Quick Start Quick start to using Photonvision. Quick Start Advanced Installation Get started with installing PhotonVision on non-supported hardware. Advanced Installation Programming Reference and PhotonLib Learn more about PhotonLib, our vendor dependency which makes it easier for teams to retrieve vision data, make various calculations, and more. Programming Reference Integration Pick how to use vision processing results to control a physical robot. Robot Integration Code Examples View various step by step guides on how to use data from PhotonVision in your code, along with game-specific examples. Code Examples Hardware Select appropriate hardware for high-quality and easy vision target detection. Hardware Selection Contributing Interested in helping with PhotonVision? Learn more about how to contribute to our main code base, documentation, and more. Contributing to PhotonVision Projects Source Code  The source code for all PhotonVision projects is available through our GitHub organization . PhotonVision Contact Us  To report a bug or submit a feature request in PhotonVision, please submit an issue on the PhotonVision GitHub or contact the developers on Discord . If you find a problem in this documentation, please submit an issue on the PhotonVision Documentation GitHub . License  PhotonVision is licensed under the GNU GPL v3 .",
      "content_preview": "Welcome to the official documentation of PhotonVision! PhotonVision is the free, fast, and easy-to-use vision processing solution for the FIRST Robotics Competition. PhotonVision is designed to get vision working on your robot quickly , without the significant cost of other similar solutions."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/examples/poseest.html",
      "title": "Using WPILib Pose Estimation, Simulation, and PhotonVision Together",
      "section": "Code Examples",
      "language": "All",
      "content": "Using WPILib Pose Estimation, Simulation, and PhotonVision Together  The following example comes from the PhotonLib example repository ( Java / C++ / Python ). Full code is available at that links. Knowledge and Equipment Needed  Everything required in Combining Aiming and Getting in Range , plus some familiarity with WPILib pose estimation functionality. Background  This example demonstrates integration of swerve drive control, a basic swerve physics simulation, and PhotonLib’s simulated vision system functionality. Walkthrough  Estimating Pose  The Drivetrain class includes functionality to fuse multiple sensor readings together (including PhotonVision) into a best-guess of the pose on the field. Please reference the WPILib documentation on using the SwerveDrivePoseEstimator class. We use the 2024 game’s AprilTag Locations: JAVA 68 visionSim . addAprilTags ( kTagLayout ); C++ 42 inline const frc :: AprilTagFieldLayout kTagLayout { 43 frc :: LoadAprilTagLayoutField ( frc :: AprilTagField :: k2024Crescendo )}; PYTHON 46 self . camPoseEst = PhotonPoseEstimator ( To incorporate PhotonVision, we need to create a PhotonCamera : JAVA 57 camera = new PhotonCamera ( kCameraName ); C++ 145 photon :: PhotonCamera camera { constants :: Vision :: kCameraName }; PYTHON 44 self . swerve = drivetrain . Drivetrain () During periodic execution, we read back camera results. If we see AprilTags in the image, we calculate the camera-measured pose of the robot and pass it to the Drivetrain . JAVA 64 // Correct pose estimate with vision measurements 65 var visionEst = vision . getEstimatedGlobalPose (); 66 visionEst . ifPresent ( 67 est -> { 68 // Change our trust in the measurement based on the tags we can see 69 var estStdDevs = vision . getEstimationStdDevs (); 70 71 drivetrain . addVisionMeasurement ( 72 est . estimatedPose . toPose2d (), est . timestampSeconds , estStdDevs ); 73 }); C++ 38 auto visionEst = vision . GetEstimatedGlobalPose (); 39 if ( visionEst . has_value ()) { 40 auto est = visionEst . value (); 41 auto estPose = est . estimatedPose . ToPose2d (); 42 auto estStdDevs = vision . GetEstimationStdDevs ( estPose ); 43 drivetrain . AddVisionMeasurement ( est . estimatedPose . ToPose2d (), est . timestamp , 44 estStdDevs ); 45 } PYTHON 54 camEstPose = self . camPoseEst . update () 55 if camEstPose : 56 self . swerve . addVisionPoseEstimate ( Simulating the Camera  First, we create a new VisionSystemSim to represent our camera and coprocessor running PhotonVision, and moving around our simulated field. JAVA 65 // Create the vision system simulation which handles cameras and targets on the field. 66 visionSim = new VisionSystemSim ( \"main\" ); 67 // Add all the AprilTags inside the tag layout as visible targets to this simulated field. 68 visionSim . addAprilTags ( kTagLayout ); 69 // Create simulated camera properties. These can be set to mimic your actual camera. C++ 49 visionSim = std :: make_unique < photon :: VisionSystemSim > ( \"main\" ); 50 51 visionSim -> AddAprilTags ( constants :: Vision :: kTagLayout ); PYTHON # Coming Soon! Then, we add configure the simulated vision system to match the camera system being simulated. JAVA 69 // Create simulated camera properties. These can be set to mimic your actual camera. 70 var cameraProp = new SimCameraProperties (); 71 cameraProp . setCalibration ( 960 , 720 , Rotation2d . fromDegrees ( 90 )); 72 cameraProp . setCalibError ( 0.35 , 0.10 ); 73 cameraProp . setFPS ( 15 ); 74 cameraProp . setAvgLatencyMs ( 50 ); 75 cameraProp . setLatencyStdDevMs ( 15 ); 76 // Create a PhotonCameraSim which will update the linked PhotonCamera's values with visible 77 // targets. 78 cameraSim = new PhotonCameraSim ( camera , cameraProp ); 79 // Add the simulated camera to view the targets on this simulated field. 80 visionSim . addCamera ( cameraSim , kRobotToCam ); 81 82 cameraSim . enableDrawWireframe ( true ); C++ 53 cameraProp = std :: make_unique < photon :: SimCameraProperties > (); 54 55 cameraProp -> SetCalibration ( 960 , 720 , frc :: Rotation2d { 90 _deg }); 56 cameraProp -> SetCalibError ( .35 , .10 ); 57 cameraProp -> SetFPS ( 15 _Hz ); 58 cameraProp -> SetAvgLatency ( 50 _ms ); 59 cameraProp -> SetLatencyStdDev ( 15 _ms ); 60 61 cameraSim = 62 std :: make_shared < photon :: PhotonCameraSim > ( & camera , * cameraProp . get ()); 63 64 visionSim -> AddCamera ( cameraSim . get (), constants :: Vision :: kRobotToCam ); 65 cameraSim -> EnableDrawWireframe ( true ); PYTHON # Coming Soon! Updating the Simulated Vision System  During simulation, we periodically update the simulated vision system. JAVA 114 @Override 115 public void simulationPeriodic () { 116 // Update drivetrain simulation 117 drivetrain . simulationPeriodic (); 118 119 // Update camera simulation 120 vision . simulationPeriodic ( drivetrain . getSimPose ()); 121 122 var debugField = vision . getSimDebugField (); 123 debugField . getObject ( \"EstimatedRobot\" ). setPose ( drivetrain . getPose ()); 124 debugField . getObject ( \"EstimatedRobotModules\" ). setPoses ( drivetrain . getModulePoses ()); 125 126 // Update gamepiece launcher simulation 127 gpLauncher . simulationPeriodic (); 128 129 // Calculate battery voltage sag due to current draw 130 RoboRioSim . setVInVoltage ( 131 BatterySim . calculateDefaultBatteryLoadedVoltage ( drivetrain . getCurrentDraw ())); 132 } C++ 95 void Robot::SimulationPeriodic () { 96 launcher . simulationPeriodic (); 97 drivetrain . SimulationPeriodic (); 98 vision . SimPeriodic ( drivetrain . GetSimPose ()); 99 100 frc :: Field2d & debugField = vision . GetSimDebugField (); 101 debugField . GetObject ( \"EstimatedRobot\" ) -> SetPose ( drivetrain . GetPose ()); 102 debugField . GetObject ( \"EstimatedRobotModules\" ) 103 -> SetPoses ( drivetrain . GetModulePoses ()); 104 105 units :: ampere_t totalCurrent = drivetrain . GetCurrentDraw (); 106 units :: volt_t loadedBattVolts = 107 frc :: sim :: BatterySim :: Calculate ({ totalCurrent }); 108 frc :: sim :: RoboRioSim :: SetVInVoltage ( loadedBattVolts ); 109 } PYTHON # Coming Soon! The rest is done behind the scenes. Your browser does not support the video tag.",
      "content_preview": "Using WPILib Pose Estimation, Simulation, and PhotonVision Together  The following example comes from the PhotonLib example repository ( Java / C++ / Python ). Full code is available at that links."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/examples/aimandrange.html",
      "title": "Combining Aiming and Getting in Range",
      "section": "Code Examples",
      "language": "All",
      "content": "Combining Aiming and Getting in Range  The following example is from the PhotonLib example repository ( Java / C++ / Python ) Knowledge and Equipment Needed  Everything required in Aiming at a Target . Code  Now that you know how to aim toward the AprilTag, let’s also drive the correct distance from the AprilTag. To do this, we’ll use the pitch of the target in the camera image and trigonometry to figure out how far away the robot is from the AprilTag. Then, like before, we’ll use the P term of a PID controller to drive the robot to the correct distance. Java 84 // Calculate drivetrain commands from Joystick values 85 double forward = - controller . getLeftY () * Constants . Swerve . kMaxLinearSpeed ; 86 double strafe = - controller . getLeftX () * Constants . Swerve . kMaxLinearSpeed ; 87 double turn = - controller . getRightX () * Constants . Swerve . kMaxAngularSpeed ; 88 89 // Read in relevant data from the Camera 90 boolean targetVisible = false ; 91 double targetYaw = 0.0 ; 92 double targetRange = 0.0 ; 93 var results = camera . getAllUnreadResults (); 94 if ( ! results . isEmpty ()) { 95 // Camera processed a new frame since last 96 // Get the last one in the list. 97 var result = results . get ( results . size () - 1 ); 98 if ( result . hasTargets ()) { 99 // At least one AprilTag was seen by the camera 100 for ( var target : result . getTargets ()) { 101 if ( target . getFiducialId () == 7 ) { 102 // Found Tag 7, record its information 103 targetYaw = target . getYaw (); 104 targetRange = 105 PhotonUtils . calculateDistanceToTargetMeters ( 106 0.5 , // Measured with a tape measure, or in CAD. 107 1.435 , // From 2024 game manual for ID 7 108 Units . degreesToRadians ( - 30.0 ), // Measured with a protractor, or in CAD. 109 Units . degreesToRadians ( target . getPitch ())); 110 111 targetVisible = true ; 112 } 113 } 114 } 115 } 116 117 // Auto-align when requested 118 if ( controller . getAButton () && targetVisible ) { 119 // Driver wants auto-alignment to tag 7 120 // And, tag 7 is in sight, so we can turn toward it. 121 // Override the driver's turn and fwd/rev command with an automatic one 122 // That turns toward the tag, and gets the range right. 123 turn = 124 ( VISION_DES_ANGLE_deg - targetYaw ) * VISION_TURN_kP * Constants . Swerve . kMaxAngularSpeed ; 125 forward = 126 ( VISION_DES_RANGE_m - targetRange ) * VISION_STRAFE_kP * Constants . Swerve . kMaxLinearSpeed ; 127 } 128 129 // Command drivetrain motors based on target speeds 130 drivetrain . drive ( forward , strafe , turn ); C++ (Header) 25 #pragma once 26 27 #include <photon/PhotonCamera.h> 28 29 #include <frc/TimedRobot.h> 30 #include <frc/XboxController.h> 31 32 #include \"Constants.h\" 33 #include \"VisionSim.h\" 34 #include \"subsystems/SwerveDrive.h\" 35 36 class Robot : public frc :: TimedRobot { 37 public : 38 void RobotInit () override ; 39 void RobotPeriodic () override ; 40 void DisabledInit () override ; 41 void DisabledPeriodic () override ; 42 void DisabledExit () override ; 43 void AutonomousInit () override ; 44 void AutonomousPeriodic () override ; 45 void AutonomousExit () override ; 46 void TeleopInit () override ; 47 void TeleopPeriodic () override ; 48 void TeleopExit () override ; 49 void TestInit () override ; 50 void TestPeriodic () override ; 51 void TestExit () override ; 52 void SimulationPeriodic () override ; 53 54 private : 55 photon :: PhotonCamera camera { constants :: Vision :: kCameraName }; 56 SwerveDrive drivetrain {}; 57 VisionSim vision { & camera }; 58 frc :: XboxController controller { 0 }; 59 static constexpr auto VISION_TURN_kP = 0.01 ; 60 static constexpr auto VISION_DES_ANGLE = 0.0 _deg ; 61 static constexpr auto VISION_STRAFE_kP = 0.5 ; 62 static constexpr auto VISION_DES_RANGE = 1.25 _m ; 63 }; C++ (Source) 58 void Robot::TeleopPeriodic () { 59 // Calculate drivetrain commands from Joystick values 60 auto forward = 61 -1.0 * controller . GetLeftY () * constants :: Swerve :: kMaxLinearSpeed ; 62 auto strafe = 63 -1.0 * controller . GetLeftX () * constants :: Swerve :: kMaxLinearSpeed ; 64 auto turn = 65 -1.0 * controller . GetRightX () * constants :: Swerve :: kMaxAngularSpeed ; 66 67 bool targetVisible = false ; 68 units :: degree_t targetYaw = 0.0 _deg ; 69 units :: meter_t targetRange = 0.0 _m ; 70 auto results = camera . GetAllUnreadResults (); 71 if ( results . size () > 0 ) { 72 // Camera processed a new frame since last 73 // Get the last one in the list. 74 auto result = results [ results . size () - 1 ]; 75 if ( result . HasTargets ()) { 76 // At least one AprilTag was seen by the camera 77 for ( auto & target : result . GetTargets ()) { 78 if ( target . GetFiducialId () == 7 ) { 79 // Found Tag 7, record its information 80 targetYaw = units :: degree_t { target . GetYaw ()}; 81 targetRange = photon :: PhotonUtils :: CalculateDistanceToTarget ( 82 0.5 _m , // Measured with a tape measure, or in CAD 83 1.435 _m , // From 2024 game manual for ID 7 84 -30.0 _deg , // Measured witha protractor, or in CAD 85 units :: degree_t { target . GetPitch ()}); 86 targetVisible = true ; 87 } 88 } 89 } 90 } 91 92 // Auto-align when requested 93 if ( controller . GetAButton () && targetVisible ) { 94 // Driver wants auto-alignment to tag 7 95 // And, tag 7 is in sight, so we can turn toward it. 96 // Override the driver's turn command with an automatic one that turns 97 // toward the tag and gets the range right. 98 turn = ( VISION_DES_ANGLE - targetYaw ). value () * VISION_TURN_kP * 99 constants :: Swerve :: kMaxAngularSpeed ; 100 forward = ( VISION_DES_RANGE - targetRange ). value () * VISION_STRAFE_kP * 101 constants :: Swerve :: kMaxLinearSpeed ; 102 } 103 104 // Command drivetrain motors based on target speeds 105 drivetrain . Drive ( forward , strafe , turn ); 106 } Python 52 def teleopPeriodic ( self ) -> None : 53 xSpeed = - 1.0 * self . controller . getLeftY () * drivetrain . kMaxSpeed 54 ySpeed = - 1.0 * self . controller . getLeftX () * drivetrain . kMaxSpeed 55 rot = - 1.0 * self . controller . getRightX () * drivetrain . kMaxAngularSpeed 56 57 # Get information from the camera 58 targetYaw = 0.0 59 targetRange = 0.0 60 targetVisible = False 61 results = self . cam . getAllUnreadResults () 62 if len ( results ) > 0 : 63 result = results [ - 1 ] # take the most recent result the camera had 64 # At least one apriltag was seen by the camera 65 for target in result . getTargets (): 66 if target . getFiducialId () == 7 : 67 # Found tag 7, record its information 68 targetVisible = True 69 targetYaw = target . getYaw () 70 heightDelta = CAM_MOUNT_HEIGHT_m - TAG_7_MOUNT_HEIGHT_m 71 angleDelta = math . radians ( CAM_MOUNT_PITCH_deg - target . getPitch ()) 72 targetRange = heightDelta / math . tan ( angleDelta ) 73 74 if self . controller . getAButton () and targetVisible : 75 # Driver wants auto-alignment to tag 7 76 # And, tag 7 is in sight, so we can turn toward it. 77 # Override the driver's turn and x-vel command with 78 # an automatic one that turns toward the tag 79 # and puts us at the right distance 80 rot = ( 81 ( VISION_DES_ANGLE_deg - targetYaw ) 82 * VISION_TURN_kP 83 * drivetrain . kMaxAngularSpeed 84 ) 85 xSpeed = ( 86 ( VISION_DES_RANGE_m - targetRange ) 87 * VISION_STRAFE_kP 88 * drivetrain . kMaxSpeed 89 ) 90 91 self . swerve . drive ( xSpeed , ySpeed , rot , True , self . getPeriod ())",
      "content_preview": "Combining Aiming and Getting in Range  The following example is from the PhotonLib example repository ( Java / C++ / Python ) Knowledge and Equipment Needed  Everything required in Aiming at a Target ."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/integration/index.html",
      "title": "Robot Integration",
      "section": "Robot Integration",
      "language": "All",
      "content": "Robot Integration  Vision - Robot Integration Background Vision Processing’s Purpose Simple Strategies Knowledge and Equipment Needed Angle Alignment Adding Range Alignment Advanced Strategies Knowledge and Equipment Needed Robot Poses from the Camera Field-Relative Pose Estimation I have a Pose Estimate, Now What?",
      "content_preview": "Robot Integration  Vision - Robot Integration Background Vision Processing’s Purpose Simple Strategies Knowledge and Equipment Needed Angle Alignment Adding Range Alignment Advanced Strategies Knowledge and Equipment Needed Robot Poses from the Camera Field-Relative Pose Estimation I have a Pose..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/integration/simpleStrategies.html",
      "title": "Simple Strategies",
      "section": "Robot Integration",
      "language": "All",
      "content": "Simple Strategies  Simple strategies for using vision processor outputs involve using the target’s position in the 2D image to infer range and angle to a particular AprilTag. Knowledge and Equipment Needed  A Coprocessor running PhotonVision A Drivetrain with wheels An AprilTag to aim at Angle Alignment  The simplest way to align a robot to an AprilTag is to rotate the drivetrain until the tag is centered in the camera image. To do this, Read the current yaw angle to the AprilTag from the vision Coprocessor. If too far off to one side, command the drivetrain to rotate in the opposite direction to compensate. See the Aiming at a Target example for more information. NOTE: This works if the camera is centered on the robot. This is easiest from a software perspective. If the camera is not centered, take a peek at the next example - it shows how to account for an offset. Adding Range Alignment  By looking at the position of the AprilTag in the “vertical” direction in the image, and applying some trigonometry, the distance between the robot and the camera can be deduced. Read the current pitch angle to the AprilTag from the vision coprocessor. Do math to calculate the distance to the AprilTag. If too far in one direction, command the drivetrain to travel in the opposite direction to compensate. This can be done simultaneously while aligning to the desired angle. See the Aim and Range example for more information.",
      "content_preview": "Simple Strategies  Simple strategies for using vision processor outputs involve using the target’s position in the 2D image to infer range and angle to a particular AprilTag."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/integration/background.html",
      "title": "Vision",
      "section": "Robot Integration",
      "language": "All",
      "content": "Vision - Robot Integration Background  Vision Processing’s Purpose  Each year, the FRC game requires a fundamental operation: Align the Robot to a Goal . Regardless of whether that alignment point is for picking up gamepieces, or for scoring, fast and effective robots must be able to align to them quickly and repeatably. Software strategies can be used to help augment the ability of a human operator, or step in when a human operator is not allowed to control the robot. Vision Processing is one key input to these software strategies. However, the inputs your coprocessor provides must be interpreted and converted (ultimately) to motor voltage commands. There are many valid strategies for doing this transformation. Picking a strategy is a balancing act between: Available team resources (time, programming skills, previous experience) Precision of alignment required Team willingness to take on risk Simple strategies are low-risk - they require comparatively little effort to implement and tune, but have hard limits on the complexity of motion they can control on the robot. Advanced methods allow for more complex and precise movement, but take more effort to implement and tune. For this reason, it is more risky to attempt to use them.",
      "content_preview": "Vision - Robot Integration Background  Vision Processing’s Purpose  Each year, the FRC game requires a fundamental operation: Align the Robot to a Goal ."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/integration/advancedStrategies.html",
      "title": "Advanced Strategies",
      "section": "Robot Integration",
      "language": "All",
      "content": "Advanced Strategies  Advanced strategies for using vision processing results involve working with the robot’s pose on the field. A pose is a combination an X/Y coordinate, and an angle describing where the robot’s front is pointed. A pose is always considered relative to some fixed point on the field. WPILib provides a Pose2d class to describe poses in software. PhotonVision can supply correcting information to keep estimates of pose accurate over a full match. Knowledge and Equipment Needed  A Coprocessor running PhotonVision - Accurate camera calibration to support “3D mode” required A Drivetrain with wheels and sensors - Sufficient sensors to measure wheel rotation - Capable of closed-loop velocity control A gyroscope or IMU measuring actual robot heading Experience using some path-planning library Robot Poses from the Camera  When using 3D mode in PhotonVision, an additional step is run to estimate the 3D position of camera, relative to one or more AprilTags. This process does not produce a unique solution. There are multiple possible camera positions which might explain the image it observed. Additionally, the camera is rarely mounted in the exact center of a robot. For these reasons, the 3D information must be filtered and transformed before they can describe the robot’s pose. PhotonLib provides a utility class to assist with this process on the roboRIO . Alternatively, a “multi-tag” strategy can do this process on the coprocessor. . Field-Relative Pose Estimation  The camera’s guess of the robot pose generally should be fused with other sensor readings. WPILib provides a set of pose estimation classes for doing this work. I have a Pose Estimate, Now What?  Triggering Actions Automatically  A simple way to use a pose estimate is to activate robot functions automatically when in the correct spot on the field. JAVA Pose3d robotPose ; boolean launcherSpinCmd ; // ... if ( robotPose . X () < 1.5 ){ // Near blue alliance wall, start spinning the launcher wheel launcherSpinCmd = True ; } else { // Far away, no need to run launcher. launcherSpinCmd = False ; } // ... PathPlanning  A common, but more complex usage of a pose estimate is an input to a path-following algorithm. Specifically, the pose estimate is used to correct for the robot straying off of the pre-defined path. See the Pose Estimation example for details on integrating this.",
      "content_preview": "Advanced Strategies  Advanced strategies for using vision processing results involve working with the robot’s pose on the field. A pose is a combination an X/Y coordinate, and an angle describing where the robot’s front is pointed."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/advanced-installation/index.html",
      "title": "Advanced Installation",
      "section": "General",
      "language": "All",
      "content": "Advanced Installation  This page will help you install PhotonVision on non-supported coprocessor. Step 1: Software Install  This section will walk you through how to install PhotonVision on your coprocessor. Your coprocessor is the device that has the camera and you are using to detect targets (ex. if you are using a Limelight / Raspberry Pi, that is your coprocessor and you should follow those instructions). Warning You only need to install PhotonVision on the coprocessor/device that is being used to detect targets, you do NOT need to install it on the device you use to view the webdashboard. All you need to view the webdashboard is for a device to be on the same network as your vision coprocessor and an internet browser. Software Installation Desktop Environments Windows PC Installation Linux PC Installation Mac OS Installation Other Other Debian-Based Co-Processor Installation Advanced Command Line Usage Romi Installation Installing Pre-Release Versions",
      "content_preview": "Advanced Installation  This page will help you install PhotonVision on non-supported coprocessor. Step 1: Software Install  This section will walk you through how to install PhotonVision on your coprocessor."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/advanced-installation/sw_install/romi.html",
      "title": "Romi Installation",
      "section": "General",
      "language": "All",
      "content": "Romi Installation  The Romi is a small robot that can be controlled with the WPILib software. The main controller is a Raspberry Pi that must be imaged with WPILibPi . Installation  The WPILibPi image includes FRCVision, which reserves USB cameras; to use PhotonVision, we need to edit the /home/pi/runCamera script to disable it. First we will need to make the file system writeable; the easiest way to do this is to go to 10.0.0.2 and choose “Writable” at the top. SSH into the Raspberry Pi (using Windows command line, or a tool like Putty ) at the Romi’s default address 10.0.0.2 . The default user is pi , and the password is raspberry . Attention The version of WPILibPi for the Romi is 2023.2.1, which is not compatible with the current version of PhotonVision. If you are using WPILibPi 2023.2.1 on your Romi, you must install PhotonVision v2023.4.2 or earlier! To install a compatible version of PhotonVision, enter these commands in the SSH terminal connected to the Raspberry Pi. This will download and run the install script, which will install PhotonVision on your Raspberry Pi and configure it to run at startup. $ wget https://git.io/JJrEP -O install.sh $ sudo chmod +x install.sh $ sudo ./install.sh -v v2023.4.2 The install script requires an internet connection, so connecting the Raspberry Pi to an internet-connected router via an Ethernet cable will be the easiest solution. The pi must remain writable while you are following these steps! Next, from the SSH terminal, run sudo nano /home/pi/runCamera then arrow down to the start of the exec line and press “Enter” to add a new line. Then add # before the exec command to comment it out. Then, arrow up to the new line and type sleep 10000 . Hit “Ctrl + O” and then “Enter” to save the file. Finally press “Ctrl + X” to exit nano. Now, reboot the Romi by typing sudo reboot now . After the Romi reboots, you should be able to open the PhotonVision UI at: http://10.0.0.2:5800/ . From here, you can adjust settings and configure Pipelines . Warning In order for settings, logs, etc. to be saved / take effect, ensure that PhotonVision is in writable mode. Attention When using an older version of PhotonVision, the user interface and features may be different than what appears in the online documentation. The Documentation link in the User Interface will open a bundled version of the documentation that matches the PhotonVision version running on your coprocessor.",
      "content_preview": "Romi Installation  The Romi is a small robot that can be controlled with the WPILib software. The main controller is a Raspberry Pi that must be imaged with WPILibPi ."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/pipelines/index.html",
      "title": "Pipelines",
      "section": "Pipelines",
      "language": "All",
      "content": "Pipelines  About Pipelines What is a pipeline? Types of Pipelines AprilTag / ArUco Object Detection Driver Mode Colored Shape Reflective Note About Multiple Cameras and Pipelines Pipeline Configuration AprilTag / ArUco Pipelines Object Detection Pipelines Reflective and Colored Shape Pipelines Camera Tuning / Input Resolution Exposure and brightness AprilTags and Motion Blur Orientation Stream Resolution Output Target Manipulation Robot Offset",
      "content_preview": "Pipelines  About Pipelines What is a pipeline? Types of Pipelines AprilTag / ArUco Object Detection Driver Mode Colored Shape Reflective Note About Multiple Cameras and Pipelines Pipeline Configuration AprilTag / ArUco Pipelines Object Detection Pipelines Reflective and Colored Shape Pipelines..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/pipelines/output.html",
      "title": "Output",
      "section": "Pipelines",
      "language": "All",
      "content": "Output  The output card contains sections for target manipulation and offset modes. Target Manipulation  In this section, the Target Offset Point changes where the “center” of the target is. This can be useful if the pitch/yaw of the middle of the top edge of the target is desired, rather than the center of mass of the target. The “top”/”bottom”/”left”/”right” of the target are defined by the Target Orientation selection. For example, a 400x200px target in landscape mode would have the “top” offset point located at the middle of the uppermost long edge of the target, while in portrait mode the “top” offset point would be located in the middle of the topmost short edge (in this case, either the left or right sides). This section also includes a switch to enable processing and sending multiple targets, up to 5, simultaneously. This information is available through PhotonLib. Note that the GetPitch / GetYaw methods will report the pitch/yaw of the “best” (lowest indexed) target. Your browser does not support the video tag. Robot Offset  PhotonVision offers both single and dual point offset modes. In single point mode, the “Take Point” button will set the crosshair location to the center of the current “best” target. In dual point mode, two snapshots are required. Take one snapshot with the target far away, and the other with the target closer. The position of the crosshair will be linearly interpolated between these two points based on the area of the current “best” target. This might be useful if single point is not accurate across the range of the tracking distance, or for significantly offset cameras.",
      "content_preview": "Output  The output card contains sections for target manipulation and offset modes. Target Manipulation  In this section, the Target Offset Point changes where the “center” of the target is."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/pipelines/input.html",
      "title": "Camera Tuning / Input",
      "section": "Pipelines",
      "language": "All",
      "content": "Camera Tuning / Input  PhotonVision’s “Input” tab contains settings that affect the image captured by the currently selected camera. This includes camera exposure and brightness, as well as resolution and orientation. Resolution  Resolution changes the resolution of the image captured. While higher resolutions are often more accurate than lower resolutions, they also run at a slower update rate. When using the reflective/colored shape pipeline, detection should be run as low of a resolution as possible as you are only trying to detect simple contours (essentially colored blobs). When using the AprilTag pipeline, you should try to use as high of a resolution as you can while still maintaining a reasonable FPS measurement. This is because higher resolution allows you to detect tags with higher accuracy and from larger distances. Exposure and brightness  Camera exposure and brightness control how bright the captured image will be, although they function differently. Camera exposure changes how long the camera shutter lets in light, which changes the overall brightness of the captured image. This is in contrast to brightness, which is a post-processing effect that boosts the overall brightness of the image at the cost of desaturating colors (making colors look less distinct). Important For all pipelines, exposure time should be set as low as possible while still allowing for the target to be reliably tracked. This allows for faster processing as decreasing exposure will increase your camera FPS. For reflective pipelines, after adjusting exposure and brightness, the target should be lit green (or the color of the vision tracking LEDs used). The more distinct the color of the target, the more likely it will be tracked reliably. Note Unlike with retroreflective tape, AprilTag tracking is not very dependent on lighting consistency. If you have trouble detecting tags due to low light, you may want to try increasing exposure, but this will likely decrease your achievable framerate. AprilTags and Motion Blur  For AprilTag pipelines, your goal is to reduce the “motion blur” as much as possible. Motion blur is the visual streaking/smearing on the camera stream as a result of movement of the camera or object of focus. You want to mitigate this as much as possible because your robot is constantly moving and you want to be able to read as many tags as you possibly can. The possible solutions to this include: Cranking your exposure as low as it goes and increasing your gain/brightness. This will decrease the effects of motion blur and increase FPS. Using a global shutter (as opposed to rolling shutter) camera. This should eliminate most, if not all motion blur. Only rely on tags when not moving. Orientation  Orientation can be used to rotate the image prior to vision processing. This can be useful for cases where the camera is not oriented parallel to the ground. Do note that this operation can in some cases significantly reduce FPS. Stream Resolution  This changes the resolution which is used to stream frames from PhotonVision. This does not change the resolution used to perform vision processing. This is useful to reduce bandwidth consumption on the field. In some high-resolution cases, decreasing stream resolution can increase processing FPS.",
      "content_preview": "Camera Tuning / Input  PhotonVision’s “Input” tab contains settings that affect the image captured by the currently selected camera. This includes camera exposure and brightness, as well as resolution and orientation. Resolution  Resolution changes the resolution of the image captured."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/pipelines/about-pipelines.html",
      "title": "About Pipelines",
      "section": "Getting Started",
      "language": "All",
      "content": "About Pipelines  What is a pipeline?  A vision pipeline represents a series of steps that are used to acquire an image, process it, and analyzing it to find a target. In most FRC games, this means processing an image in order to detect a piece of retroreflective tape or an AprilTag. Types of Pipelines  AprilTag / ArUco  This pipeline type is based on detecting AprilTag fiducial markers. More information about AprilTags can be found in the WPILib documentation . This pipeline provides easy to use 3D pose information which allows localization. Note In order to get 3D Pose data about AprilTags, you are required to calibrate your camera . Object Detection  This pipeline type is based on detecting objects using a neural network. The object detection pipeline uses a pre-trained model to detect objects in the camera stream. Note This pipeline type is only supported on the Orange Pi 5/5+ coprocessors due to its Neural Processing Unit used by PhotonVision to support running ML-based object detection. Driver Mode  Driver Mode is a type of pipeline that doesn’t run any vision processing, intended for human viewing. For more information about Driver Mode, see the Driver Mode documentation . Colored Shape  This pipeline type is based on detecting different shapes like circles, triangles, quadrilaterals, or a polygon. An example usage would be detecting yellow PowerCells from the 2020 FRC game. You can read more about the specific settings available in the contours page. Reflective  This pipeline type is based on detecting targets with reflective tape. In the contours tab of this pipeline type, you can filter the area, width/height ratio, fullness, degree of speckle rejection. Note This pipeline type is not used anymore due to FRC’s removal of retro-reflective tape from the game. It is still available as a pipeline for legacy purposes. Note About Multiple Cameras and Pipelines  When using more than one camera, it is important to keep in mind that all cameras run one pipeline each, all publish to NT, and all send both streams. This will have a noticeable affect on performance and we recommend users limit themselves to 1-2 cameras per coprocessor. Pipeline Configuration  Each pipeline has a set of tabs that are used to configure the pipeline. All pipelines follow a similar structure with an Input and Output tab, as well as a set of tabs that are specific to the pipeline type. Input: This tab allows the raw camera image to be modified before it gets processed. Here, you can set exposure, brightness, gain, orientation, and resolution. Output: This allows you to manipulate the detected target via the target offset point (for calculating pitch/yaw) and robot (crosshair) offset. In addition, it allows users to send additional (up to 5) outputs through PhotonLib. Pipielines also have additional tabs that are specific to the pipeline type. Listed below are the tabs for each pipeline type. AprilTag / ArUco Pipelines  AprilTag: This tab includes AprilTag specific tuning parameters, such as decimate, blur, threads, pose iterations, and more. Object Detection Pipelines  Object Detection: This tab allows you to filter results from the neural network, such as confidence, area, and width/height ratio. The end goal of this tab is to filter out any false positives. Reflective and Colored Shape Pipelines  Threshold: This tab allows you to filter out specific colors/pixels in your camera stream through HSV tuning. The end goal here is having a black and white image that will only have your target lit up. Contours: After thresholding, contiguous white pixels are grouped together, and described by a curve that outlines the group. This curve is called a “contour” which represent various targets on your screen. Regardless of type, you can filter how the targets are grouped, their intersection, and how the targets are sorted. Other available filters will change based on different pipeline types.",
      "content_preview": "About Pipelines  What is a pipeline?  A vision pipeline represents a series of steps that are used to acquire an image, process it, and analyzing it to find a target. In most FRC games, this means processing an image in order to detect a piece of retroreflective tape or an AprilTag."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/driver-mode/index.html",
      "title": "Driver Mode",
      "section": "General",
      "language": "All",
      "content": "Driver Mode  Driver Mode is a type of pipeline that doesn’t run any vision processing, intended for viewing from a human. Enabling Driver Mode  To enable Driver Mode, toggle the switch at the top of the Dashboard page for a selected camera. Alternatively, visit the camera settings page and toggle the “Driver Mode” switch for a selected camera. Hiding the Crosshair  When Driver Mode is enabled, a green crosshair will be shown at the center of the camera stream. If you do not want to show the green crosshair at the center of the camera stream, toggle the “Crosshair” switch under the Input tab, as shown in the image below.",
      "content_preview": "Driver Mode  Driver Mode is a type of pipeline that doesn’t run any vision processing, intended for viewing from a human. Enabling Driver Mode  To enable Driver Mode, toggle the switch at the top of the Dashboard page for a selected camera."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/advanced-installation/sw_install/windows-pc.html",
      "title": "Windows PC Installation",
      "section": "General",
      "language": "All",
      "content": "Windows PC Installation  PhotonVision may be run on a Windows Desktop PC for basic testing and evaluation. Note You do not need to install PhotonVision on a Windows PC in order to access the webdashboard (assuming you are using an external coprocessor like a Raspberry Pi). Install Bonjour  Bonjour provides more stable networking when using Windows PCs. Install Bonjour here before continuing to ensure a stable experience while using PhotonVision. Installing Java  PhotonVision requires a JDK installed and on the system path. JDK 17 is needed. Windows Users must use the JDK that ships with WPILib. Download and install it from here. Either ensure the only Java on your PATH is the WPILIB Java or specify it to gradle with -Dorg.gradle.java.home=C:\\Users\\Public\\wpilib\\2025\\jdk : > ./ gradlew run \"-Dorg.gradle.java.home=C:\\Users\\Public\\wpilib \\202 5\\jdk\" Warning Using a JDK other than WPILIB’s JDK17 will cause issues when running PhotonVision and is not supported. Downloading the Latest Stable Release of PhotonVision  Go to the GitHub releases page and download the winx64.jar file. Running PhotonVision  To run PhotonVision, open a terminal window of your choice and run the following command: > java - jar C : \\ path \\ to \\ photonvision \\ NAME OF JAR FILE GOES HERE . jar If your computer has a compatible webcam connected, PhotonVision should startup without any error messages. If there are error messages, your webcam isn’t supported or another issue has occurred. If it is the latter, please open an issue on the PhotonVision issues page . Warning Using an integrated laptop camera may cause issues when trying to run PhotonVision. If you are unable to run PhotonVision on a laptop with an integrated camera, try disabling the camera’s driver in Windows Device Manager. Accessing the PhotonVision Interface  Once the Java backend is up and running, you can access the main vision interface by navigating to localhost:5800 inside your browser.",
      "content_preview": "Windows PC Installation  PhotonVision may be run on a Windows Desktop PC for basic testing and evaluation. Note You do not need to install PhotonVision on a Windows PC in order to access the webdashboard (assuming you are using an external coprocessor like a Raspberry Pi)."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/advanced-installation/sw_install/linux-pc.html",
      "title": "Linux PC Installation",
      "section": "General",
      "language": "All",
      "content": "Linux PC Installation  PhotonVision may be run on a Debian-based Linux Desktop PC for basic testing and evaluation. Note You do not need to install PhotonVision on a Windows PC in order to access the webdashboard (assuming you are using an external coprocessor like a Raspberry Pi). Installing Java  PhotonVision requires a JDK installed and on the system path. JDK 17 is needed (different versions will not work). If you don’t have JDK 17 already, run the following to install it: $ sudo apt-get install openjdk-17-jdk Warning Using a JDK other than JDK17 will cause issues when running PhotonVision and is not supported. Downloading the Latest Stable Release of PhotonVision  Go to the GitHub releases page and download the relevant .jar file for your coprocessor. Note If your coprocessor has a 64 bit ARM based CPU architecture (OrangePi, Raspberry Pi, etc.), download the LinuxArm64.jar file. If your coprocessor has an 64 bit x86 based CPU architecture (Mini PC, laptop, etc.), download the Linuxx64.jar file. Warning Be careful to pick the latest stable release. “Draft” or “Pre-Release” versions are not stable and often have bugs. Running PhotonVision  To run PhotonVision, open a terminal window of your choice and run the following command: $ java -jar /path/to/photonvision/photonvision-xxx.jar If your computer has a compatible webcam connected, PhotonVision should startup without any error messages. If there are error messages, your webcam isn’t supported or another issue has occurred. If it is the latter, please open an issue on the PhotonVision issues page . Accessing the PhotonVision Interface  Once the Java backend is up and running, you can access the main vision interface by navigating to localhost:5800 inside your browser.",
      "content_preview": "Linux PC Installation  PhotonVision may be run on a Debian-based Linux Desktop PC for basic testing and evaluation. Note You do not need to install PhotonVision on a Windows PC in order to access the webdashboard (assuming you are using an external coprocessor like a Raspberry Pi)."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/advanced-installation/sw_install/other-coprocessors.html",
      "title": "Other Debian",
      "section": "General",
      "language": "All",
      "content": "Other Debian-Based Co-Processor Installation  Warning Working with unsupported coprocessors requires some level of “know how” of your system. The install script has only been tested on Debian/Raspberry Pi OS Buster and Ubuntu Bionic. If any issues arise with your specific OS, please open an issue on our issues page . Note We’d love to have your input! If you get PhotonVision working on another coprocessor, consider documenting your steps and submitting a docs issue ., pull request , or ping us on Discord . For example, Limelight and Romi install instructions came about because someone spent the time to figure it out, and did a writeup. Installing PhotonVision  We provide an install script for other Debian-based systems (with apt ) that will automatically install PhotonVision and make sure that it runs on startup. $ wget https://git.io/JJrEP -O install.sh $ sudo chmod +x install.sh $ sudo ./install.sh $ sudo reboot now Note Your co-processor will require an Internet connection for this process to work correctly. For installation on any other co-processors, we recommend reading the advanced command line documentation . Updating PhotonVision  PhotonVision can be updated by downloading the latest jar file, copying it onto the processor, and restarting the service. For example, from another computer, run the following commands. Substitute the correct username for “[user]” ( Provided images use username “pi”) $ scp [ jar name ] .jar [ user ] @photonvision.local:~/ $ ssh [ user ] @photonvision.local $ sudo mv [ jar name ] .jar /opt/photonvision/photonvision.jar $ sudo systemctl restart photonvision.service",
      "content_preview": "Other Debian-Based Co-Processor Installation  Warning Working with unsupported coprocessors requires some level of “know how” of your system. The install script has only been tested on Debian/Raspberry Pi OS Buster and Ubuntu Bionic."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/advanced-installation/sw_install/advanced-cmd.html",
      "title": "Advanced Command Line Usage",
      "section": "General",
      "language": "All",
      "content": "Advanced Command Line Usage  PhotonVision exposes some command line options which may be useful for customizing execution on Debian-based installations. Running a JAR File  Assuming java has been installed, and the appropriate environment variables have been set upon installation (a package manager like apt should automatically set these), you can use java -jar to run a JAR file. If you downloaded the latest stable JAR of PhotonVision from the GitHub releases page , you can run the following to start the program: java -jar /path/to/photonvision/photonvision.jar Updating a JAR File  When you need to update your JAR file, run the following: wget https://git.io/JqkQ9 -O update.sh sudo chmod +x update.sh sudo ./update.sh sudo reboot now Creating a systemd Service  You can also create a systemd service that will automatically run on startup. To do so, first navigate to /lib/systemd/system . Create a file called photonvision.service (or name it whatever you want) using touch photonvision.service . Then open this file in the editor of your choice and paste the following text: [ Unit ] Description = Service that runs PhotonVision [ Service ] WorkingDirectory =/ path / to / photonvision # Optional: run photonvision at \"nice\" -10, which is higher priority than standard # Nice=-10 ExecStart =/ usr / bin / java - jar / path / to / photonvision / photonvision . jar [ Install ] WantedBy = multi - user . target Then copy the .service file to /etc/systemd/system/ using cp photonvision.service /etc/systemd/system/photonvision.service . Then modify the file to have 644 permissions using chmod 644 /etc/systemd/system/photonvision.service . Note Many ARM processors have a big.LITTLE architecture where some of the CPU cores are more powerful than others. On this type of architecture, you may get more consistent performance by limiting which cores PhotonVision can use. To do this, add the parameter AllowedCPUs to the systemd service file in the [Service] section. For instance, for an Orange Pi 5, cores 4 through 7 are the fast ones, and you can target those cores with the line AllowedCPUs=4-7 . Installing the systemd Service  To install the service, simply run systemctl enable photonvision.service . Note It is recommended to reload configurations by running systemctl daemon-reload .",
      "content_preview": "Advanced Command Line Usage  PhotonVision exposes some command line options which may be useful for customizing execution on Debian-based installations."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/advanced-installation/prerelease-software.html",
      "title": "Installing Pre",
      "section": "General",
      "language": "All",
      "content": "Installing Pre-Release Versions  Pre-release/development version of PhotonVision can be tested by installing/downloading artifacts from Github Actions (see below), which are built automatically on commits to open pull requests and to PhotonVision’s main branch, or by compiling PhotonVision locally . Warning If testing a pre-release version of PhotonVision with a robot, PhotonLib must be updated to match the version downloaded! If not, packet schema definitions may not match and unexpected things will occur. To update PhotonLib, refer to installing specific version of PhotonLib . GitHub Actions builds pre-release version of PhotonVision automatically on PRs and on each commit merged to main. To test a particular commit to main, navigate to the PhotonVision commit list and click on the check mark (below). Scroll to “Build / Build fat JAR - PLATFORM”, click details, and then summary. From here, JAR and image files can be downloaded to be flashed or uploaded using “Offline Update”. Built JAR files (but not image files) can also be downloaded from PRs before they are merged. Navigate to the PR in GitHub, and select Checks at the top. Click on “Build” to display the same artifact list as above.",
      "content_preview": "Installing Pre-Release Versions  Pre-release/development version of PhotonVision can be tested by installing/downloading artifacts from Github Actions (see below), which are built automatically on commits to open pull requests and to PhotonVision’s main branch, or by compiling PhotonVision locally..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/advanced-installation/sw_install/index.html",
      "title": "Software Installation",
      "section": "General",
      "language": "All",
      "content": "Software Installation  Desktop Environments  Windows PC Installation Linux PC Installation Mac OS Installation Other  Other Debian-Based Co-Processor Installation Advanced Command Line Usage Romi Installation",
      "content_preview": "Software Installation  Desktop Environments  Windows PC Installation Linux PC Installation Mac OS Installation Other  Other Debian-Based Co-Processor Installation Advanced Command Line Usage Romi Installation"
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/advanced-installation/sw_install/mac-os.html",
      "title": "Mac OS Installation",
      "section": "General",
      "language": "All",
      "content": "Mac OS Installation  Warning Due to current cscore restrictions, the PhotonVision server backend may have issues running macOS. Note You do not need to install PhotonVision on a Mac in order to access the webdashboard (assuming you are using an external coprocessor like a Raspberry Pi). VERY Limited macOS support is available. Installing Java  PhotonVision requires a JDK installed and on the system path. JDK 17 is needed (different versions will not work). You may already have this if you have installed WPILib 2025+. If not, download and install it from here . Warning Using a JDK other than JDK17 will cause issues when running PhotonVision and is not supported. Downloading the Latest Stable Release of PhotonVision  Go to the GitHub releases page and download the relevant .jar file for your coprocessor. Note If you have an M Series Mac, download the macarm64.jar file. If you have an Intel based Mac, download the macx64.jar file. Warning Be careful to pick the latest stable release. “Draft” or “Pre-Release” versions are not stable and often have bugs. Running PhotonVision  To run PhotonVision, open a terminal window of your choice and run the following command: $ java -jar /path/to/photonvision/photonvision-xxx.jar Warning Due to current cscore restrictions, the PhotonVision using test mode is all that is known to work currently. Accessing the PhotonVision Interface  Once the Java backend is up and running, you can access the main vision interface by navigating to localhost:5800 inside your browser. Warning Due to current cscore restrictions, it is unlikely any streams will open from real webcams.",
      "content_preview": "Mac OS Installation  Warning Due to current cscore restrictions, the PhotonVision server backend may have issues running macOS. Note You do not need to install PhotonVision on a Mac in order to access the webdashboard (assuming you are using an external coprocessor like a Raspberry Pi)."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/hardware/index.html",
      "title": "Hardware Selection",
      "section": "Hardware Selection",
      "language": "All",
      "content": "Hardware Selection  Selecting Hardware Choosing a Coprocessor Choosing a Camera Performance Matrix Deploying on Custom Hardware Configuration LED Support Hardware Interaction Commands Known Camera FOV Cosmetic & Branding Example",
      "content_preview": "Hardware Selection  Selecting Hardware Choosing a Coprocessor Choosing a Camera Performance Matrix Deploying on Custom Hardware Configuration LED Support Hardware Interaction Commands Known Camera FOV Cosmetic & Branding Example"
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/hardware/customhardware.html",
      "title": "Deploying on Custom Hardware",
      "section": "Hardware Selection",
      "language": "All",
      "content": "Deploying on Custom Hardware  Configuration  By default, PhotonVision attempts to make minimal assumptions of the hardware it runs on. However, it may be configured to enable custom LED control, branding, and other functionality. hardwareConfig.json is the location for this configuration. It is included when settings are exported, and can be uploaded as part of a .zip, or on its own. LED Support  For Raspberry-Pi based hardware, PhotonVision can use PiGPIO to control IO pins. The mapping of which pins control which LED’s is part of the hardware config. The pins are active-high: set high when LED’s are commanded on, and set low when commanded off. JSON { \"ledPins\" : [ 13 ], \"ledSetCommand\" : \"\" , \"ledsCanDim\" : true , \"ledPWMRange\" : [ 0 , 100 ], \"ledPWMSetRange\" : \"\" , \"ledPWMFrequency\" : 0 , \"ledDimCommand\" : \"\" , \"ledBlinkCommand\" : \"\" , \"statusRGBPins\" : [ ], } Note No hardware boards with status RGB LED pins or non-dimming LED’s have been tested yet. Please reach out to the development team if these features are desired, they can assist with configuration and testing. Hardware Interaction Commands  For Non-Raspberry-Pi hardware, users must provide valid hardware-specific commands for some parts of the UI interaction (including performance metrics, and executing system restarts). Leaving a command blank will disable the associated functionality. JSON { \"cpuTempCommand\" : \"\" , \"cpuMemoryCommand\" : \"\" , \"cpuUtilCommand\" : \"\" , \"gpuMemoryCommand\" : \"\" , \"gpuTempCommand\" : \"\" , \"ramUtilCommand\" : \"\" , \"restartHardwareCommand\" : \"\" , } Note These settings have no effect if PhotonVision detects it is running on a Raspberry Pi. See the MetricsBase class for the commands utilized. Known Camera FOV  If your hardware contains a camera with a known field of vision, it can be entered into the hardware configuration. This will prevent users from editing it in the GUI. JSON { \"vendorFOV\" : 98.9 } Cosmetic & Branding  To help differentiate your hardware from other solutions, some customization is allowed. JSON { \"deviceName\" : \"Super Cool Custom Hardware\" , \"deviceLogoPath\" : \"\" , \"supportURL\" : \"https://cat-bounce.com/\" , } Note Not all configuration is currently presented in the User Interface. Additional file uploads may be needed to support custom images. Example  Here is a complete example hardwareConfig.json : JSON { \"deviceName\" : \"Blinky McBlinkface\" , \"deviceLogoPath\" : \"\" , \"supportURL\" : \"https://www.youtube.com/watch?v=b-CvLWbfZhU\" , \"ledPins\" : [ 2 , 13 ], \"ledSetCommand\" : \"\" , \"ledsCanDim\" : true , \"ledPWMRange\" : [ 0 , 100 ], \"ledPWMSetRange\" : \"\" , \"ledPWMFrequency\" : 0 , \"ledDimCommand\" : \"\" , \"ledBlinkCommand\" : \"\" , \"statusRGBPins\" : [ ], \"cpuTempCommand\" : \"\" , \"cpuMemoryCommand\" : \"\" , \"cpuUtilCommand\" : \"\" , \"gpuMemoryCommand\" : \"\" , \"gpuTempCommand\" : \"\" , \"ramUtilCommand\" : \"\" , \"restartHardwareCommand\" : \"\" , \"vendorFOV\" : 72.5 }",
      "content_preview": "Deploying on Custom Hardware  Configuration  By default, PhotonVision attempts to make minimal assumptions of the hardware it runs on. However, it may be configured to enable custom LED control, branding, and other functionality. hardwareConfig.json is the location for this configuration."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/hardware/selecting-hardware.html",
      "title": "Selecting Hardware",
      "section": "Hardware Selection",
      "language": "All",
      "content": "Selecting Hardware  Note See the quick start guide , for latest, specific recommendations on hardware to use for PhotonVision. In order to use PhotonVision, you need a coprocessor and a camera. This page discusses the specifics of why that hardware is recommended. Choosing a Coprocessor  Minimum System Requirements  Ubuntu 22.04 LTS or Windows 10/11 We don’t recommend using Windows for anything except testing out the system on a local machine. CPU: ARM Cortex-A53 (the CPU on Raspberry Pi 3) or better At least 8GB of storage 2GB of RAM PhotonVision isn’t very RAM intensive, but you’ll need at least 2GB to run the OS and PhotonVision. The following IO: At least 1 USB or MIPI-CSI port for the camera Note that we only support using the Raspberry Pi’s MIPI-CSI port, other MIPI-CSI ports from other coprocessors will probably not work. Ethernet port for networking Note these are bare minimums. Most high-performance vision processing will require higher specs. Coprocessor Recommendations  Vision processing on one camera stream is usually a CPU-bound operation. Some operations are able to be done in parallel, but not all. USB bandwidth and network data transfer also cause a fixed overhead. Faster CPU’s generally result in lower latency, but eventually with diminishing returns. More cores allow for some improvement, especially if multiple camera streams are being processed. PhotonVision is most commonly tested around Raspbian (Debian-based) operating systems. Other coprocessors can be used but may require some extra work / command line usage in order to get it working properly. Power Supply  Coprocessors need a steady, regulated power supply. Under-volting the processor will result in CPU throttling, low performance, unexpected reboots, and sometimes electrical damage. Many coprocessors draw 5-10 amps of current. Be sure to select a power supply which regulate’s the robot’s variable battery voltage into something steady that the robot can use. Storage Media  Most single-board computer coprocessors use micro SD cards as their storage media. Three important considerations include total storage space, read/write speed, and robustness. PhotonVision is not usually disk-bound, other than during coprocessor boot-up and initial startup. Some disk writing is done at runtime for logging, settings, and saving camera images on command. Better storage space and read/write speed mostly matter if image capture is used frequently on the field. Industrial-grade SD cards are recommended for their stability under shock, vibration, variable voltage, and power-off. Raspberry Pi and Orange Pi coprocessors are generally robust against robot power interruptions, teams have anecdotally reported that Sandisk industrial SD cards reduce the chances of an unexpected settings or log file corruption on shutdown. Choosing a Camera  PhotonVision relies on CSCore to detect and process cameras, so camera support is determined based off compatibility with CScore along with native support for the camera within your OS (ex. V4L compatibility ). PhotonVision attempts to support most USB Cameras. Exceptions include: All Logitech brand cameras Logitech uses a non-standard driver which is not currently supported Built-in webcams Driver support is too varied. Some may happen to work, but most have been found to be non-functional virtual cameras (OBS, Snapchat camera, etc.) PhotonVision assumes the camera has real physical hardware to control - these do not expose the minimum number of controls. Use caution when using multiple identical cameras, as only the physical USB port they are plugged into can differentiate them. PhotonVision provides a “strict matching” setting which can reduce errors related to identical cameras. Arducam has a tool that allows for identical cameras to be renamed by their physical location or purpose. Cameras Attributes  For colored shape detection, any non-fisheye camera supported by PhotonVision will work. For driver camera, we recommend a USB camera with a fisheye lens, so your driver can see more of the field. Use the minimum acceptable resolution to help keep latency low. For AprilTag detection, we recommend you use a camera that has ~100 degree diagonal FOV. This will allow you to see more AprilTags in frame, and will allow for more accurate pose estimation. You also want a camera that supports high FPS, as this will allow you to update your pose estimator at a higher frequency. For object detection, we recommend a USB camera. Some fisheye lenses may be ok, but very wide angle cameras may distort the gamepiece beyond recognition. Global shutter cameras are recommended in all cases, to reduce rolling-shutter image sheer while the robot is moving. Cameras capable of capturing a good image with very short exposures will also help reduce image blur. Usually, high-FPS-capable cameras designed for computer vision are better at this than “consumer-grade” USB webcams. Using Multiple Cameras  Keeping the target(s) in view of the robot often requires more than one camera. PhotonVision has no hardcoded limit on the number of cameras supported. The limit is usually dependant on CPU (can all frames be processed fast enough?) and USB bandwidth (Can all cameras send their images without overwhelming the bus?). Note that cameras are not synchronized together. Frames are captured and processed asynchronously. Robot Code must fuse estimates together. For more information, see the programming reference. . Performance Matrix  Loading… Please submit performance data to be added to the matrix here: Loading…",
      "content_preview": "Selecting Hardware  Note See the quick start guide , for latest, specific recommendations on hardware to use for PhotonVision. In order to use PhotonVision, you need a coprocessor and a camera. This page discusses the specifics of why that hardware is recommended."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/quick-start/common-setups.html",
      "title": "Common Hardware Setups",
      "section": "Getting Started",
      "language": "All",
      "content": "Common Hardware Setups  PhotonVision requires dedicated hardware, above and beyond a roboRIO. This page lists hardware that is frequently used with PhotonVision. Coprocessors  Orange Pi 5 4GB Supports up to 2 object detection streams, along with 2 AprilTag streams at 1280x800 (30fps). Raspberry Pi 5 2GB Supports up to 2 AprilTag streams at 1280x800 (30fps). Note The Orange Pi 5 is the only currently supported device for object detection. SD Cards  8GB or larger micro SD card Important Industrial grade SD cards from major manufacturers are recommended for robotics applications. For example: Sandisk SDSDQAF3-016G-I . Cameras  Innomaker and Arducam are common manufacturers of hardware designed specifically for vision processing. AprilTag Detection OV9281 Object Detection OV9782 Driver Camera OV9281 OV9782 Pi Camera Module V1 (More setup info) Feel free to get started with any color webcam you have sitting around. Power  Pololu S13V30F5 Regulator Redux Robotics Zinc-V Regulator See (Selecting Hardware) for info on why these are recommended.",
      "content_preview": "Common Hardware Setups  PhotonVision requires dedicated hardware, above and beyond a roboRIO. This page lists hardware that is frequently used with PhotonVision. Coprocessors  Orange Pi 5 4GB Supports up to 2 object detection streams, along with 2 AprilTag streams at 1280x800 (30fps)."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/camera-specific-configuration/picamconfig.html",
      "title": "Pi Camera Configuration",
      "section": "Camera Configuration",
      "language": "All",
      "content": "Pi Camera Configuration  This page covers specifics about the Raspberry Pi CSI camera configuration. Background  The Raspberry Pi CSI Camera port is routed through and processed by the GPU. Since the GPU boots before the CPU, it must be configured properly for the attached camera. Additionally, this configuration cannot be changed without rebooting. The GPU is not always capable of detecting other cameras automatically. The file /boot/config.txt is parsed by the GPU at boot time to determine what camera, if any, is expected to be attached. This file must be updated for some cameras. Warning Incorrect camera configuration will cause the camera to not be detected. It looks exactly the same as if the camera was unplugged. Updating config.txt  After flashing the pi image onto an SD card, open the boot segment in a file browser. Note Windows may report “There is a problem with this drive”. This should be ignored. Locate config.txt in the folder, and open it with your favorite text editor. Within the file, find this block of text: ############################################################## ### PHOTONVISION CAM CONFIG ### Comment/Uncomment to change which camera is supported ### Picam V1, V2 or HQ: uncomment (remove leading # ) from camera_auto_detect=1, ### and comment out all following lines ### IMX290/327/OV9281/Any other cameras that require additional overlays: ### Comment out (add a # ) to camera_auto_detect=1, and uncomment the line for ### the sensor you're trying to user cameraAutoDetect = 1 # dtoverlay=imx290,clock-frequency=74250000 # dtoverlay=imx290,clock-frequency=37125000 # dtoverlay=imx378 # dtoverlay=ov9281 ############################################################## Remove the leading # character to uncomment the line associated with your camera. Add a # in front of other cameras. Warning Leave lines outside the PhotonVision Camera Config block untouched. They are necessary for proper raspberry pi functionality. Save the file, close the editor, and eject the drive. The boot configuration should now be ready for your selected camera. Additional Information  See the libcamera documentation for more details on configuring cameras.",
      "content_preview": "Pi Camera Configuration  This page covers specifics about the Raspberry Pi CSI camera configuration. Background  The Raspberry Pi CSI Camera port is routed through and processed by the GPU. Since the GPU boots before the CPU, it must be configured properly for the attached camera."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/quick-start/index.html",
      "title": "Quick Start",
      "section": "Getting Started",
      "language": "All",
      "content": "Quick Start  Common Hardware Setups Coprocessors SD Cards Cameras Power Quick Installation Guide Raspberry Pi and Orange Pi Installation Limelight Installation Rubik Pi 3 Installation Wiring Coprocessor with regulator Limelight Coprocessor with Passive POE (Pi with SnakeEyes) Off-Robot Wiring Networking Physical Networking Network Hostname Robot Networking Port Forwarding Camera Stream Ports SSH Access Camera Matching Activating and Deactivating Cameras Deleting Cameras Matching Cameras Camera Calibration Print the Calibration Target Prepare the Calibration Target Calibrate your Camera Camera Focusing Prepare Camera Using Focus Mode Quick Configure Settings to configure Pipeline Settings",
      "content_preview": "Quick Start  Common Hardware Setups Coprocessors SD Cards Cameras Power Quick Installation Guide Raspberry Pi and Orange Pi Installation Limelight Installation Rubik Pi 3 Installation Wiring Coprocessor with regulator Limelight Coprocessor with Passive POE (Pi with SnakeEyes) Off-Robot Wiring..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/quick-start/quick-configure.html",
      "title": "Quick Configure",
      "section": "Getting Started",
      "language": "All",
      "content": "Quick Configure  Settings to configure  Team number  In order for photonvision to connect to the roborio it needs to know your team number. Camera Nickname  You must nickname your cameras in PhotonVision to ensure that every camera has a unique name. This is how you will identify cameras in robot code. The camera can be nicknamed using the edit button next to the camera name in the upper right of the Dashboard tab. Pipeline Settings  AprilTag  When using an Orange Pi 5 with an Arducam OV9281 teams will usually change the following settings. For more info on AprilTag settings please review this . Resolution: 1280x800 Decimate: 2 Mode: 3D Exposure and Gain: Adjust these to achieve good brightness without flicker and low motion blur. This may vary based on lighting conditions in your competition environment. Enable MultiTag Set arducam specific camera type selector to OV9281 AprilTags and Motion Blur and Rolling Shutter  When detecting AprilTags, it’s important to minimize ‘motion blur’ as much as possible. Motion blur appears as visual streaking or smearing in the camera feed, resulting from the movement of either the camera or the object in focus. Reducing this effect is essential, as the robot is often in motion, and a clearer image allows for detecting as many tags as possible. This is not to be confused with rolling shutter . Fixes Lower your exposure as low as possible. Using gain and brightness to account for lack of brightness. Other Options: Don’t use/rely on vision measurements while moving. Object Detection  When using an Orange Pi 5 with an OV9782 teams will usually change the following settings. For more info on object detection settings please review this . Resolution: Resolutions higher than 640x640 may not result in any higher detection accuracy and may lower performance . Confidence: 0.75 - 0.95 Lower values are for detecting worn game pieces or less ideal game pieces. Higher for less worn, more ideal game pieces. White Balance Temperature: Adjust this to achieve better color accuracy. This may be needed to increase confidence. Set arducam specific camera type selector to OV9782",
      "content_preview": "Quick Configure  Settings to configure  Team number  In order for photonvision to connect to the roborio it needs to know your team number. Camera Nickname  You must nickname your cameras in PhotonVision to ensure that every camera has a unique name."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/objectDetection/about-object-detection.html",
      "title": "About Object Detection",
      "section": "Getting Started",
      "language": "All",
      "content": "About Object Detection  How does it work?  PhotonVision supports object detection using neural network accelerator hardware, commonly known as an NPU. The two coprocessors currently supported are the Orange Pi 5 and the Rubik Pi 3 . PhotonVision currently ships with a model trained on the COCO dataset by Ultralytics (this model is licensed under AGPLv3 ). This model is meant to be used for testing and other miscellaneous purposes. It is not meant to be used in competition. For the 2025 post-season, PhotonVision also ships with a pretrained ALGAE model. A model to detect coral is available in the PhotonVision discord, but will not be distributed with PhotonVision. Tracking Objects  Before you get started with object detection, ensure that you have followed the previous sections on installation, wiring, and networking. Next, open the Web UI, go to the top right card, and switch to the “Object Detection” type. You should see a screen similar to the image above. Models are trained to detect one or more object “classes” (such as cars, stoplights) in an input image. For each detected object, the model outputs a bounding box around where in the image the object is located, what class the object belongs to, and a unitless confidence between 0 and 1. Note This model output means that while its fairly easy to say that “this rectangle probably contains an object”, we don’t have any information about the object’s orientation or location. Further math in user code would be required to make estimates about where an object is physically located relative to the camera. Tuning and Filtering  Compared to other pipelines, object detection exposes very few tuning handles. The Confidence slider changes the minimum confidence that the model needs to have in a given detection to consider it valid, as a number between 0 and 1 (with 0 meaning completely uncertain and 1 meaning maximally certain). The Non-Maximum Suppresion (NMS) Threshold slider is used to filter out overlapping detections. Higher values mean more detections are allowed through, but may result in false positives. It’s generally recommended that teams leave this set at the default, unless they find they’re unable to get usable results with solely the Confidence slider. Your browser does not support the video tag. The same area, aspect ratio, and target orientation/sort parameters from reflective pipelines are also exposed in the object detection card. Letterboxing  Photonvision will letterbox your camera frame to 640x640. This means that if you select a resolution that is larger than 640 it will be scaled down to fit inside a 640x640 frame with black bars if needed. Smaller frames will be scaled up with black bars if needed. It is recommended that you select a resolution that results in the smaller dimension being just greater than, or equal to, 640. Anything above this will not see any increased performance. Custom Models  For information regarding converting custom models and supported models for each platform, refer to the page detailing information about your specific coprocessor. Orange Pi 5 Rubik Pi 3 Training Custom Models  PhotonVision does not offer any support for training custom models, only conversion. For information on which models are supported for a given coprocessor, use the links above. Managing Custom Models  Custom models can now be managed from the Object Detection tab in settings. You can upload a custom model by clicking the “Upload Model” button, selecting your model file, and filling out the property fields. Models can also be exported, both individually and in bulk. Models exported in bulk can be imported using the import bulk button. Models exported individually must be re-imported as an individual model, and all the relevant metadata is stored in the filename of the model.",
      "content_preview": "About Object Detection  How does it work?  PhotonVision supports object detection using neural network accelerator hardware, commonly known as an NPU. The two coprocessors currently supported are the Orange Pi 5 and the Rubik Pi 3 ."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/objectDetection/opi.html",
      "title": "Orange Pi 5 (and variants) Object Detection",
      "section": "Object Detection",
      "language": "All",
      "content": "Orange Pi 5 (and variants) Object Detection  How it works  PhotonVision runs object detection on the Orange Pi 5 by use of the RKNN model architecture, and this JNI code . Supported models  PhotonVision currently ONLY supports 640x640 Ultralytics YOLOv5, YOLOv8, and YOLOv11 models trained and converted to .rknn format for RK3588 SOCs! Other models require different post-processing code and will NOT work. Converting Custom Models  Warning Only quantized models are supported, so take care when exporting to select the option for quantization. PhotonVision now ships with a Python Notebook that you can use in Google Colab or in a local Linux environment (since rknn-toolkit2 only supports Linux). In Google Colab, you can simply paste the PhotonVision GitHub URL into the “GitHub” tab and select the rknn_conversion.ipynb notebook without needing to manually download anything. Please ensure that the model you are attempting to convert is among the supported models and using the PyTorch format.",
      "content_preview": "Orange Pi 5 (and variants) Object Detection  How it works  PhotonVision runs object detection on the Orange Pi 5 by use of the RKNN model architecture, and this JNI code ."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/objectDetection/rubik.html",
      "title": "Rubik Pi 3 Object Detection",
      "section": "Object Detection",
      "language": "All",
      "content": "Rubik Pi 3 Object Detection  How it works  PhotonVision runs object detection on the Rubik Pi 3 by use of TensorflowLite , and this JNI code . Supported models  PhotonVision currently ONLY supports 640x640 Ultralytics YOLOv8 and YOLOv11 models trained and converted to .tflite format for QCS6490 SOCs! Other models require different post-processing code and will NOT work. Converting Custom Models  Warning Only quantized models are supported, so take care when exporting to select the option for quantization. PhotonVision now ships with a Python Notebook that you can use in Google Colab , Kaggle , or in a local environment. In Google Colab, you can simply paste the PhotonVision GitHub URL into the “GitHub” tab and select the rubik_conversion.ipynb notebook without needing to manually download anything. Please ensure that the model you are attempting to convert is among the supported models and using the PyTorch format. Benchmarking  Before you can perform benchmarking, it’s necessary to install tensorflow-lite-qcom-apps with apt. By SSHing into your Rubik Pi and running this command, replacing PATH/TO/MODEL with the path to your model, benchmark_model --graph=src/test/resources/yolov8nCoco.tflite --external_delegate_path=/usr/lib/libQnnTFLiteDelegate.so --external_delegate_options=backend_type:htp --external_delegate_options=htp_use_conv_hmx:1 --external_delegate_options=htp_performance_mode:2 you can determine how long it takes for inference to be performed with your model.",
      "content_preview": "Rubik Pi 3 Object Detection  How it works  PhotonVision runs object detection on the Rubik Pi 3 by use of TensorflowLite , and this JNI code . Supported models  PhotonVision currently ONLY supports 640x640 Ultralytics YOLOv8 and YOLOv11 models trained and converted to .tflite format for QCS6490..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/apriltag-pipelines/2D-tracking-tuning.html",
      "title": "2D AprilTag Tuning / Tracking",
      "section": "AprilTag Detection",
      "language": "All",
      "content": "2D AprilTag Tuning / Tracking  Tracking AprilTags  Before you get started tracking AprilTags, ensure that you have followed the previous sections on installation, wiring and networking. Next, open the Web UI, go to the top right card, and switch to the “AprilTag” or “ArUco” type. You should see a screen similar to the one below. You are now able to detect and track AprilTags in 2D (yaw, pitch, roll, etc.). In order to get 3D data from your AprilTags, please see here. Tuning AprilTags  AprilTag pipelines come with reasonable defaults to get you up and running with tracking. However, in order to optimize your performance and accuracy, you must tune your AprilTag pipeline using the settings below. Note that the settings below are different between the AprilTag and ArUco detectors but the concepts are the same. Target Family  Target families are defined by two numbers (before and after the h). The first number is the number of bits the tag is able to encode (which means more tags are available in the respective family) and the second is the hamming distance. Hamming distance describes the ability for error correction while identifying tag ids. A high hamming distance generally means that it will be easier for a tag to be identified even if there are errors. However, as hamming distance increases, the number of available tags decreases. The 2025 FRC game will be using 36h11 tags, which can be found here . Decimate  Decimation (also known as down-sampling) is the process of reducing the sampling frequency of a signal (in our case, the image). Increasing decimate will lead to an increased detection rate while decreasing detection distance. We recommend keeping this at the default value. Blur  This controls the sigma of Gaussian blur for tag detection. In clearer terms, increasing blur will make the image blurrier, decreasing it will make it closer to the original image. We strongly recommend that you keep blur to a minimum (0) due to it’s high performance intensity unless you have an extremely noisy image. Threads  Threads refers to the threads within your coprocessor’s CPU. The theoretical maximum is device dependent, but we recommend that users to stick to one less than the amount of CPU threads that your coprocessor has. Increasing threads will increase performance at the cost of increased CPU load, temperature increase, etc. It may take some experimentation to find the most optimal value for your system. Refine Edges  The edges of the each polygon are adjusted to “snap to” high color differences surrounding it. It is recommended to use this in tandem with decimate as it can increase the quality of the initial estimate. Pose Iterations  Pose iterations represents the amount of iterations done in order for the AprilTag algorithm to converge on its pose solution(s). A smaller number between 0-100 is recommended. A smaller amount of iterations cause a more noisy set of poses when looking at the tag straight on, while higher values much more consistently stick to a (potentially wrong) pair of poses. WPILib contains many useful filter classes in order to account for a noisy tag reading. Max Error Bits  Max error bits, also known as hamming distance, is the number of positions at which corresponding pieces of data / tag are different. Put more generally, this is the number of bits (think of these as squares in the tag) that need to be changed / corrected in the tag to correctly detect it. A higher value means that more tags will be detected while a lower value cuts out tags that could be “questionable” in terms of detection. We recommend a value of 0 for the 16h5 and at most 3 for the 36h11 family. Decision Margin Cutoff  The decision margin cutoff is how much “margin” the detector has left before it rejects a tag; increasing this rejects poorer tags. We recommend you keep this value around a 30.",
      "content_preview": "2D AprilTag Tuning / Tracking  Tracking AprilTags  Before you get started tracking AprilTags, ensure that you have followed the previous sections on installation, wiring and networking. Next, open the Web UI, go to the top right card, and switch to the “AprilTag” or “ArUco” type."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/apriltag-pipelines/3D-tracking.html",
      "title": "3D Tracking",
      "section": "AprilTag Detection",
      "language": "All",
      "content": "3D Tracking  3D AprilTag tracking will allow you to track the real-world position and rotation of a tag relative to the camera’s image sensor. This is useful for robot pose estimation and other applications like autonomous scoring. In order to use 3D tracking, you must first calibrate your camera . Once you have, you need to enable 3D mode in the UI and you will now be able to get 3D pose information from the tag! For information on getting and using this information in your code, see the programming reference . Ambiguity  Translating from 2D to 3D using data from the calibration and the four tag corners can lead to “pose ambiguity”, where it appears that the AprilTag pose is flipping between two different poses. You can read more about this issue here . Ambiguity is calculated as the ratio of reprojection errors between two pose solutions (if they exist), where reprojection error is the error corresponding to the image distance between where the apriltag’s corners are detected vs where we expect to see them based on the tag’s estimated camera relative pose. There are a few steps you can take to resolve/mitigate this issue: Mount cameras at oblique angles so it is less likely that the tag will be seen straight on. Use the MultiTag system in order to combine the corners from multiple tags to get a more accurate and unambiguous pose. Reject all tag poses where the ambiguity ratio (available via PhotonLib) is greater than 0.2.",
      "content_preview": "3D Tracking  3D AprilTag tracking will allow you to track the real-world position and rotation of a tag relative to the camera’s image sensor. This is useful for robot pose estimation and other applications like autonomous scoring."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/quick-start/camera-calibration.html",
      "title": "Camera Calibration",
      "section": "Getting Started",
      "language": "All",
      "content": "Camera Calibration  Important In order to detect AprilTags and use 3D mode, your camera must be calibrated at the desired resolution! Inaccurate calibration will lead to poor performance. If you’re not using cameras in 3D mode, calibration is optional, but it can still offer benefits. Calibrating cameras helps refine the pitch and yaw values, leading to more accurate positional data in every mode. For a more in-depth view . Print the Calibration Target  Downloaded from our demo site , or directly from your coprocessors cameras tab. Use the ChArUco calibration board: Board Type: ChAruCo Tag Family: 4x4 Pattern Spacing: 1.00in Marker Size: 0.75in Board Height : 8 Board Width : 8 Prepare the Calibration Target  Measure Accurately: Use calipers to measure the actual size of the squares and markers. Accurate measurements are crucial for effective calibration. Ensure Flatness: The calibration board must be perfectly flat, without any wrinkles or bends, to avoid introducing errors into the calibration process. Calibrate your Camera  Take lots of photos: It’s recommended to capture more than 50 images to properly calibrate your camera for accuracy. 12 is the bare minimum and may not provide good results. Other Tips Move the board not the camera. Take photos of lots of angles: The more angles the more better (up to 45 deg). A couple of up close images is good. Cover the entire cameras fov. Avoid images with the board facing straight towards the camera.",
      "content_preview": "Camera Calibration  Important In order to detect AprilTags and use 3D mode, your camera must be calibrated at the desired resolution! Inaccurate calibration will lead to poor performance. If you’re not using cameras in 3D mode, calibration is optional, but it can still offer benefits."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/quick-start/networking.html",
      "title": "Networking",
      "section": "Getting Started",
      "language": "All",
      "content": "Networking  Physical Networking  Warning When using PhotonVision off robot, you MUST plug the coprocessor into a physical router/radio. You can then connect your laptop/device used to view the webdashboard to the same network. Any other networking setup will not work and will not be supported in any capacity. New Radio (2025 - present) Danger Ensure that the radio’s DIP switches 1 and 2 are turned off; otherwise, the radio PoE feature may electrically destroy your coprocessor. More info. Old Radio (pre 2025) PhotonVision STRONGLY recommends the usage of a network switch on your robot. This is because the second radio port on the old FRC radios is known to be buggy and cause frequent connection issues that are detrimental during competition. An in-depth guide on how to install a network switch can be found on FRC 900’s website . Network Hostname  Rename each device from the default “photonvision” to a unique hostname (e.g., “Photon-OrangePi-Left” or “Photon-RPi5-Back”). This helps differentiate multiple coprocessors on your network, making it easier to manage them. Navigate to the settings page and scroll down to the network section. You will find the hostname is set to “photonvision” by default, this can only contain letters (A-Z), numeric characters (0-9), and the minus sign (-). Robot Networking  PhotonVision STRONGLY recommends the usage of Static IPs as it increases reliability on the field and when using PhotonVision in general. To properly set up your static IP, follow the steps below: Warning Only use a static IP when connected to the robot radio , and never when testing at home, unless you are well versed in networking or have the relevant “know how”. Ensure your robot is on and you are connected to the robot network. Navigate to photonvision.local:5800 in your browser. Open the settings tab on the left pane. Under the Networking section, set your team number. Change your IP to Static. Set your coprocessor’s IP address to “10.TE.AM.11”. More information on IP format can be found here . Click the “Save” button. Power-cycle your robot and then you will now be access the PhotonVision dashboard at 10.TE.AM.11:5800 . The “team number” field will accept (in addition to a team number) an IP address or hostname. This is useful for testing PhotonVision on the same computer as a simulated robot program; you can set the team number to “localhost”, and PhotonVision will send data to the network tables in the simulated robot. Port Forwarding  Note If you are using a VH-109 radio (2025 and later, excluding China and Taiwan), you should not use port forwarding. Instead, tether to the dedicated DS ethernet port on the VH-109. The VH-109 does not exhibit the issues found in the OM5P radio with multiple ports, and with a dedicated DS port, it provides more realistic match conditions and removes the need to tether over USB. If you would like to access your Ethernet-connected vision device from a computer when tethered to the USB port on the roboRIO, you can use WPILib’s PortForwarder . JAVA PortForwarder . add ( 5800 , \"photonvision.local\" , 5800 ); C++ wpi :: PortForwarder :: GetInstance (). Add ( 5800 , \"photonvision.local\" , 5800 ); PYTHON # Coming Soon! Note The address in the code above ( photonvision.local ) is the hostname of the coprocessor. This can be different depending on your hardware, and can be checked in the settings tab under “hostname”. Camera Stream Ports  The camera streams start at 1181 with two ports for each camera (ex. 1181 and 1182 for camera one, 1183 and 1184 for camera two, etc.). The easiest way to identify the port of the camera that you want is by double clicking on the stream, which opens it in a separate page. The port will be listed below the stream. Warning If your camera stream isn’t sent to the same port as it’s originally found on, its stream will not be visible in the UI. SSH Access  For advanced users, SSH access is available for coprocessors running PhotonVision. This allows you to perform advanced configurations and troubleshooting. The default credentials are: photon:vision for all devices using an image of v2026.0.3 or later. The legacy credentials of pi:raspberry will still work, but it’s recommended to switch to the new credentials as the old ones will be deprecated in a future release.",
      "content_preview": "Networking  Physical Networking  Warning When using PhotonVision off robot, you MUST plug the coprocessor into a physical router/radio. You can then connect your laptop/device used to view the webdashboard to the same network."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/quick-start/wiring.html",
      "title": "Wiring",
      "section": "Getting Started",
      "language": "All",
      "content": "Wiring  Coprocessor with regulator  IT IS STRONGLY RECOMMENDED to use one of the recommended power regulators to prevent vision from cutting out from voltage drops while operating the robot. We recommend wiring the regulator directly to the power header pins or using a locking USB C cable. In any case we recommend hot gluing the connector. Run an ethernet cable from your Pi to your network switch / radio. This diagram shows how to use the recommended regulator to power a coprocessor. Orange Pi Zinc V USB C Orange Pi 5 Zinc V Orange Pi 5 Pololu S13V30F5 Orange Pi 5 Pololu S13V30F5 Pigtail Raspberry Pi 5 Zinc V USB C Raspberry Pi 5 Zinc V Raspberry Pi 5 Pololu S13V30F5 Raspberry Pi 5 Pololu S13V30F5 Pigtail Pigtails can be purchased from many sources we recommend (USB C) (Micro USB) Limelight  Follow the wiring instructions located in the Limelight Documentation for your Limelight model. Coprocessor with Passive POE (Pi with SnakeEyes)  Plug the passive POE injector into the coprocessor and wire it to PDP/PDH (NOT the VRM). Add a breaker to relevant slot in your PDP/PDH Run an ethernet cable from the passive POE injector to your network switch / radio. Off-Robot Wiring  Plugging your coprocessor into the wall via a power brick will suffice for off robot wiring. Note Please make sure your chosen power supply can provide enough power for your coprocessor. Undervolting (where enough power isn’t being supplied) can cause many issues.",
      "content_preview": "Wiring  Coprocessor with regulator  IT IS STRONGLY RECOMMENDED to use one of the recommended power regulators to prevent vision from cutting out from voltage drops while operating the robot. We recommend wiring the regulator directly to the power header pins or using a locking USB C cable."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/quick-start/camera-matching.html",
      "title": "Camera Matching",
      "section": "Getting Started",
      "language": "All",
      "content": "Camera Matching  Activating and Deactivating Cameras  When you first plug in a camera, it will be detected and added to the list of cameras with the “Unassigned” status, as shown below. You can press the “Activate” button to enable PhotonVision to use the camera. If a camera has been activated in the past, it will be listed as “Deactivated” in the camera list. You can press the “Activate” button to enable PhotonVision to use the camera. Once a camera is activated, it will be listed as “Active” in the camera list. You can press the “Deactivate” button to stop PhotonVision from using the camera. Deleting Cameras  If you want to remove a camera from the list, you can press the delete button. This will clear all settings for that particular camera, including the calibration data and any other settings you have configured. It is recommended to make a backup of the camera’s settings before deleting it, as this action cannot be undone. Matching Cameras  When you plug in a camera, PhotonVision will attempt to match it to a previously configured camera based on the physical USB port it is connected to. If you plug another camera into that port, the cameras will have a “Camera Mismatch” status, indicating that the camera is not recognized as the one that was previously configured. Additionally, pressing on the Details button will show you the details of the camera mismatch, allowing you to compare the current camera with the previously configured camera. Note Camera matching is based on the USB ports on the device. If you unplug a camera and plug it into a different port, PhotonVision will attempt to use settings from the camera that was previously configured in that port, causing unexpected behavior. To resolve the camera mismatch, you should ensure each camera is plugged into the same port that you configured it in.",
      "content_preview": "Camera Matching  Activating and Deactivating Cameras  When you first plug in a camera, it will be detected and added to the list of cameras with the “Unassigned” status, as shown below. You can press the “Activate” button to enable PhotonVision to use the camera."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/quick-start/camera-focusing.html",
      "title": "Camera Focusing",
      "section": "Getting Started",
      "language": "All",
      "content": "Camera Focusing  Prepare Camera  Warning Refocusing your camera will make your calibration inaccurate, make sure to recalibrate after focusing. To ensure that your camera is focused properly, mount it to a secure surface and ensure it does not move drastically. Point your camera at a detailed surface like a calibration board, and make sure that it not too close to the camera. Using Focus Mode  Important When you enable Focus Mode, it will assign a Score to the current focus, this score depends on your environment and the lighting. This score cannot be compared to a focus score collected from other environments. In the Cameras tab, turn on Focus Mode. Rotate the lens on your camera to try and get the focus score as high as possible. Once you cannot get a higher score, this indicates that your camera is fully focused and can be set in place using glue if desired.",
      "content_preview": "Camera Focusing  Prepare Camera  Warning Refocusing your camera will make your calibration inaccurate, make sure to recalibrate after focusing. To ensure that your camera is focused properly, mount it to a secure surface and ensure it does not move drastically."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/quick-start/quick-install.html",
      "title": "Quick Installation Guide",
      "section": "Getting Started",
      "language": "All",
      "content": "Quick Installation Guide  For the following supported coprocessors Raspberry Pi 3,4,5 Orange Pi 5, 5B, 5 Pro Limelight 2, 2+, 3, 3G, 4 Rubik Pi 3 For installing on non-supported devices see here. Download the latest preconfigured image of photonvision for your coprocessor Coprocessor Image filename Jar Raspberry Pi 3, 4, 5 photonvision-{version}-linuxarm64_RaspberryPi.img.xz photonvision-{version}-linuxarm64.jar OrangePi 5 photonvision-{version}-linuxarm64_orangepi5.img.xz photonvision-{version}-linuxarm64.jar OrangePi 5B photonvision-{version}-linuxarm64_orangepi5b.img.xz photonvision-{version}-linuxarm64.jar OrangePi 5 Pro photonvision-{version}-linuxarm64_orangepi5pro.img.xz photonvision-{version}-linuxarm64.jar Limelight 2 photonvision-{version}-linuxarm64_limelight2.img.xz photonvision-{version}-linuxarm64.jar Limelight 3 photonvision-{version}-linuxarm64_limelight3.img.xz photonvision-{version}-linuxarm64.jar Limelight 3G photonvision-{version}-linuxarm64_limelight3G.img.xz photonvision-{version}-linuxarm64.jar Limelight 4 photonvision-{version}-linuxarm64_limelight4.img.xz photonvision-{version}-linuxarm64.jar Rubik Pi 3 photonvision-{version}-linuxarm64_rubikpi3.tar.xz photonvision-{version}-linuxarm64.jar Unless otherwise noted in release notes or if updating from the prior years version, to update PhotonVision after the initial installation, use the offline update option in the settings page with the downloaded jar file from the latest release. Raspberry Pi and Orange Pi Installation  Use the Raspberry Pi Imager to flash the image onto the coprocessors microSD card. Select the downloaded .img.xz file, select your microSD card, and flash. Warning Balena Etcher can also be used, but historically has had issues such as bootlooping (the system will repeatedly boot and restart) when imaging your device. Use at your own risk. Limelight Installation  In order to flash your Limelight you should follow the instructions on the Limelight documentation for the relevant version. Make sure to replace the Limelight OS image with the relevant PhotonVision image. Limelight Version Limelight Documentation PhotonVision Image 2 Updating Limelight 2 OS photonvision-{version}-linuxarm64_limelight2.img.xz 3 Updating Limelight 3 OS photonvision-{version}-linuxarm64_limelight3.img.xz 3G Updating Limelight 3G OS photonvision-{version}-linuxarm64_limelight3g.img.xz 4 Updating Limelight 4 OS photonvision-{version}-linuxarm64_limelight4.img.xz Note Limelight models will need a custom hardware config file for LEDs or other hardware features to work. Rubik Pi 3 Installation  Warning The Qualcomm Launcher caches files. If you flash multiple times, you may need to clear the cache by navigating to your temp directory, and deleting the qualcomm-launcher folder. To flash the Rubik Pi 3 coprocessor, it’s necessary to use the Qualcomm Launcher . Upload a custom image by selecting the Custom option in the launcher. Choose the downloaded PhotonVision .tar.xz file and follow the prompts to complete the installation. It is recommended to skip the Configure Login process, as PhotonVision will handle the necessary settings.",
      "content_preview": "Quick Installation Guide  For the following supported coprocessors Raspberry Pi 3,4,5 Orange Pi 5, 5B, 5 Pro Limelight 2, 2+, 3, 3G, 4 Rubik Pi 3 For installing on non-supported devices see here."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/apriltag-pipelines/index.html",
      "title": "AprilTag Detection",
      "section": "AprilTag Detection",
      "language": "All",
      "content": "AprilTag Detection  About AprilTags AprilTag Pipeline Types AprilTag ArUco 2D AprilTag Tuning / Tracking Tracking AprilTags Tuning AprilTags Target Family Decimate Blur Threads Refine Edges Pose Iterations Max Error Bits Decision Margin Cutoff 3D Tracking Ambiguity MultiTag Localization Enabling MultiTag Updating the Field Layout Coordinate Systems Field and Robot Coordinate Frame Camera Coordinate Frame AprilTag Coordinate Frame",
      "content_preview": "AprilTag Detection  About AprilTags AprilTag Pipeline Types AprilTag ArUco 2D AprilTag Tuning / Tracking Tracking AprilTags Tuning AprilTags Target Family Decimate Blur Threads Refine Edges Pose Iterations Max Error Bits Decision Margin Cutoff 3D Tracking Ambiguity MultiTag Localization Enabling..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/apriltag-pipelines/about-apriltags.html",
      "title": "About AprilTags",
      "section": "Getting Started",
      "language": "All",
      "content": "About AprilTags  AprilTags are a common type of visual fiducial marker. Visual fiducial markers are artificial landmarks added to a scene to allow “localization” (finding your current position) via images. In simpler terms, tags mark known points of reference that you can use to find your current location. They are similar to QR codes in which they encode information, however, they hold only a single number. By placing AprilTags in known locations around the field and detecting them using PhotonVision, you can easily get full field localization / pose estimation. Alternatively, you can use AprilTags the same way you used retroreflective tape, simply using them to turn to goal without any pose estimation. A more technical explanation can be found in the WPILib documentation . Note You can get FIRST’s official PDF of the targets used in 2025 here .",
      "content_preview": "About AprilTags  AprilTags are a common type of visual fiducial marker. Visual fiducial markers are artificial landmarks added to a scene to allow “localization” (finding your current position) via images."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/apriltag-pipelines/detector-types.html",
      "title": "AprilTag Pipeline Types",
      "section": "AprilTag Detection",
      "language": "All",
      "content": "AprilTag Pipeline Types  PhotonVision offers two different AprilTag pipeline types based on different implementations of the underlying algorithm. Each one has its advantages / disadvantages, which are detailed below. Note Note that both of these pipeline types detect AprilTag markers and are just two different algorithms for doing so. AprilTag  The AprilTag pipeline type is based on the AprilTag library from the University of Michigan and we recommend it for most use cases. It is (to our understanding) most accurate pipeline type, but is also ~2x slower than ArUco. This was the pipeline type used by teams in the 2023 season and is well tested. ArUco  The ArUco pipeline is based on the ArUco library implementation from OpenCV. It is ~2x higher fps and ~2x lower latency than the AprilTag pipeline type, but is less accurate. We recommend this pipeline type for teams that need to run at a higher framerate or have a lower powered device. This pipeline type was new for the 2024 season.",
      "content_preview": "AprilTag Pipeline Types  PhotonVision offers two different AprilTag pipeline types based on different implementations of the underlying algorithm. Each one has its advantages / disadvantages, which are detailed below."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/reflectiveAndShape/thresholding.html",
      "title": "Thresholding",
      "section": "Reflective & Shape Detection",
      "language": "All",
      "content": "Thresholding  For colored shape detection, we want to tune our HSV thresholds such that only the goal color remains after the thresholding. The HSV color representation is similar to RGB in that it represents colors. However, HSV represents colors with hue, saturation and value components. Hue refers to the color, while saturation and value describe its richness and brightness. In PhotonVision, HSV thresholds is available in the “Threshold” tab. Your browser does not support the video tag. Color Picker  The color picker can be used to quickly adjust HSV values. “Set to average” will set the HSV range to the color of the pixel selected, while “shrink range” and “expand range” will change the HSV threshold to include or exclude the selected pixel, respectively. Your browser does not support the video tag. Tuning Steps  The following steps were derived from FRC 254’s 2016 Championship presentation on computer vision and allows you to accurately tune PhotonVision to track your target. In order to properly capture the colors that you want, first turn your exposure low until you have a mostly dark image with the target still showing. A darker image ensures that you don’t see things that aren’t your target (ex. overhead lights). Be careful not to overexpose your image (you will be able to tell this if a target looks more cyan/white or equivalent instead of green when looking at it through the video feed) since that can give you poor results. For HSV tuning, start with Hue, as it is the most important/differentiating factor when it comes to detecting color. You want to make the range for Hue as small as possible in order to get accurate tracking. Feel free to reference the chart below to help. After you have properly tuned Hue, tune for high saturation/color intensity (S), and then brightness (V). Using this method will decrease the likelihood that you need to calibrate on the field. Saturation and Value’s upper bounds will often end up needing to be the maximum (255).",
      "content_preview": "Thresholding  For colored shape detection, we want to tune our HSV thresholds such that only the goal color remains after the thresholding. The HSV color representation is similar to RGB in that it represents colors. However, HSV represents colors with hue, saturation and value components."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/reflectiveAndShape/3D.html",
      "title": "3D Tuning",
      "section": "Reflective & Shape Detection",
      "language": "All",
      "content": "3D Tuning  In 3D mode, the SolvePNP algorithm is used to compute the position and rotation of the AprilTag or other target relative to the robot. This requires your camera to be calibrated which can be done through the cameras tab. The target model dropdown is used to select the target model used to compute target position. This should match the target your camera will be tracking. If solvePNP is working correctly, the target should be displayed as a small rectangle within the “Target Location” minimap. The X/Y/Angle reading will also be displayed in the “Target Info” card. Your browser does not support the video tag. Your browser does not support the video tag. Contour Simplification (Non-AprilTag)  3D mode internally computes a polygon that approximates the target contour being tracked. This polygon is used to detect the extreme corners of the target. The contour simplification slider changes how far from the original contour the approximation is allowed to deviate. Note that the approximate polygon is drawn on the output image for tuning.",
      "content_preview": "3D Tuning  In 3D mode, the SolvePNP algorithm is used to compute the position and rotation of the AprilTag or other target relative to the robot. This requires your camera to be calibrated which can be done through the cameras tab."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/objectDetection/index.html",
      "title": "Object Detection",
      "section": "Object Detection",
      "language": "All",
      "content": "Object Detection  About Object Detection How does it work? Tracking Objects Tuning and Filtering Letterboxing Custom Models Training Custom Models Managing Custom Models Orange Pi 5 (and variants) Object Detection How it works Supported models Converting Custom Models Rubik Pi 3 Object Detection How it works Supported models Converting Custom Models Benchmarking",
      "content_preview": "Object Detection  About Object Detection How does it work? Tracking Objects Tuning and Filtering Letterboxing Custom Models Training Custom Models Managing Custom Models Orange Pi 5 (and variants) Object Detection How it works Supported models Converting Custom Models Rubik Pi 3 Object Detection..."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/simulation/index.html",
      "title": "Simulation",
      "section": "Simulation",
      "language": "All",
      "content": "Simulation  Simulation Support in PhotonLib in Java Simulation Support in PhotonLib in C++ Simulation Support in PhotonLib in Python Hardware In The Loop Simulation",
      "content_preview": "Simulation  Simulation Support in PhotonLib in Java Simulation Support in PhotonLib in C++ Simulation Support in PhotonLib in Python Hardware In The Loop Simulation"
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/simulation/hardware-in-the-loop-sim.html",
      "title": "Hardware In The Loop Simulation",
      "section": "Simulation",
      "language": "All",
      "content": "Hardware In The Loop Simulation  Hardware in the loop simulation is using a physical device, such as a supported co-processor running PhotonVision, to enhance simulation capabilities. This is useful for developing and validating code before the camera is attached to a robot, as well as reducing the work required to use WPILib simulation with PhotonVision. Before continuing, ensure PhotonVision is installed on your device. Instructions can be found here for all devices. Your coprocessor and computer running simulation will have to be connected to the same network, like a home router. Connecting the coprocessor directly to the computer will not work. To simulate with hardware in the loop, a one-line change is required. From the PhotonVision UI, go to the sidebar and select the Settings option. Within the Networking settings, find “Team Number/NetworkTables Server Address”. During normal robot operation, a team’s number would be entered into this field so that the PhotonVision coprocessor connects to the roboRIO as a NT client. Instead, enter the IP address of your computer running the simulation here. Note To find the IP address of your Windows computer, open command prompt and run ipconfig . C:/Users/you>ipconfig Windows IP Configuration Ethernet adapter Ethernet: Connection-specific DNS Suffix . : home Link-local IPv6 Address . . . . . : fe80::b41d:e861:ef01:9dba%10 IPv4 Address. . . . . . . . . . . : 192.168.254.13 Subnet Mask . . . . . . . . . . . : 255.255.255.0 Default Gateway . . . . . . . . . : 192.168.254.254 No code changes are required, PhotonLib should function similarly to normal operation. Now launch simulation, and you should be able to see the PhotonVision table on your simulation’s NetworkTables dashboard.",
      "content_preview": "Hardware In The Loop Simulation  Hardware in the loop simulation is using a physical device, such as a supported co-processor running PhotonVision, to enhance simulation capabilities."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/simulation/simulation-java.html",
      "title": "Simulation Support in PhotonLib in Java",
      "section": "Simulation",
      "language": "All",
      "content": "Simulation Support in PhotonLib in Java  What Is Simulated?  Simulation is a powerful tool for validating robot code without access to a physical robot. Read more about simulation in WPILib . In Java, PhotonLib can simulate cameras on the field and generate target data approximating what would be seen in reality. This simulation attempts to include the following: Camera Properties Field of Vision Lens distortion Image noise Framerate Latency Target Data Detected / minimum-area-rectangle corners Center yaw/pitch Contour image area percentage Fiducial ID Fiducial ambiguity Fiducial solvePNP transform estimation Camera Raw/Processed Streams (grayscale) Note Simulation does NOT include the following: Full physical camera/world simulation (targets are automatically thresholded) Image Thresholding Process (camera gain, brightness, etc) Pipeline switching Snapshots This scope was chosen to balance fidelity of the simulation with the ease of setup, in a way that would best benefit most teams. Drivetrain Simulation Prerequisite  A prerequisite for simulating vision frames is knowing where the camera is on the field– to utilize PhotonVision simulation, you’ll need to supply the simulated robot pose periodically. This requires drivetrain simulation for your robot project if you want to generate camera frames as your robot moves around the field. References for using PhotonVision simulation with drivetrain simulation can be found in the PhotonLib Java Examples for both a differential drivetrain and a swerve drive. Important The simulated drivetrain pose must be separate from the drivetrain estimated pose if a pose estimator is utilized. Vision System Simulation  A VisionSystemSim represents the simulated world for one or more cameras, and contains the vision targets they can see. It is constructed with a unique label: JAVA // A vision system sim labelled as \"main\" in NetworkTables VisionSystemSim visionSim = new VisionSystemSim ( \"main\" ); PhotonLib will use this label to put a Field2d widget on NetworkTables at /VisionSystemSim-[label]/Sim Field . This label does not need to match any camera name or pipeline name in PhotonVision. Vision targets require a TargetModel , which describes the shape of the target. For AprilTags, PhotonLib provides TargetModel.kAprilTag16h5 for the tags used in 2023, and TargetModel.kAprilTag36h11 for the tags used starting in 2024. For other target shapes, convenience constructors exist for spheres, cuboids, and planar rectangles. For example, a planar rectangle can be created with: JAVA // A 0.5 x 0.25 meter rectangular target TargetModel targetModel = new TargetModel ( 0.5 , 0.25 ); These TargetModel are paired with a target pose to create a VisionTargetSim . A VisionTargetSim is added to the VisionSystemSim to become visible to all of its cameras. JAVA // The pose of where the target is on the field. // Its rotation determines where \"forward\" or the target x-axis points. // Let's say this target is flat against the far wall center, facing the blue driver stations. Pose3d targetPose = new Pose3d ( 16 , 4 , 2 , new Rotation3d ( 0 , 0 , Math . PI )); // The given target model at the given pose VisionTargetSim visionTarget = new VisionTargetSim ( targetPose , targetModel ); // Add this vision target to the vision system simulation to make it visible visionSim . addVisionTargets ( visionTarget ); Note The pose of a VisionTargetSim object can be updated to simulate moving targets. Note, however, that this will break latency simulation for that target. For convenience, an AprilTagFieldLayout can also be added to automatically create a target for each of its AprilTags. JAVA // The layout of AprilTags which we want to add to the vision system AprilTagFieldLayout tagLayout = AprilTagFieldLayout . loadFromResource ( AprilTagFields . kDefaultField . m_resourceFile ); visionSim . addAprilTags ( tagLayout ); Note The poses of the AprilTags from this layout depend on its current alliance origin (e.g. blue or red). If this origin is changed later, the targets will have to be cleared from the VisionSystemSim and re-added. Camera Simulation  Now that we have a simulation world with vision targets, we can add simulated cameras to view it. Before adding a simulated camera, we need to define its properties. This is done with the SimCameraProperties class: JAVA // The simulated camera properties SimCameraProperties cameraProp = new SimCameraProperties (); By default, this will create a 960 x 720 resolution camera with a 90 degree diagonal FOV(field-of-view) and no noise, distortion, or latency. If we want to change these properties, we can do so: JAVA // A 640 x 480 camera with a 100 degree diagonal FOV. cameraProp . setCalibration ( 640 , 480 , Rotation2d . fromDegrees ( 100 )); // Approximate detection noise with average and standard deviation error in pixels. cameraProp . setCalibError ( 0.25 , 0.08 ); // Set the camera image capture framerate (Note: this is limited by robot loop rate). cameraProp . setFPS ( 20 ); // The average and standard deviation in milliseconds of image data latency. cameraProp . setAvgLatencyMs ( 35 ); cameraProp . setLatencyStdDevMs ( 5 ); These properties are used in a PhotonCameraSim , which handles generating captured frames of the field from the simulated camera’s perspective, and calculating the target data which is sent to the PhotonCamera being simulated. JAVA // The PhotonCamera used in the real robot code. PhotonCamera camera = new PhotonCamera ( \"cameraName\" ); // The simulation of this camera. Its values used in real robot code will be updated. PhotonCameraSim cameraSim = new PhotonCameraSim ( camera , cameraProp ); The PhotonCameraSim can now be added to the VisionSystemSim . We have to define a robot-to-camera transform, which describes where the camera is relative to the robot pose (this can be measured in CAD or by hand). JAVA // Our camera is mounted 0.1 meters forward and 0.5 meters up from the robot pose, // (Robot pose is considered the center of rotation at the floor level, or Z = 0) Translation3d robotToCameraTrl = new Translation3d ( 0.1 , 0 , 0.5 ); // and pitched 15 degrees up. Rotation3d robotToCameraRot = new Rotation3d ( 0 , Math . toRadians ( - 15 ), 0 ); Transform3d robotToCamera = new Transform3d ( robotToCameraTrl , robotToCameraRot ); // Add this camera to the vision system simulation with the given robot-to-camera transform. visionSim . addCamera ( cameraSim , robotToCamera ); Important You may add multiple cameras to one VisionSystemSim , but not one camera to multiple VisionSystemSim . All targets in the VisionSystemSim will be visible to all its cameras. If the camera is mounted on a mobile mechanism (like a turret) this transform can be updated in a periodic loop. JAVA // The turret the camera is mounted on is rotated 5 degrees Rotation3d turretRotation = new Rotation3d ( 0 , 0 , Math . toRadians ( 5 )); robotToCamera = new Transform3d ( robotToCameraTrl . rotateBy ( turretRotation ), robotToCameraRot . rotateBy ( turretRotation )); visionSim . adjustCamera ( cameraSim , robotToCamera ); Updating The Simulation World  To update the VisionSystemSim , we simply have to pass in the simulated robot pose periodically (in simulationPeriodic() ). JAVA // Update with the simulated drivetrain pose. This should be called every loop in simulation. visionSim . update ( robotPoseMeters ); Targets and cameras can be added and removed, and camera properties can be changed at any time. Visualizing Results  Each VisionSystemSim has its own built-in Field2d for displaying object poses in the simulation world such as the robot, simulated cameras, and actual/measured target poses. JAVA // Get the built-in Field2d used by this VisionSystemSim visionSim . getDebugField (); A VisionSystemSim ’s internal Field2d customized with target images and colors  A PhotonCameraSim can also draw and publish generated camera frames to a MJPEG stream similar to an actual PhotonVision process. JAVA // Enable the raw and processed streams. These are enabled by default. cameraSim . enableRawStream ( true ); cameraSim . enableProcessedStream ( true ); // Enable drawing a wireframe visualization of the field to the camera streams. // This is extremely resource-intensive and is disabled by default. cameraSim . enableDrawWireframe ( true ); These streams follow the port order mentioned in Camera Stream Ports . For example, a single simulated camera will have its raw stream at localhost:1181 and processed stream at localhost:1182 , which can also be found in the CameraServer tab of Shuffleboard like a normal camera stream. A frame from the processed stream of a simulated camera viewing some 2023 AprilTags with the field wireframe enabled ",
      "content_preview": "Simulation Support in PhotonLib in Java  What Is Simulated?  Simulation is a powerful tool for validating robot code without access to a physical robot. Read more about simulation in WPILib ."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/troubleshooting/index.html",
      "title": "Troubleshooting",
      "section": "Troubleshooting",
      "language": "All",
      "content": "Troubleshooting  Common Issues / Questions Logging Camera Troubleshooting Networking Troubleshooting Useful Unix Commands",
      "content_preview": "Troubleshooting  Common Issues / Questions Logging Camera Troubleshooting Networking Troubleshooting Useful Unix Commands"
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/troubleshooting/logging.html",
      "title": "Logging",
      "section": "Troubleshooting",
      "language": "All",
      "content": "Logging  Note Logging is very helpful when trying to debug issues within PhotonVision, as it allows us to see what is happening within the program after it is ran. Whenever reporting an issue to PhotonVision, we request that you include logs whenever possible. In addition to storing logs in timestamped files in the config directory, PhotonVision streams logs to the web dashboard. These logs can be viewed later by pressing the ` key. In this view, logs can be filtered by level or downloaded. Note When the program first starts, it sends logs from startup to the client that first connects. This does not happen on subsequent connections. Note Logs are stored inside the photonvision_config/logs directory. Exporting the settings ZIP will also download all old logs for further review. Your browser does not support the video tag. Robot mode transitions are also recorded in program logs. These transition messages look something like the two shown below, and show the contents of the HAL Control Word that the robot was in previously, and what it is now in. This includes: Enabled state Robot state (autonomous vs teleoperated) If the robot e-stop is active If the robot is connected to the FMS at an event, we will additionally print out: Event name Match type and number Driver station position [2025-04-19 19:52:08] [NetworkTables - NTDriverStation] [INFO] ROBOT TRANSITIONED MODES! From NtControlWord[m_enabled=true, m_autonomous=false, m_test=false, m_emergencyStop=false, m_fmsAttached=true, m_dsAttached=true] to NtControlWord[m_enabled=true, m_autonomous=false, m_test=true, m_emergencyStop=false, m_fmsAttached=true, m_dsAttached=true] [2025-04-19 19:52:09] [NetworkTables - NTDriverStation] [INFO] ROBOT TRANSITIONED MODES! From NtControlWord[m_enabled=true, m_autonomous=false, m_test=true, m_emergencyStop=false, m_fmsAttached=true, m_dsAttached=true] to NtControlWord[m_enabled=false, m_autonomous=false, m_test=false, m_emergencyStop=false, m_fmsAttached=false, m_dsAttached=false] [2025-04-19 19:52:19] [NetworkTables - NTDriverStation] [INFO] ROBOT TRANSITIONED MODES! From NtControlWord[m_enabled=false, m_autonomous=false, m_test=false, m_emergencyStop=false, m_fmsAttached=false, m_dsAttached=false] to NtControlWord[m_enabled=true, m_autonomous=true, m_test=false, m_emergencyStop=false, m_fmsAttached=true, m_dsAttached=true]",
      "content_preview": "Logging  Note Logging is very helpful when trying to debug issues within PhotonVision, as it allows us to see what is happening within the program after it is ran. Whenever reporting an issue to PhotonVision, we request that you include logs whenever possible."
    },
    {
      "url": "https://docs.photonvision.org/en/latest/docs/troubleshooting/unix-commands.html",
      "title": "Useful Unix Commands",
      "section": "Troubleshooting",
      "language": "All",
      "content": "Useful Unix Commands  Networking  SSH  SSH (Secure Shell) is used to securely connect from a local to a remote system (ex. from a laptop to a coprocessor). Unlike other commands on this page, ssh is not Unix specific and can be done on Windows and MacOS from their respective terminals. Note You may see a warning similar to The authenticity of host 'xxx' can't be established... or WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED! , in most cases this can be safely ignored if you have confirmed that you are connecting to the correct host over a secure connection, and the fingerprint will change when your operating system is reinstalled or PhotonVision’s coprocessor image is re-flashed. This can also occur if you have multiple coprocessors with the same hostname on your network. You can read more about it here Example: ssh pi @hostname For PhotonVision, the username will be pi and the password will be raspberry . ip  Run ip address with your coprocessor connected to a monitor in order to see its IP address and other network configuration information. Your output might look something like this: 2 : end1 : < BROADCAST , MULTICAST , UP , LOWER_UP > mtu 1500 qdisc mq state UP group default qlen 1000 link / ether de : 9 a : 8 f : 7 d : 31 : aa brd ff : ff : ff : ff : ff : ff inet 10.88.47.12 / 24 brd 10.88.47.255 scope global dynamic noprefixroute end1 valid_lft 27367 sec preferred_lft 27367 sec In this example, the numbers following inet (10.88.47.12) are your IP address. ping  ping is a command-line utility used to test the reachability of a host on an IP network. It also measures the round-trip time for messages sent from the originating host to a destination computer. It can be used to determine if a network interface is available, which can be helpful when debugging. File Transfer  All files under /opt/photonvision are owned by the root user. This means that if you want to modify them, the commands to do so must be ran as sudo. SCP  SCP (Secure Copy) is used to securely transfer files between local and remote systems. Example: scp [ file ] pi @hostname : / path / to / destination SFTP  SFTP (SSH File Transfer Protocol) is another option for transferring files between local and remote systems. Filezilla  Filezilla is a GUI alternative to SCP and SFTP. It is available for Windows, MacOS, and Linux. Miscellaneous  v4l2-ctl  v4l2-ctl is a command-line tool for controlling video devices. List available video devices (used to verify the device recognized a connected camera): v4l2 - ctl -- list - devices List supported formats and resolutions for a specific video device: v4l2 - ctl -- list - formats - ext -- device / path / to / video_device List all video device’s controls and their values: v4l2 - ctl -- list - ctrls -- device path / to / video_device Note This command is especially useful in helping to debug when certain camera controls, like exposure, aren’t behaving as expected. If you see an error in the logs similar to WARNING 30: failed to set property [property name] (UsbCameraImpl.cpp:646) , that means that PhotonVision is trying to use a control that doesn’t exist or has a different name on your hardware. If you encounter this issue, please file an issue with the necessary logs and output of the v4l2-ctl --list-ctrls command. systemctl  systemctl is a command that controls the systemd system and service manager. Start PhotonVision: systemctl start photonvision Stop PhotonVision: systemctl stop photonvision Restart PhotonVision: systemctl restart photonvision Check the status of PhotonVision: systemctl status photonvision journalctl  journalctl is a command that queries the systemd journal, which is a logging system used by many Linux distributions. View the PhotonVision logs: journalctl -- output cat - u photonvision View the PhotonVision logs in real-time: journalctl -- output cat - u photonvision - f --output cat is used to prevent journalctl from printing its own timestamps, because we log our own timestamps.",
      "content_preview": "Useful Unix Commands  Networking  SSH  SSH (Secure Shell) is used to securely connect from a local to a remote system (ex. from a laptop to a coprocessor). Unlike other commands on this page, ssh is not Unix specific and can be done on Windows and MacOS from their respective terminals."
    }
  ]
}